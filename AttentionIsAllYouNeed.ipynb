{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AttentionIsAllYouNeed.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uRd5F_7HaFZ",
        "colab_type": "code",
        "outputId": "71985497-b312-4324-8fc7-a8f5796c40a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!hd login --github\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Opening browser, please wait. If something goes wrong, press CTRL+C to cancel.\n",
            "\u001b[1m SSH'd into a remote machine, or just don't have access to a browser? Open this link in any browser and then copy/paste the provided access token: \u001b[4mhttps://hyperdash.io/oauth/github/start?state=client_cli_manual\u001b[0m \u001b[0m\n",
            "Waiting for Github OAuth to complete.\n",
            "If something goes wrong, press CTRL+C to cancel.\n",
            "Access token: qFd+bm6VEC2q43EbzgjhBxh43KMVY49C2HbQ+8fysWE=\n",
            "Successfully logged in! We also installed: 6dqfQAL9Xij4kBZzoFO+iDTxNHszbaxsxhzaeg0f/DE= as your default API key\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKOHJfp-bHRC",
        "colab_type": "code",
        "outputId": "3af2daff-550c-47ee-a74a-0d15c1896d9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install wget\n",
        "!pip install hyperdash\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!pip install -U torchtext\n",
        "!pip install git+git://github.com/williamFalcon/pytorch-lightning.git@master --upgrade"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=0026aa09aa1da6293dd97199eb1ebc396252e7841ec6f41bdeb96370b9006af5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting hyperdash\n",
            "  Downloading https://files.pythonhosted.org/packages/fb/a1/2606aa8a8c3bf083cb305dba3164cb00285f97db34080d63c22b6c413175/hyperdash-0.15.3.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hyperdash) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from hyperdash) (1.12.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from hyperdash) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->hyperdash) (1.3)\n",
            "Building wheels for collected packages: hyperdash\n",
            "  Building wheel for hyperdash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hyperdash: filename=hyperdash-0.15.3-cp36-none-any.whl size=28553 sha256=edca5a2b73cebf07a9165294905771c1ba2088256e1638430f8209b73dd77716\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/5f/af/bbcaeb6570e4904c14fb4c1b70fee559a3788182ce4d104ce7\n",
            "Successfully built hyperdash\n",
            "Installing collected packages: hyperdash\n",
            "Successfully installed hyperdash-0.15.3\n",
            "Collecting tokenizers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.7.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 2.8MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 12.6MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 18.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 25.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.39)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.39)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=d0c31c6ba1a4e2470098685ec1f3935e8b727844ee4d67fef3d797da6237ce6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "  Found existing installation: tokenizers 0.7.0\n",
            "    Uninstalling tokenizers-0.7.0:\n",
            "      Successfully uninstalled tokenizers-0.7.0\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.85 tokenizers-0.5.2 transformers-2.8.0\n",
            "Collecting torchtext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/ef/54b8da26f37787f5c670ae2199329e7dccf195c060b25628d99e587dac51/torchtext-0.5.0-py3-none-any.whl (73kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: sentencepiece in /usr/local/lib/python3.6/dist-packages (from torchtext) (0.1.85)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from torchtext) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.5.0\n",
            "Collecting git+git://github.com/williamFalcon/pytorch-lightning.git@master\n",
            "  Cloning git://github.com/williamFalcon/pytorch-lightning.git (to revision master) to /tmp/pip-req-build-_838h7jn\n",
            "  Running command git clone -q git://github.com/williamFalcon/pytorch-lightning.git /tmp/pip-req-build-_838h7jn\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.4rc1) (1.4.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.4rc1) (2.2.0)\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 2.7MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.41.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.16.4 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.4rc1) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (46.1.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (1.28.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->pytorch-lightning==0.7.4rc1) (3.1.0)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-0.7.4rc1-cp36-none-any.whl size=221768 sha256=9ca4498d5731a4ab70a2acf6c535fff99d62afd52153e8a7782e201198bac9ff\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_mb9y1m7/wheels/02/e9/33/ecf2ab0b937f47c530a3d24222ca1a784412a0c7d490195c5f\n",
            "Successfully built pytorch-lightning\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=f6760299d176f5e0e68444ada5ff3a4a7c6f95a303d1dd608589110b30e582d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "Installing collected packages: future, tqdm, pytorch-lightning\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: tqdm 4.38.0\n",
            "    Uninstalling tqdm-4.38.0:\n",
            "      Successfully uninstalled tqdm-4.38.0\n",
            "Successfully installed future-0.18.2 pytorch-lightning-0.7.4rc1 tqdm-4.45.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut_1paJ7qXo6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title LanguageIndex\n",
        "\n",
        "import spacy\n",
        "from collections import Counter \n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class LanguageIndex():\n",
        "\n",
        "    def __init__(self, lang,tokenizer=\"spacy\", pad=\"<PAD>\",init_token=\"<SOS>\",eos_token=\"<EOS>\",unk_token=\"<UNK>\",max_len=None,vocab_size=None,lower_case=True):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.special={}\n",
        "        self.max_len=max_len\n",
        "        self.vocab_size=vocab_size-4 if vocab_size!=None else sys.maxsize\n",
        "        self.lower=lower_case\n",
        "        self.tokenizer=tokenizer\n",
        "        if self.tokenizer==\"BERT\":\n",
        "            model_type = 'bert-base-uncased'\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx[pad] = 0\n",
        "        self.special[\"pad_token\"]=pad\n",
        "\n",
        "        self.word2idx[init_token] = 1\n",
        "        self.special[\"init_token\"]=init_token\n",
        "\n",
        "        self.word2idx[eos_token] = 2\n",
        "        self.special[\"eos_token\"]=eos_token\n",
        "\n",
        "        self.word2idx[unk_token] = 3\n",
        "        self.special[\"unk_token\"]=unk_token\n",
        "\n",
        "        self.vocab = set()\n",
        "        self.counter=Counter()\n",
        "        self.spacy=None\n",
        "        self.create_index()\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def unicode_to_ascii(s):\n",
        "        \"\"\"\n",
        "        Normalizes latin chars with accent to their canonical decomposition\n",
        "        \"\"\"\n",
        "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn')\n",
        "        \n",
        "    @staticmethod\n",
        "    def preprocess_sentence(w):\n",
        "        w = unicode_to_ascii(w.lower().strip())\n",
        "        \n",
        "        # creating a space between a word and the punctuation following it\n",
        "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "        w = re.sub(r'[\" \"]+', \" \", w)\n",
        "        \n",
        "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "        \n",
        "        w = w.rstrip().strip()\n",
        "        \n",
        "        # adding a start and an end token to the sentence\n",
        "        # so that the model know when to start and stop predicting.\n",
        "        return w\n",
        "        \n",
        "    def tokenize(self,phrase):\n",
        "        if self.lower:\n",
        "            phrase=phrase.lower()\n",
        "        if self.tokenizer==\"spacy\":\n",
        "            if not self.spacy:\n",
        "                self.spacy = spacy.load('en')\n",
        "            return [tok.text for tok in self.spacy.tokenizer(phrase)]\n",
        "        if self.tokenizer==\"BERT\":\n",
        "            return self.bert_tokenizer.tokenize(phrase)\n",
        "        else:\n",
        "            return self.preprocess(phrase)\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            # update with individual tokens\n",
        "            tokens=self.tokenize(phrase.lower() if self.lower else phrase)\n",
        "            self.vocab.update(tokens)\n",
        "            self.counter.update(tokens)\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        start_index = max(self.word2idx.values())+1\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.counter.most_common(self.vocab_size)):\n",
        "            self.word2idx[word[0]] = index + start_index \n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word\n",
        "\n",
        "    def encode_batch(self,batch,special_tokens=True):\n",
        "        return np.array([self.encode(obj,special_tokens=special_tokens) for obj in batch],dtype=np.int64)\n",
        "    def decode_batch(self,batch):\n",
        "        return [self.decode(obj) for obj in batch]\n",
        "\n",
        "    def encode(self,input,special_tokens=True):\n",
        "        pad_len=self.max_len\n",
        "        input=input.lower() if self.lower else input\n",
        "        tokens=[tok for tok in self.tokenize(input)]\n",
        "        if pad_len!=None:\n",
        "            if len(tokens)>pad_len-(2 if special_tokens else 0):\n",
        "                if special_tokens:\n",
        "                    return [1]+[self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens][:pad_len-2]+[2]\n",
        "                else:\n",
        "                    return [self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens][:pad_len]\n",
        "            else:\n",
        "                return ([1] if special_tokens else []) + [self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens] +([2] if special_tokens else []) +[0 for i in range(pad_len-(2 if special_tokens else 0)-len(tokens))]\n",
        "        return ([1] if special_tokens else []) + [self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens] +([2] if special_tokens else []) \n",
        "    def decode(self,input,to_string=False):\n",
        "        sent=[self.idx2word[s] if s in self.idx2word.keys() else self.special[\"unk_token\"] for s in input]\n",
        "        if self.tokenizer==\"BERT\" and to_string:\n",
        "            return self.bert_tokenizer.convert_tokens_to_string(sent)\n",
        "        return sent\n",
        "    def vocab_size_final(self):\n",
        "        return len(self.word2idx.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9sTKJcsqmg4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title torch dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class TrainData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "        \n",
        "class TestData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        return x,y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92s4FSCxqdaA",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title QGenDataset\n",
        "import wget\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torchtext\n",
        "import spacy\n",
        "import zipfile\n",
        "import unicodedata\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class QGenDataset(object):\n",
        "    def __init__(self,squad=True,USE_ENTIRE_SENTENCE=True):\n",
        "        self.USE_ENTIRE_SENTENCE=USE_ENTIRE_SENTENCE\n",
        "        self.squad=squad\n",
        "        if squad:\n",
        "            if not os.path.exists(\"./train-v2.0.json\"):\n",
        "                wget.download(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
        "            with open(\"./train-v2.0.json\",'r') as f:\n",
        "                self.raw_data=json.load(f)\n",
        "            self.data=self._get_dataset()\n",
        "        if not squad:\n",
        "            if not os.path.exists(\"./spa-eng.zip\"):\n",
        "                wget.download('http://download.tensorflow.org/data/spa-eng.zip')\n",
        "            if not os.path.exists(\"./spa/spa-eng/spa.txt\"):\n",
        "                with zipfile.ZipFile(\"./spa-eng.zip\", 'r') as zip_ref:\n",
        "                    zip_ref.extractall(\"./spa/\")\n",
        "            with open(\"./spa/spa-eng/spa.txt\",'r') as f:\n",
        "                self.nmt_raw=f.read().strip().split('\\n')\n",
        "            self.__get_NMT__()\n",
        "    def __get_NMT__(self):\n",
        "        original_word_pairs = [[w for w in l.split('\\t')] for l in self.nmt_raw]\n",
        "        self.eng=[i[0] for i in original_word_pairs]\n",
        "        self.spa=[i[1] for i in original_word_pairs]\n",
        "\n",
        "    def get_AQ(self,max_len=80,sample=True):\n",
        "        raw_data = {'ans' : [line[0] for line in self.data], 'que': [line[1] for line in self.data]}\n",
        "        df = pd.DataFrame(raw_data, columns=[\"ans\", \"que\"])\n",
        "        # remove very long sentences and sentences where translations are \n",
        "        # not of roughly equal length\n",
        "        df['ans_len'] = df['ans'].str.count(' ')\n",
        "        df['que_len'] = df['que'].str.count(' ')\n",
        "        df = df.query('ans_len <'+str(max_len)+' & que_len <'+str(max_len))\n",
        "        df = df.drop_duplicates()\n",
        "        if sample:\n",
        "            return df[\"ans\"].values[:2000],df[\"que\"].values[:2000]\n",
        "        return df[\"ans\"].values,df[\"que\"].values\n",
        "        \n",
        "\n",
        "    def get_NMT(self,sample=False):\n",
        "        if sample:\n",
        "            return self.eng[:2000],self.spa[:2000]   \n",
        "        return self.eng,self.spa \n",
        "\n",
        "    def _create_dataset(self,data,normalize=True):\n",
        "        load_failure=0\n",
        "        try:\n",
        "            if \"data\" in data.keys():\n",
        "                data=data[\"data\"]\n",
        "        except:\n",
        "            pass\n",
        "        que_ans=[]\n",
        "        for topic in data:\n",
        "            for para in topic[\"paragraphs\"]:\n",
        "                for qa in para[\"qas\"]:\n",
        "                    try:\n",
        "                        res=[]\n",
        "                        if normalize:\n",
        "                            res.append(self._normalize(self._get_sentence(para[\"context\"],qa[\"answers\"][0][\"answer_start\"],qa[\"answers\"][0][\"text\"])))\n",
        "                            res.append(self._normalize(qa[\"question\"]))\n",
        "                        else:\n",
        "                            res.append(self._get_sentence(para[\"context\"],qa[\"answers\"][0][\"answer_start\"],qa[\"answers\"][0][\"text\"]))\n",
        "                            res.append(qa[\"question\"])\n",
        "                        que_ans.append(res)\n",
        "                    except:\n",
        "                        load_failure+=1\n",
        "        print(\"Load Failure : \",load_failure)\n",
        "        return que_ans\n",
        "    @staticmethod\n",
        "    def _get_sentence(context,position,text):\n",
        "        if \".\" in text[:-1]:\n",
        "            return_2=True\n",
        "        else:\n",
        "            return_2=False\n",
        "        context=context.split(\".\")\n",
        "        count=0\n",
        "        for sent in range(len(context)):\n",
        "            if count+len(context[sent])>position:\n",
        "                if return_2:\n",
        "                    return \".\".join(context[sent:sent+2])\n",
        "                else:\n",
        "                    return context[sent]\n",
        "            else:\n",
        "                count+=len(context[sent])+1\n",
        "        return False\n",
        "\n",
        "    def _get_dataset(self,normalize=True):\n",
        "        data =  self._create_dataset(self.raw_data,normalize=normalize)\n",
        "        return data  \n",
        "    def __len__(self):\n",
        "        return self.data_len\n",
        "    def apply(self,function,all=True):\n",
        "        for i in tqdm(range(self.data_len),position=0,leave=True):\n",
        "            self.context[i]=function(self.context[i])\n",
        "            self.answers[i]=function(self.answers[i])\n",
        "            self.questions[i]=function(self.questions[i])\n",
        "\n",
        "    def bert_format(self):\n",
        "        X=[0 for i in range(self.data_len)]\n",
        "        Y=[0 for i in range(self.data_len)]\n",
        "        for i in range(self.data_len):\n",
        "            X[i]=\"[CLS] \" + self.context[i] +\"[SEP]\"+ self.answers[i] + \"[SEP]\"\n",
        "            Y[i]=self.questions[i]\n",
        "        return (X,Y)\n",
        "    @staticmethod\n",
        "    def unicodeToAscii(s):\n",
        "        return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    def _normalize(self,s):\n",
        "        s = self.unicodeToAscii(s.lower().strip())\n",
        "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "        #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "        return s\n",
        "\n",
        "    def getData(self,input_vocab,output_vocab,max_len,tokenizer,sample=False,batch_size=64,val_split=0.1,test_split=0.1):\n",
        "        if self.squad:\n",
        "            input_,output_=self.get_AQ(max_len=max_len,sample=sample)\n",
        "        else:\n",
        "            input_,output_=self.get_NMT(sample=sample)\n",
        "        print(f\"Loaded: {len(input_)} samples\")\n",
        "        train_set_input,test_set_input,train_set_output,test_set_output=train_test_split(input_,output_,test_size=test_split)\n",
        "        input_train,input_test,output_train,output_test=train_test_split(train_set_input,train_set_output,test_size=val_split)\n",
        "        inpLang=LanguageIndex(input_train,vocab_size=input_vocab,max_len=max_len,tokenizer=tokenizer)\n",
        "        optLang=LanguageIndex(output_train,vocab_size=output_vocab,max_len=max_len,tokenizer=tokenizer)\n",
        "        input_train_tokens=inpLang.encode_batch(input_train)\n",
        "        input_test_tokens=inpLang.encode_batch(input_test)\n",
        "        ouptut_train_tokens=optLang.encode_batch(output_train)\n",
        "        output_test_tokens=optLang.encode_batch(output_test)\n",
        "        test_dataset = TestData(test_set_input,test_set_output)\n",
        "        train_dataset = TrainData(input_train_tokens,ouptut_train_tokens)\n",
        "        val_dataset = TrainData(input_test_tokens, output_test_tokens)\n",
        "        return train_dataset,val_dataset,test_dataset,inpLang,optLang\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjzHINLjH2SX",
        "colab_type": "code",
        "outputId": "90a9d6a3-1393-4aa2-e638-a9e173b22566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import argparse\n",
        "hparams={\n",
        "\"usetpu\":False,\n",
        "\"SQUAD\":True,\n",
        "\"SAMPLE\":False,\n",
        "\"EPOCHS\":8,\n",
        "\"INPUT_VOCAB\":80000,\n",
        "\"OUTPUT_VOCAB\":40000,\n",
        "\"MAX_LEN\":100,\n",
        "\"BATCH_SIZE\":128,\n",
        "\"EMB_DIM\":300,\n",
        "\"tokenizer\":\"spacy\",\n",
        "\"lr\":1e-3,\n",
        "\"model_name\":\"transformer\",\n",
        "\"HID_DIM\" : 256,\n",
        "\"ENC_LAYERS\" : 3,\n",
        "\"DEC_LAYERS\" : 3,\n",
        "\"ENC_HEADS\" : 8,\n",
        "\"DEC_HEADS\" : 8,\n",
        "\"ENC_PF_DIM\" : 600,\n",
        "\"DEC_PF_DIM\" : 600,\n",
        "\"ENC_DROPOUT\" : 0.1,\n",
        "\"DEC_DROPOUT\" : 0.1\n",
        "}\n",
        "hparams=argparse.Namespace(**hparams)\n",
        "\n",
        "qg=QGenDataset(squad=hparams.SQUAD)\n",
        "train_data,val_data,test_data,inpLang,optLang=qg.getData(input_vocab=hparams.INPUT_VOCAB,\n",
        "                                                         output_vocab=hparams.OUTPUT_VOCAB,\n",
        "                                                         max_len=hparams.MAX_LEN,\n",
        "                                                         tokenizer=hparams.tokenizer,\n",
        "                                                         sample=hparams.SAMPLE,\n",
        "                                                         batch_size=hparams.BATCH_SIZE)\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader=DataLoader(train_data,batch_size=hparams.BATCH_SIZE,num_workers=10)\n",
        "val_dataloader=DataLoader(val_data,batch_size=hparams.BATCH_SIZE,num_workers=10)\n",
        "test_dataloader=DataLoader(test_data,batch_size=hparams.BATCH_SIZE,num_workers=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load Failure :  43498\n",
            "Loaded: 86617 samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsBqRm3xBjaR",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Pytorhc Lightning\n",
        "import os\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import pytorch_lightning as pl\n",
        "import random\n",
        "from torch import nn\n",
        "usetpu=False\n",
        "if usetpu:\n",
        "    import torch_xla.core.xla_model as xm\n",
        "import argparse\n",
        "import pickle\n",
        "import json\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        self.tok_embedding=nn.Embedding(input_dim,hid_dim)\n",
        "        self.pos_embedding=nn.Embedding(max_length,hid_dim)\n",
        "        self.hid_dim=hid_dim\n",
        "        self.layers=nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout) for _ in range(n_layers)])\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,src,src_mask):\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        self.scale=torch.sqrt(torch.FloatTensor([self.hid_dim]).type_as(src).float())\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).type_as(src)\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src\n",
        "\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,src,src_mask):\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        return src\n",
        "\n",
        "\n",
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        assert hid_dim % n_heads == 0\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        batch_size = query.shape[0]\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim]).type_as(query).float())\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(energy, dim = -1)                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        x = self.fc_o(x)\n",
        "        return x, attention\n",
        "\n",
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout) \n",
        "                                     for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):  \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).type_as(trg)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.hid_dim]).type_as(trg).float())\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.fc_out(trg)\n",
        "        return output, attention\n",
        "\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):  \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, trg len]\n",
        "        #src_mask = [batch size, src len]\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        return trg, attention\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,hparams):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hparams.INPUT_VOCAB, \n",
        "                    hparams.HID_DIM, \n",
        "                    hparams.ENC_LAYERS, \n",
        "                    hparams.ENC_HEADS, \n",
        "                    hparams.ENC_PF_DIM, \n",
        "                    hparams.ENC_DROPOUT)\n",
        "        self.decoder = Decoder(hparams.OUTPUT_VOCAB, \n",
        "                    hparams.HID_DIM, \n",
        "                    hparams.DEC_LAYERS, \n",
        "                    hparams.DEC_HEADS, \n",
        "                    hparams.DEC_PF_DIM, \n",
        "                    hparams.DEC_DROPOUT)\n",
        "\n",
        "    def make_src_mask(self, src):        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "\n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask=torch.tril(torch.ones((trg_len, trg_len)).type_as(trg)).bool()\n",
        "        # print(trg_sub_mask)\n",
        "        # trg_sub_mask = torch.tril(trg_sub_mask.new((trg_len, trg_len))).bool()\n",
        "        # print(trg_sub_mask)\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)        \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output, attention\n",
        "\n",
        "\n",
        "class Seq2seq(pl.LightningModule):\n",
        "    def __init__(self,hparams):\n",
        "        super(Seq2seq,self).__init__()\n",
        "        if isinstance(hparams,argparse.Namespace):\n",
        "            self.hparams=hparams\n",
        "        else:\n",
        "            with open(hparams+\"hparams.p\",'rb') as f:\n",
        "                self.hparams=pickle.load(f)\n",
        "        self.transformer=Transformer(self.hparams)\n",
        "        if not isinstance(hparams,argparse.Namespace):\n",
        "            self.load_from(hparams)\n",
        "\n",
        "    def add_logger(self,logger):\n",
        "        self.logger=logger\n",
        "\n",
        "    def forward(self,src,trg):\n",
        "        self.transformer(src,trg)\n",
        "\n",
        "    def decode(self,sentence,device):\n",
        "        with torch.no_grad():\n",
        "            tokens = self.inpLang.tokenize(sentence)\n",
        "            src_indexes = self.inpLang.encode(sentence)\n",
        "            src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "            src_mask = self.transformer.make_src_mask(src_tensor)\n",
        "            enc_src = self.transformer.encoder(src_tensor, src_mask)\n",
        "            trg_indexes = [self.optLang.word2idx[self.optLang.special[\"init_token\"]]]\n",
        "            for i in range(hparams.MAX_LEN):\n",
        "                trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "                trg_mask = self.transformer.make_trg_mask(trg_tensor)\n",
        "                output, attention = self.transformer.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "                pred_token = output.argmax(2)[:,-1].item()\n",
        "                trg_indexes.append(pred_token)\n",
        "                if pred_token == self.optLang.word2idx[self.optLang.special[\"eos_token\"]]:\n",
        "                    break\n",
        "            trg_tokens = self.optLang.decode(trg_indexes)\n",
        "        return trg_tokens[1:], attention\n",
        "\n",
        "    def cross_entropy_loss(self, input, target):\n",
        "        return F.cross_entropy(input,target,ignore_index=self.opt_pad_idx)\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        x,y,x_l= train_batch\n",
        "        src=x\n",
        "        trg=y\n",
        "        output,_=self.transformer.forward(src,trg[:,:-1])\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        loss = self.cross_entropy_loss(output,trg)\n",
        "        logs = {'train_loss': loss}\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': logs}\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        x,y,x_l= val_batch\n",
        "        src=x\n",
        "        trg=y\n",
        "        output,_=self.transformer.forward(src,trg[:,:-1])\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        loss = self.cross_entropy_loss(output,trg)\n",
        "        return {'val_loss': loss}\n",
        "\n",
        "    def validation_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        tensorboard_logs = {'val_loss': avg_loss}\n",
        "        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def prepare_data(self):\n",
        "        qg=QGenDataset(squad=self.hparams.SQUAD)\n",
        "        self.train_data,self.val_data,self.test_data,self.inpLang,self.optLang=qg.getData(self.hparams.INPUT_VOCAB,self.hparams.OUTPUT_VOCAB,self.hparams.MAX_LEN,self.hparams.tokenizer,sample=self.hparams.SAMPLE)\n",
        "        self.opt_pad_idx=self.optLang.word2idx[self.optLang.special[\"pad_token\"]]\n",
        "        self.transformer.src_pad_idx=self.inpLang.word2idx[self.inpLang.special[\"pad_token\"]]\n",
        "        self.transformer.trg_pad_idx=self.inpLang.word2idx[self.inpLang.special[\"pad_token\"]]\n",
        "        self.df=pd.DataFrame()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        if self.hparams.usetpu:\n",
        "            sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                self.train_data,\n",
        "                num_replicas=xm.xrt_world_size(),\n",
        "                rank=xm.get_ordinal(),\n",
        "                shuffle=True\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(\n",
        "                self.train_data,\n",
        "                sampler=sampler,\n",
        "                batch_size=self.hparams.BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            return loader\n",
        "        else:\n",
        "            loader = DataLoader(self.train_data, batch_size=self.hparams.BATCH_SIZE,num_workers=10)\n",
        "            return loader\n",
        "\n",
        "    def clean_till_eos(self,sent):\n",
        "        out=[]\n",
        "        for i in sent:\n",
        "            if i ==self.optLang.special[\"eos_token\"]:\n",
        "                break\n",
        "            out.append(i)\n",
        "        return out\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        loader = DataLoader(self.val_data, batch_size=self.hparams.BATCH_SIZE,num_workers=10)\n",
        "        return loader\n",
        "\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        self.logger.experiment.add_text(\"Input\", x[0],global_step=batch_idx)\n",
        "        self.logger.experiment.add_text(\"Target\", y[0], global_step=batch_idx)\n",
        "        dev =torch.device(\"cuda\") if next(self.parameters()).is_cuda else torch.device(\"cpu\")\n",
        "        y=[self.optLang.tokenize(i) for i in y ]\n",
        "        bleu=[]\n",
        "        out=[]\n",
        "        for i,(y_,x_) in enumerate(zip(y,x)):\n",
        "            bleu1,bleu2,bleu3,bleu4=0,0,0,0\n",
        "            o_,_= self.decode(x_,dev)\n",
        "            o_ = self.clean_till_eos(o_)\n",
        "            out.append(o_)\n",
        "            if i==0:\n",
        "                self.logger.experiment.add_text(\"Output\", \" \".join(o_),global_step=batch_idx)\n",
        "            try:\n",
        "                bleu1=torchtext.data.metrics.bleu_score([o_],[[y_]],weights=[1,0,0,0])\n",
        "            except:\n",
        "                print(\"error in input- \",y_)\n",
        "            try:\n",
        "                bleu2=torchtext.data.metrics.bleu_score([o_],[[y_]],weights=[0.5,0.5,0,0])\n",
        "            except:\n",
        "                print(\"error in input- \",y_)\n",
        "            try:\n",
        "                bleu3=torchtext.data.metrics.bleu_score([o_],[[y_]],weights=[0.33,0.33,0.33,0])\n",
        "            except:\n",
        "                print(\"error in input- \",y_)\n",
        "            try:\n",
        "                bleu4=torchtext.data.metrics.bleu_score([o_],[[y_]],weights=[0.25,0.25,0.25,0.25])\n",
        "            except:\n",
        "                print(\"error in input- \",y_)\n",
        "            bleu.append([bleu1,bleu2,bleu3,bleu4])\n",
        "        df=pd.DataFrame({\"Input\":x,\"Target\":y,\"Output\":out,\"BleuScore\":bleu})\n",
        "        self.df=self.df.append(df)\n",
        "        return {'bleu': bleu}\n",
        "    def test_epoch_end(self,output):\n",
        "        total=0\n",
        "        bleu1,bleu2,bleu3,bleu4=0,0,0,0\n",
        "        for i in output:\n",
        "            for batch in i[\"bleu\"]:\n",
        "                total+=1\n",
        "                bleu1+=batch[0]\n",
        "                bleu2+=batch[1]\n",
        "                bleu3+=batch[2]\n",
        "                bleu4+=batch[3]\n",
        "        bleu1=bleu1/total\n",
        "        bleu2=bleu2/total\n",
        "        bleu3=bleu3/total\n",
        "        bleu4=bleu4/total\n",
        "        data={\"BLEU\":{'bleu1':bleu1,'bleu2':bleu2,'bleu3':bleu3,\"belu4\":bleu4},\n",
        "              \"PARAMS\":vars(self.hparams)}\n",
        "        with open(\"./metrics.json\",'w+') as f:\n",
        "            json.dump(data, f)\n",
        "        self.df.to_csv(\"./outputs.csv\")\n",
        "        self.logger.log_metrics({'bleu1':bleu1,'bleu2':bleu2,'bleu3':bleu3,\"belu4\":bleu4})\n",
        "        return {'bleu1':bleu1,'bleu2':bleu2,'bleu3':bleu3,\"belu4\":bleu4}\n",
        "\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        loader = DataLoader(self.test_data, batch_size=self.hparams.BATCH_SIZE)\n",
        "        return loader\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
        "        return optimizer \n",
        "    def save_to(self,path):\n",
        "        if not os.path.exists(path):\n",
        "            os.mkdir(path)\n",
        "        with open(path+\"inpLang.p\",'wb') as f:\n",
        "            pickle.dump(self.inpLang,f)\n",
        "        with open(path+\"optLang.p\",'wb') as f:\n",
        "            pickle.dump(self.optLang,f)\n",
        "        with open(path+\"hparams.p\",'wb') as f:\n",
        "            pickle.dump(self.hparams,f)\n",
        "        with open(path+'hparams.json', 'w') as fp:\n",
        "            json.dump(vars(self.hparams), fp)\n",
        "        torch.save(self.state_dict(),path+\"model.pt\")\n",
        "    def load_from(self,path):\n",
        "        with open(path+\"inpLang.p\",'rb') as f:\n",
        "            self.inpLang=pickle.load(f)\n",
        "        with open(path+\"optLang.p\",'rb') as f:\n",
        "            self.optLang=pickle.load(f)\n",
        "        with open(path+\"optLang.p\",'rb') as f:\n",
        "            self.optLang=pickle.load(f)\n",
        "        self.opt_pad_idx=self.optLang.word2idx[self.optLang.special[\"pad_token\"]]\n",
        "        self.src_pad_idx=self.inpLang.word2idx[self.inpLang.special[\"pad_token\"]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMU73zcuBtgi",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Pytorch Lightning\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "logger = TensorBoardLogger('tb_logs', name=hparams.model_name)\n",
        "trainer = Trainer(gpus=1,gradient_clip_val=1,max_epochs=hparams.EPOCHS,fast_dev_run=False,logger=logger,auto_lr_find=1)\n",
        "\n",
        "model = Seq2seq(hparams)\n",
        "model.add_logger(logger)\n",
        "trainer.fit(model)\n",
        "trainer.test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJNO6FOFBtlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start tensorboard.\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir tb_logs/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTl3KIY6Btqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R4eFOh0Bto0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADeaaUp2Bti3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MetZlrfoITEE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim,\n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        self.device=device\n",
        "        self.tok_embedding=nn.Embedding(input_dim,hid_dim)\n",
        "        self.pos_embedding=nn.Embedding(max_length,hid_dim)\n",
        "\n",
        "        self.layers=nn.ModuleList([EncoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim,\n",
        "                                                  dropout, \n",
        "                                                  device) for _ in range(n_layers)])\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.scale=torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "    def forward(self,src,src_mask):\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, src_mask)\n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ekhvSlgPnIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim,  \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,src,src_mask):\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "        return src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSL1-MxRQqFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "        assert hid_dim % n_heads == 0\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "        \n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        batch_size = query.shape[0]\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale        \n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "        attention = torch.softmax(energy, dim = -1)                \n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "        x = self.fc_o(x)        \n",
        "        return x, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIovKZrGRDG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):        \n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "        x = self.fc_2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjx1OPMARX9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
        "                                                  n_heads, \n",
        "                                                  pf_dim, \n",
        "                                                  dropout, \n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):  \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
        "        for layer in self.layers:\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
        "        output = self.fc_out(trg)\n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7zZqzdOSG6O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 hid_dim, \n",
        "                 n_heads, \n",
        "                 pf_dim, \n",
        "                 dropout, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, \n",
        "                                                                     pf_dim, \n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):  \n",
        "        #trg = [batch size, trg len, hid dim]\n",
        "        #enc_src = [batch size, src len, hid dim]\n",
        "        #trg_mask = [batch size, trg len]\n",
        "        #src_mask = [batch size, src len]\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
        "        _trg = self.positionwise_feedforward(trg)\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
        "        return trg, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv8A4TDkOXUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, \n",
        "                 encoder, \n",
        "                 decoder, \n",
        "                 src_pad_idx, \n",
        "                 trg_pad_idx, \n",
        "                 device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "    def make_src_mask(self, src):        \n",
        "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask\n",
        "    \n",
        "    def make_trg_mask(self, trg):\n",
        "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\n",
        "        return trg_mask\n",
        "\n",
        "    def forward(self, src, trg):\n",
        "        src_mask = self.make_src_mask(src)\n",
        "        trg_mask = self.make_trg_mask(trg)\n",
        "        enc_src = self.encoder(src, src_mask)        \n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCkjhbpAPIOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "device=torch.device(\"cuda\")\n",
        "\n",
        "enc = Encoder(hparams.INPUT_VOCAB, \n",
        "              hparams.HID_DIM, \n",
        "              hparams.ENC_LAYERS, \n",
        "              hparams.ENC_HEADS, \n",
        "              hparams.ENC_PF_DIM, \n",
        "              hparams.ENC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "dec = Decoder(hparams.OUTPUT_VOCAB, \n",
        "              hparams.HID_DIM, \n",
        "              hparams.DEC_LAYERS, \n",
        "              hparams.DEC_HEADS, \n",
        "              hparams.DEC_PF_DIM, \n",
        "              hparams.DEC_DROPOUT, \n",
        "              device)\n",
        "\n",
        "SRC_PAD_IDX = inpLang.word2idx[inpLang.special[\"pad_token\"]]\n",
        "TRG_PAD_IDX = optLang.word2idx[optLang.special[\"pad_token\"]]\n",
        "model = Seq2Seq(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8uX14iCWm4O",
        "colab_type": "code",
        "outputId": "71a6541d-ddae-46d2-fabd-65b965257e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 45,275,728 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO0qB7BvYTG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "model.apply(initialize_weights);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIMpnK4tZBwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATE = 0.0005\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKpSu5BIZEPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8NJfvjlZFt6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def train(model, iterator, optimizer, criterion, clip,exp):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    print(f\"total_batches={len(iterator)}\")\n",
        "    for i, (x,y,_) in enumerate(iterator):\n",
        "        src = x.to(device)\n",
        "        trg = y.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "                \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #trg = [batch size, trg len]\n",
        "            \n",
        "        output_dim = output.shape[-1]\n",
        "            \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        #output = [batch size * trg len - 1, output dim]\n",
        "        #trg = [batch size * trg len - 1]\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if exp!=None:\n",
        "            exp.metric('loss',loss.item())\n",
        "        if i%50==1:\n",
        "            print(f\"Batch {i}, loss{loss.item()}\")\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdZqP-JcZWk2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, iterator, criterion,exp):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i,(x,y,_) in enumerate(iterator):\n",
        "\n",
        "            src = x.to(device)\n",
        "            trg = y.to(device)\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            \n",
        "            #output = [batch size, trg len - 1, output dim]\n",
        "            #trg = [batch size, trg len]\n",
        "            \n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            #output = [batch size * trg len - 1, output dim]\n",
        "            #trg = [batch size * trg len - 1]\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "            if exp!=None:\n",
        "                exp.metric('val loss',loss.item())\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rozk01_lZe1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BKJtWWLZhBk",
        "colab_type": "code",
        "outputId": "067d8b57-2bc9-43a4-e29f-2e04a1b4fe33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N_EPOCHS = hparams.EPOCHS\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "HYPERDASH=True\n",
        "if HYPERDASH==True:\n",
        "    from hyperdash import Experiment\n",
        "    exp = Experiment(\"Transformer Model\")\n",
        "else:\n",
        "    exp=None\n",
        "for epoch in range(N_EPOCHS):\n",
        "    if HYPERDASH:\n",
        "        exp.metric(\"epoch\",epoch)\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP,exp)\n",
        "    valid_loss = evaluate(model, val_dataloader, criterion,exp)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut6-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch:   0.000000 |\n",
            "total_batches=549\n",
            "| loss:  10.603890 |\n",
            "Batch 1, loss10.389145851135254\n",
            "| loss:   9.982836 |\n",
            "| loss:   9.471398 |\n",
            "| loss:   8.899152 |\n",
            "| loss:   8.344980 |\n",
            "| loss:   7.912068 |\n",
            "| loss:   7.522696 |\n",
            "| loss:   7.144539 |\n",
            "| loss:   6.819767 |\n",
            "| loss:   6.703557 |\n",
            "| loss:   6.514258 |\n",
            "| loss:   6.363647 |\n",
            "| loss:   6.262069 |\n",
            "Batch 51, loss6.21600341796875\n",
            "| loss:   6.334122 |\n",
            "| loss:   6.292692 |\n",
            "| loss:   5.979951 |\n",
            "| loss:   5.978721 |\n",
            "| loss:   5.844148 |\n",
            "| loss:   5.852944 |\n",
            "| loss:   5.749522 |\n",
            "| loss:   5.709649 |\n",
            "| loss:   5.485275 |\n",
            "| loss:   5.693063 |\n",
            "| loss:   5.450484 |\n",
            "| loss:   5.695716 |\n",
            "| loss:   5.514398 |\n",
            "Batch 101, loss5.507345199584961\n",
            "| loss:   5.655450 |\n",
            "| loss:   5.502065 |\n",
            "| loss:   5.451263 |\n",
            "| loss:   5.366963 |\n",
            "| loss:   5.469927 |\n",
            "| loss:   5.540291 |\n",
            "| loss:   5.446814 |\n",
            "| loss:   5.280555 |\n",
            "| loss:   5.407705 |\n",
            "| loss:   5.347281 |\n",
            "| loss:   5.277894 |\n",
            "| loss:   5.271786 |\n",
            "Batch 151, loss5.241090774536133\n",
            "| loss:   5.201852 |\n",
            "| loss:   5.212903 |\n",
            "| loss:   5.245591 |\n",
            "| loss:   5.224899 |\n",
            "| loss:   5.230129 |\n",
            "| loss:   5.190251 |\n",
            "| loss:   5.163907 |\n",
            "| loss:   5.246799 |\n",
            "| loss:   5.214653 |\n",
            "| loss:   5.079648 |\n",
            "| loss:   5.042467 |\n",
            "| loss:   5.260589 |\n",
            "| loss:   5.223969 |\n",
            "Batch 201, loss4.987985134124756\n",
            "| loss:   4.982630 |\n",
            "| loss:   5.199698 |\n",
            "| loss:   5.207515 |\n",
            "| loss:   5.061024 |\n",
            "| loss:   5.112495 |\n",
            "| loss:   5.104922 |\n",
            "| loss:   4.931817 |\n",
            "| loss:   5.015332 |\n",
            "| loss:   5.028011 |\n",
            "| loss:   5.081432 |\n",
            "| loss:   5.013200 |\n",
            "| loss:   5.130805 |\n",
            "Batch 251, loss5.089493751525879\n",
            "| loss:   5.159895 |\n",
            "| loss:   5.080142 |\n",
            "| loss:   5.009922 |\n",
            "| loss:   5.069815 |\n",
            "| loss:   4.974233 |\n",
            "| loss:   5.034907 |\n",
            "| loss:   4.895248 |\n",
            "| loss:   5.012843 |\n",
            "| loss:   5.058033 |\n",
            "| loss:   5.130808 |\n",
            "| loss:   5.060861 |\n",
            "| loss:   5.018046 |\n",
            "| loss:   5.002316 |\n",
            "Batch 301, loss4.900757312774658\n",
            "| loss:   5.100464 |\n",
            "| loss:   4.934702 |\n",
            "| loss:   4.918883 |\n",
            "| loss:   4.918740 |\n",
            "| loss:   4.836804 |\n",
            "| loss:   5.033841 |\n",
            "| loss:   4.883042 |\n",
            "| loss:   4.859676 |\n",
            "| loss:   5.028906 |\n",
            "| loss:   4.966238 |\n",
            "| loss:   5.016104 |\n",
            "| loss:   4.917801 |\n",
            "Batch 351, loss4.962775707244873\n",
            "| loss:   4.964285 |\n",
            "| loss:   4.842755 |\n",
            "| loss:   4.947886 |\n",
            "| loss:   4.991256 |\n",
            "| loss:   4.875526 |\n",
            "| loss:   4.991601 |\n",
            "| loss:   4.974291 |\n",
            "| loss:   4.884714 |\n",
            "| loss:   4.796450 |\n",
            "| loss:   4.663356 |\n",
            "| loss:   4.783381 |\n",
            "| loss:   4.984908 |\n",
            "| loss:   4.693069 |\n",
            "Batch 401, loss4.8260016441345215\n",
            "| loss:   4.731408 |\n",
            "| loss:   4.753183 |\n",
            "| loss:   4.818130 |\n",
            "| loss:   4.856727 |\n",
            "| loss:   4.769899 |\n",
            "| loss:   4.696358 |\n",
            "| loss:   4.580936 |\n",
            "| loss:   4.963022 |\n",
            "| loss:   4.861916 |\n",
            "| loss:   4.738717 |\n",
            "| loss:   4.748842 |\n",
            "| loss:   4.775896 |\n",
            "Batch 451, loss4.790094375610352\n",
            "| loss:   4.737475 |\n",
            "| loss:   4.657685 |\n",
            "| loss:   4.817731 |\n",
            "| loss:   4.859201 |\n",
            "| loss:   4.699827 |\n",
            "| loss:   4.758703 |\n",
            "| loss:   4.861122 |\n",
            "| loss:   4.689373 |\n",
            "| loss:   4.602448 |\n",
            "| loss:   4.705556 |\n",
            "| loss:   4.642467 |\n",
            "| loss:   4.745134 |\n",
            "| loss:   4.628633 |\n",
            "Batch 501, loss4.75575590133667\n",
            "| loss:   4.601382 |\n",
            "| loss:   4.675921 |\n",
            "| loss:   4.777442 |\n",
            "| loss:   4.761815 |\n",
            "| loss:   4.546721 |\n",
            "| loss:   4.653776 |\n",
            "| loss:   4.712836 |\n",
            "| loss:   4.697434 |\n",
            "| loss:   4.819688 |\n",
            "| loss:   4.659485 |\n",
            "| loss:   4.580152 |\n",
            "| loss:   4.543975 |\n",
            "| val loss:   4.603117 |\n",
            "| val loss:   4.600968 |\n",
            "| val loss:   4.475823 |\n",
            "| val loss:   4.649297 |\n",
            "| val loss:   4.504885 |\n",
            "| val loss:   4.683854 |\n",
            "Epoch: 01 | Time: 3m 1s\n",
            "\tTrain Loss: 5.315 | Train PPL: 203.399\n",
            "\t Val. Loss: 4.632 |  Val. PPL: 102.757\n",
            "| epoch:   1.000000 |\n",
            "total_batches=549\n",
            "| loss:   4.429338 |\n",
            "Batch 1, loss4.48621940612793\n",
            "| loss:   4.575778 |\n",
            "| loss:   4.693116 |\n",
            "| loss:   4.644131 |\n",
            "| loss:   4.507468 |\n",
            "| loss:   4.626554 |\n",
            "| loss:   4.684351 |\n",
            "| loss:   4.542567 |\n",
            "| loss:   4.423944 |\n",
            "| loss:   4.565844 |\n",
            "| loss:   4.536635 |\n",
            "| loss:   4.483419 |\n",
            "| loss:   4.455745 |\n",
            "Batch 51, loss4.506206035614014\n",
            "| loss:   4.647982 |\n",
            "| loss:   4.673507 |\n",
            "| loss:   4.399125 |\n",
            "| loss:   4.504965 |\n",
            "| loss:   4.490119 |\n",
            "| loss:   4.517326 |\n",
            "| loss:   4.398259 |\n",
            "| loss:   4.402550 |\n",
            "| loss:   4.276102 |\n",
            "| loss:   4.513460 |\n",
            "| loss:   4.263970 |\n",
            "| loss:   4.568052 |\n",
            "| loss:   4.449214 |\n",
            "Batch 101, loss4.386236190795898\n",
            "| loss:   4.505769 |\n",
            "| loss:   4.430214 |\n",
            "| loss:   4.420952 |\n",
            "| loss:   4.373155 |\n",
            "| loss:   4.411250 |\n",
            "| loss:   4.497200 |\n",
            "| loss:   4.473557 |\n",
            "| loss:   4.281659 |\n",
            "| loss:   4.387870 |\n",
            "| loss:   4.405722 |\n",
            "| loss:   4.336895 |\n",
            "| loss:   4.300579 |\n",
            "Batch 151, loss4.303870677947998\n",
            "| loss:   4.318371 |\n",
            "| loss:   4.275905 |\n",
            "| loss:   4.319551 |\n",
            "| loss:   4.377213 |\n",
            "| loss:   4.317570 |\n",
            "| loss:   4.285634 |\n",
            "| loss:   4.339226 |\n",
            "| loss:   4.358935 |\n",
            "| loss:   4.354625 |\n",
            "| loss:   4.241980 |\n",
            "| loss:   4.151260 |\n",
            "| loss:   4.376548 |\n",
            "| loss:   4.354563 |\n",
            "Batch 201, loss4.166504383087158\n",
            "| loss:   4.126298 |\n",
            "| loss:   4.363574 |\n",
            "| loss:   4.380450 |\n",
            "| loss:   4.221957 |\n",
            "| loss:   4.274640 |\n",
            "| loss:   4.252047 |\n",
            "| loss:   4.092456 |\n",
            "| loss:   4.257221 |\n",
            "| loss:   4.235885 |\n",
            "| loss:   4.327775 |\n",
            "| loss:   4.225880 |\n",
            "| loss:   4.351010 |\n",
            "Batch 251, loss4.330352306365967\n",
            "| loss:   4.380349 |\n",
            "| loss:   4.320199 |\n",
            "| loss:   4.217827 |\n",
            "| loss:   4.328029 |\n",
            "| loss:   4.172313 |\n",
            "| loss:   4.223374 |\n",
            "| loss:   4.130764 |\n",
            "| loss:   4.222567 |\n",
            "| loss:   4.288298 |\n",
            "| loss:   4.319231 |\n",
            "| loss:   4.329976 |\n",
            "| loss:   4.324335 |\n",
            "| loss:   4.253389 |\n",
            "Batch 301, loss4.115421295166016\n",
            "| loss:   4.329236 |\n",
            "| loss:   4.172049 |\n",
            "| loss:   4.147399 |\n",
            "| loss:   4.216041 |\n",
            "| loss:   4.149662 |\n",
            "| loss:   4.286732 |\n",
            "| loss:   4.207489 |\n",
            "| loss:   4.077813 |\n",
            "| loss:   4.313767 |\n",
            "| loss:   4.252975 |\n",
            "| loss:   4.274882 |\n",
            "| loss:   4.204123 |\n",
            "Batch 351, loss4.274411201477051\n",
            "| loss:   4.230634 |\n",
            "| loss:   4.132783 |\n",
            "| loss:   4.167902 |\n",
            "| loss:   4.275995 |\n",
            "| loss:   4.176847 |\n",
            "| loss:   4.288499 |\n",
            "| loss:   4.249431 |\n",
            "| loss:   4.160601 |\n",
            "| loss:   4.076834 |\n",
            "| loss:   3.922792 |\n",
            "| loss:   4.107887 |\n",
            "| loss:   4.262595 |\n",
            "| loss:   3.994989 |\n",
            "Batch 401, loss4.163563251495361\n",
            "| loss:   4.011607 |\n",
            "| loss:   4.056038 |\n",
            "| loss:   4.105549 |\n",
            "| loss:   4.179592 |\n",
            "| loss:   4.091142 |\n",
            "| loss:   4.005429 |\n",
            "| loss:   3.912329 |\n",
            "| loss:   4.296170 |\n",
            "| loss:   4.175930 |\n",
            "| loss:   4.077389 |\n",
            "| loss:   4.006108 |\n",
            "| loss:   4.059042 |\n",
            "Batch 451, loss4.092415809631348\n",
            "| loss:   4.066431 |\n",
            "| loss:   4.032454 |\n",
            "| loss:   4.139702 |\n",
            "| loss:   4.234109 |\n",
            "| loss:   4.043231 |\n",
            "| loss:   4.078532 |\n",
            "| loss:   4.195601 |\n",
            "| loss:   4.064690 |\n",
            "| loss:   3.996685 |\n",
            "| loss:   4.069537 |\n",
            "| loss:   4.007629 |\n",
            "| loss:   4.116746 |\n",
            "| loss:   3.959041 |\n",
            "Batch 501, loss4.0905585289001465\n",
            "| loss:   3.972685 |\n",
            "| loss:   4.084952 |\n",
            "| loss:   4.154479 |\n",
            "| loss:   4.116854 |\n",
            "| loss:   3.896017 |\n",
            "| loss:   4.056614 |\n",
            "| loss:   4.055155 |\n",
            "| loss:   4.051920 |\n",
            "| loss:   4.135247 |\n",
            "| loss:   4.034538 |\n",
            "| loss:   4.030046 |\n",
            "| loss:   3.731431 |\n",
            "| val loss:   4.243974 |\n",
            "| val loss:   4.251290 |\n",
            "| val loss:   4.110647 |\n",
            "| val loss:   4.368392 |\n",
            "| val loss:   4.245852 |\n",
            "| val loss:   4.335556 |\n",
            "Epoch: 02 | Time: 3m 1s\n",
            "\tTrain Loss: 4.266 | Train PPL:  71.247\n",
            "\t Val. Loss: 4.298 |  Val. PPL:  73.539\n",
            "| epoch:   2.000000 |\n",
            "total_batches=549\n",
            "| loss:   3.886458 |\n",
            "Batch 1, loss3.9318501949310303\n",
            "| loss:   3.971625 |\n",
            "| loss:   4.029837 |\n",
            "| loss:   3.999348 |\n",
            "| loss:   3.879748 |\n",
            "| loss:   4.035357 |\n",
            "| loss:   4.083025 |\n",
            "| loss:   3.953281 |\n",
            "| loss:   3.872892 |\n",
            "| loss:   3.951467 |\n",
            "| loss:   3.969231 |\n",
            "| loss:   3.922379 |\n",
            "| loss:   3.865842 |\n",
            "Batch 51, loss3.977557897567749\n",
            "| loss:   4.076732 |\n",
            "| loss:   4.068333 |\n",
            "| loss:   3.882832 |\n",
            "| loss:   3.902477 |\n",
            "| loss:   3.940517 |\n",
            "| loss:   3.939235 |\n",
            "| loss:   3.828451 |\n",
            "| loss:   3.843339 |\n",
            "| loss:   3.694495 |\n",
            "| loss:   3.960682 |\n",
            "| loss:   3.699558 |\n",
            "| loss:   3.995463 |\n",
            "| loss:   3.857132 |\n",
            "Batch 101, loss3.8422274589538574\n",
            "| loss:   3.989902 |\n",
            "| loss:   3.853015 |\n",
            "| loss:   3.884693 |\n",
            "| loss:   3.854083 |\n",
            "| loss:   3.828454 |\n",
            "| loss:   3.960473 |\n",
            "| loss:   3.888778 |\n",
            "| loss:   3.776729 |\n",
            "| loss:   3.860785 |\n",
            "| loss:   3.857418 |\n",
            "| loss:   3.797241 |\n",
            "| loss:   3.759949 |\n",
            "Batch 151, loss3.779252767562866\n",
            "| loss:   3.808728 |\n",
            "| loss:   3.744521 |\n",
            "| loss:   3.807995 |\n",
            "| loss:   3.834493 |\n",
            "| loss:   3.777083 |\n",
            "| loss:   3.752651 |\n",
            "| loss:   3.789577 |\n",
            "| loss:   3.833215 |\n",
            "| loss:   3.797588 |\n",
            "| loss:   3.754213 |\n",
            "| loss:   3.696990 |\n",
            "| loss:   3.849788 |\n",
            "| loss:   3.792192 |\n",
            "Batch 201, loss3.6879656314849854\n",
            "| loss:   3.680372 |\n",
            "| loss:   3.847393 |\n",
            "| loss:   3.824585 |\n",
            "| loss:   3.704413 |\n",
            "| loss:   3.757671 |\n",
            "| loss:   3.728906 |\n",
            "| loss:   3.524022 |\n",
            "| loss:   3.758971 |\n",
            "| loss:   3.768257 |\n",
            "| loss:   3.846527 |\n",
            "| loss:   3.706584 |\n",
            "| loss:   3.847677 |\n",
            "Batch 251, loss3.847313404083252\n",
            "| loss:   3.861497 |\n",
            "| loss:   3.815362 |\n",
            "| loss:   3.655586 |\n",
            "| loss:   3.803157 |\n",
            "| loss:   3.683227 |\n",
            "| loss:   3.696723 |\n",
            "| loss:   3.631471 |\n",
            "| loss:   3.692248 |\n",
            "| loss:   3.776979 |\n",
            "| loss:   3.777610 |\n",
            "| loss:   3.806742 |\n",
            "| loss:   3.859430 |\n",
            "| loss:   3.759159 |\n",
            "Batch 301, loss3.621917724609375\n",
            "| loss:   3.855550 |\n",
            "| loss:   3.711086 |\n",
            "| loss:   3.652128 |\n",
            "| loss:   3.746691 |\n",
            "| loss:   3.633787 |\n",
            "| loss:   3.789725 |\n",
            "| loss:   3.702707 |\n",
            "| loss:   3.581188 |\n",
            "| loss:   3.768568 |\n",
            "| loss:   3.719541 |\n",
            "| loss:   3.737350 |\n",
            "| loss:   3.720078 |\n",
            "Batch 351, loss3.775275468826294\n",
            "| loss:   3.711325 |\n",
            "| loss:   3.688382 |\n",
            "| loss:   3.694016 |\n",
            "| loss:   3.753723 |\n",
            "| loss:   3.701916 |\n",
            "| loss:   3.772387 |\n",
            "| loss:   3.740610 |\n",
            "| loss:   3.636650 |\n",
            "| loss:   3.570129 |\n",
            "| loss:   3.405790 |\n",
            "| loss:   3.612988 |\n",
            "| loss:   3.799947 |\n",
            "| loss:   3.492082 |\n",
            "Batch 401, loss3.632941484451294\n",
            "| loss:   3.541334 |\n",
            "| loss:   3.557220 |\n",
            "| loss:   3.575392 |\n",
            "| loss:   3.699530 |\n",
            "| loss:   3.650190 |\n",
            "| loss:   3.521737 |\n",
            "| loss:   3.448896 |\n",
            "| loss:   3.831070 |\n",
            "| loss:   3.678096 |\n",
            "| loss:   3.594916 |\n",
            "| loss:   3.515608 |\n",
            "| loss:   3.555643 |\n",
            "Batch 451, loss3.5794880390167236\n",
            "| loss:   3.576267 |\n",
            "| loss:   3.564734 |\n",
            "| loss:   3.655404 |\n",
            "| loss:   3.761743 |\n",
            "| loss:   3.589844 |\n",
            "| loss:   3.572817 |\n",
            "| loss:   3.742463 |\n",
            "| loss:   3.580212 |\n",
            "| loss:   3.551915 |\n",
            "| loss:   3.613008 |\n",
            "| loss:   3.506601 |\n",
            "| loss:   3.686113 |\n",
            "| loss:   3.481980 |\n",
            "Batch 501, loss3.5948543548583984\n",
            "| loss:   3.516666 |\n",
            "| loss:   3.624591 |\n",
            "| loss:   3.633542 |\n",
            "| loss:   3.685568 |\n",
            "| loss:   3.438023 |\n",
            "| loss:   3.591475 |\n",
            "| loss:   3.561540 |\n",
            "| loss:   3.598986 |\n",
            "| loss:   3.638383 |\n",
            "| loss:   3.601051 |\n",
            "| loss:   3.589843 |\n",
            "| loss:   3.054729 |\n",
            "| val loss:   4.093894 |\n",
            "| val loss:   4.118979 |\n",
            "| val loss:   3.971719 |\n",
            "| val loss:   4.295891 |\n",
            "| val loss:   4.026017 |\n",
            "| val loss:   4.185229 |\n",
            "Epoch: 03 | Time: 3m 1s\n",
            "\tTrain Loss: 3.750 | Train PPL:  42.521\n",
            "\t Val. Loss: 4.177 |  Val. PPL:  65.194\n",
            "| epoch:   3.000000 |\n",
            "total_batches=549\n",
            "| loss:   3.454275 |\n",
            "Batch 1, loss3.5082736015319824\n",
            "| loss:   3.474014 |\n",
            "| loss:   3.551339 |\n",
            "| loss:   3.558058 |\n",
            "| loss:   3.372440 |\n",
            "| loss:   3.590443 |\n",
            "| loss:   3.603684 |\n",
            "| loss:   3.498708 |\n",
            "| loss:   3.447767 |\n",
            "| loss:   3.479253 |\n",
            "| loss:   3.530193 |\n",
            "| loss:   3.487394 |\n",
            "| loss:   3.393936 |\n",
            "Batch 51, loss3.5135395526885986\n",
            "| loss:   3.632235 |\n",
            "| loss:   3.588888 |\n",
            "| loss:   3.439896 |\n",
            "| loss:   3.504156 |\n",
            "| loss:   3.503577 |\n",
            "| loss:   3.491778 |\n",
            "| loss:   3.388724 |\n",
            "| loss:   3.416804 |\n",
            "| loss:   3.249187 |\n",
            "| loss:   3.540750 |\n",
            "| loss:   3.254441 |\n",
            "| loss:   3.521476 |\n",
            "| loss:   3.378144 |\n",
            "Batch 101, loss3.4153757095336914\n",
            "| loss:   3.475278 |\n",
            "| loss:   3.352122 |\n",
            "| loss:   3.442719 |\n",
            "| loss:   3.456984 |\n",
            "| loss:   3.405810 |\n",
            "| loss:   3.494455 |\n",
            "| loss:   3.450206 |\n",
            "| loss:   3.372016 |\n",
            "| loss:   3.438641 |\n",
            "| loss:   3.422501 |\n",
            "| loss:   3.363468 |\n",
            "| loss:   3.335070 |\n",
            "Batch 151, loss3.3742563724517822\n",
            "| loss:   3.399169 |\n",
            "| loss:   3.362114 |\n",
            "| loss:   3.439660 |\n",
            "| loss:   3.432169 |\n",
            "| loss:   3.373145 |\n",
            "| loss:   3.327390 |\n",
            "| loss:   3.375726 |\n",
            "| loss:   3.403886 |\n",
            "| loss:   3.353635 |\n",
            "| loss:   3.357890 |\n",
            "| loss:   3.292505 |\n",
            "| loss:   3.418641 |\n",
            "| loss:   3.375944 |\n",
            "Batch 201, loss3.243062734603882\n",
            "| loss:   3.257222 |\n",
            "| loss:   3.488118 |\n",
            "| loss:   3.409487 |\n",
            "| loss:   3.323168 |\n",
            "| loss:   3.336311 |\n",
            "| loss:   3.336624 |\n",
            "| loss:   3.071744 |\n",
            "| loss:   3.347860 |\n",
            "| loss:   3.405872 |\n",
            "| loss:   3.434870 |\n",
            "| loss:   3.277386 |\n",
            "| loss:   3.400739 |\n",
            "Batch 251, loss3.4607625007629395\n",
            "| loss:   3.479439 |\n",
            "| loss:   3.399281 |\n",
            "| loss:   3.259312 |\n",
            "| loss:   3.416750 |\n",
            "| loss:   3.282142 |\n",
            "| loss:   3.292020 |\n",
            "| loss:   3.239734 |\n",
            "| loss:   3.285677 |\n",
            "| loss:   3.341301 |\n",
            "| loss:   3.364535 |\n",
            "| loss:   3.416651 |\n",
            "| loss:   3.424802 |\n",
            "| loss:   3.349699 |\n",
            "Batch 301, loss3.1980888843536377\n",
            "| loss:   3.476702 |\n",
            "| loss:   3.320672 |\n",
            "| loss:   3.270833 |\n",
            "| loss:   3.355389 |\n",
            "| loss:   3.223508 |\n",
            "| loss:   3.349453 |\n",
            "| loss:   3.294489 |\n",
            "| loss:   3.176574 |\n",
            "| loss:   3.316999 |\n",
            "| loss:   3.322171 |\n",
            "| loss:   3.318178 |\n",
            "| loss:   3.295691 |\n",
            "Batch 351, loss3.41723895072937\n",
            "| loss:   3.275069 |\n",
            "| loss:   3.267306 |\n",
            "| loss:   3.275489 |\n",
            "| loss:   3.334931 |\n",
            "| loss:   3.293685 |\n",
            "| loss:   3.351670 |\n",
            "| loss:   3.284744 |\n",
            "| loss:   3.235447 |\n",
            "| loss:   3.180874 |\n",
            "| loss:   2.998448 |\n",
            "| loss:   3.194833 |\n",
            "| loss:   3.416326 |\n",
            "| loss:   3.124964 |\n",
            "Batch 401, loss3.2589168548583984\n",
            "| loss:   3.185879 |\n",
            "| loss:   3.131325 |\n",
            "| loss:   3.152394 |\n",
            "| loss:   3.279111 |\n",
            "| loss:   3.265857 |\n",
            "| loss:   3.096541 |\n",
            "| loss:   3.082055 |\n",
            "| loss:   3.402753 |\n",
            "| loss:   3.296021 |\n",
            "| loss:   3.208101 |\n",
            "| loss:   3.084599 |\n",
            "| loss:   3.138225 |\n",
            "Batch 451, loss3.1541380882263184\n",
            "| loss:   3.162861 |\n",
            "| loss:   3.205690 |\n",
            "| loss:   3.229186 |\n",
            "| loss:   3.330853 |\n",
            "| loss:   3.199685 |\n",
            "| loss:   3.148066 |\n",
            "| loss:   3.344638 |\n",
            "| loss:   3.207476 |\n",
            "| loss:   3.180083 |\n",
            "| loss:   3.212277 |\n",
            "| loss:   3.089066 |\n",
            "| loss:   3.289431 |\n",
            "| loss:   3.104416 |\n",
            "Batch 501, loss3.2186732292175293\n",
            "| loss:   3.113850 |\n",
            "| loss:   3.282053 |\n",
            "| loss:   3.242037 |\n",
            "| loss:   3.294667 |\n",
            "| loss:   3.044410 |\n",
            "| loss:   3.181816 |\n",
            "| loss:   3.175875 |\n",
            "| loss:   3.182176 |\n",
            "| loss:   3.239129 |\n",
            "| loss:   3.233727 |\n",
            "| loss:   3.226956 |\n",
            "| loss:   2.465858 |\n",
            "| val loss:   4.074311 |\n",
            "| val loss:   4.126250 |\n",
            "| val loss:   3.952632 |\n",
            "| val loss:   4.200211 |\n",
            "| val loss:   4.146081 |\n",
            "| val loss:   4.207935 |\n",
            "Epoch: 04 | Time: 3m 1s\n",
            "\tTrain Loss: 3.337 | Train PPL:  28.124\n",
            "\t Val. Loss: 4.189 |  Val. PPL:  65.935\n",
            "| epoch:   4.000000 |\n",
            "total_batches=549\n",
            "| loss:   3.089258 |\n",
            "Batch 1, loss3.166546106338501\n",
            "| loss:   3.068099 |\n",
            "| loss:   3.111993 |\n",
            "| loss:   3.180378 |\n",
            "| loss:   3.020174 |\n",
            "| loss:   3.169946 |\n",
            "| loss:   3.227283 |\n",
            "| loss:   3.096691 |\n",
            "| loss:   3.043611 |\n",
            "| loss:   3.047163 |\n",
            "| loss:   3.167308 |\n",
            "| loss:   3.142516 |\n",
            "| loss:   3.042844 |\n",
            "Batch 51, loss3.1143710613250732\n",
            "| loss:   3.274454 |\n",
            "| loss:   3.216439 |\n",
            "| loss:   3.066165 |\n",
            "| loss:   3.118942 |\n",
            "| loss:   3.113123 |\n",
            "| loss:   3.099739 |\n",
            "| loss:   3.031648 |\n",
            "| loss:   3.048342 |\n",
            "| loss:   2.908765 |\n",
            "| loss:   3.187000 |\n",
            "| loss:   2.866464 |\n",
            "| loss:   3.147656 |\n",
            "| loss:   2.986686 |\n",
            "Batch 101, loss3.0520949363708496\n",
            "| loss:   3.097687 |\n",
            "| loss:   2.898775 |\n",
            "| loss:   3.101893 |\n",
            "| loss:   3.172365 |\n",
            "| loss:   3.006687 |\n",
            "| loss:   3.079586 |\n",
            "| loss:   3.082211 |\n",
            "| loss:   3.017658 |\n",
            "| loss:   3.046931 |\n",
            "| loss:   3.024685 |\n",
            "| loss:   2.982444 |\n",
            "| loss:   2.976205 |\n",
            "Batch 151, loss3.0020172595977783\n",
            "| loss:   3.051775 |\n",
            "| loss:   3.088612 |\n",
            "| loss:   3.102156 |\n",
            "| loss:   3.110209 |\n",
            "| loss:   2.967417 |\n",
            "| loss:   2.947760 |\n",
            "| loss:   3.030256 |\n",
            "| loss:   3.019370 |\n",
            "| loss:   2.977147 |\n",
            "| loss:   2.995792 |\n",
            "| loss:   2.902928 |\n",
            "| loss:   3.003216 |\n",
            "| loss:   3.017236 |\n",
            "Batch 201, loss2.8738646507263184\n",
            "| loss:   2.930651 |\n",
            "| loss:   3.125134 |\n",
            "| loss:   3.066134 |\n",
            "| loss:   2.946416 |\n",
            "| loss:   2.958486 |\n",
            "| loss:   2.977598 |\n",
            "| loss:   2.702841 |\n",
            "| loss:   2.971445 |\n",
            "| loss:   3.054560 |\n",
            "| loss:   3.055497 |\n",
            "| loss:   2.898976 |\n",
            "| loss:   3.009503 |\n",
            "Batch 251, loss3.1137325763702393\n",
            "| loss:   3.098948 |\n",
            "| loss:   3.069973 |\n",
            "| loss:   2.918458 |\n",
            "| loss:   3.020549 |\n",
            "| loss:   2.919733 |\n",
            "| loss:   2.899448 |\n",
            "| loss:   2.896764 |\n",
            "| loss:   2.878775 |\n",
            "| loss:   2.956898 |\n",
            "| loss:   2.981683 |\n",
            "| loss:   3.027402 |\n",
            "| loss:   3.065989 |\n",
            "| loss:   2.935290 |\n",
            "Batch 301, loss2.8179268836975098\n",
            "| loss:   3.104970 |\n",
            "| loss:   3.000918 |\n",
            "| loss:   2.922477 |\n",
            "| loss:   3.027302 |\n",
            "| loss:   2.891529 |\n",
            "| loss:   2.946088 |\n",
            "| loss:   2.961354 |\n",
            "| loss:   2.804245 |\n",
            "| loss:   2.952162 |\n",
            "| loss:   2.961158 |\n",
            "| loss:   2.936268 |\n",
            "| loss:   2.925849 |\n",
            "Batch 351, loss3.0063202381134033\n",
            "| loss:   2.917808 |\n",
            "| loss:   2.891922 |\n",
            "| loss:   2.924439 |\n",
            "| loss:   2.911536 |\n",
            "| loss:   2.884764 |\n",
            "| loss:   2.984061 |\n",
            "| loss:   2.934895 |\n",
            "| loss:   2.915215 |\n",
            "| loss:   2.780017 |\n",
            "| loss:   2.648943 |\n",
            "| loss:   2.862745 |\n",
            "| loss:   3.044710 |\n",
            "| loss:   2.702079 |\n",
            "Batch 401, loss2.8630948066711426\n",
            "| loss:   2.841531 |\n",
            "| loss:   2.767039 |\n",
            "| loss:   2.767143 |\n",
            "| loss:   2.939504 |\n",
            "| loss:   2.933507 |\n",
            "| loss:   2.741642 |\n",
            "| loss:   2.779169 |\n",
            "| loss:   2.992039 |\n",
            "| loss:   2.880249 |\n",
            "| loss:   2.849238 |\n",
            "| loss:   2.766315 |\n",
            "| loss:   2.757391 |\n",
            "Batch 451, loss2.779193639755249\n",
            "| loss:   2.824769 |\n",
            "| loss:   2.853355 |\n",
            "| loss:   2.859653 |\n",
            "| loss:   3.017324 |\n",
            "| loss:   2.825899 |\n",
            "| loss:   2.794479 |\n",
            "| loss:   2.974805 |\n",
            "| loss:   2.850323 |\n",
            "| loss:   2.830228 |\n",
            "| loss:   2.866485 |\n",
            "| loss:   2.792525 |\n",
            "| loss:   2.917718 |\n",
            "| loss:   2.754493 |\n",
            "Batch 501, loss2.8329615592956543\n",
            "| loss:   2.718692 |\n",
            "| loss:   2.924719 |\n",
            "| loss:   2.910594 |\n",
            "| loss:   2.907261 |\n",
            "| loss:   2.722681 |\n",
            "| loss:   2.818057 |\n",
            "| loss:   2.801383 |\n",
            "| loss:   2.840829 |\n",
            "| loss:   2.902044 |\n",
            "| loss:   2.926873 |\n",
            "| loss:   2.928586 |\n",
            "| loss:   1.948537 |\n",
            "| val loss:   4.107063 |\n",
            "| val loss:   4.197020 |\n",
            "| val loss:   3.995817 |\n",
            "| val loss:   4.403788 |\n",
            "| val loss:   4.079353 |\n",
            "| val loss:   4.259978 |\n",
            "Epoch: 05 | Time: 3m 1s\n",
            "\tTrain Loss: 2.971 | Train PPL:  19.508\n",
            "\t Val. Loss: 4.249 |  Val. PPL:  70.035\n",
            "| epoch:   5.000000 |\n",
            "total_batches=549\n",
            "| loss:   2.771063 |\n",
            "Batch 1, loss2.804913282394409\n",
            "| loss:   2.711543 |\n",
            "| loss:   2.777487 |\n",
            "| loss:   2.780892 |\n",
            "| loss:   2.654343 |\n",
            "| loss:   2.841642 |\n",
            "| loss:   2.923715 |\n",
            "| loss:   2.744555 |\n",
            "| loss:   2.717112 |\n",
            "| loss:   2.660405 |\n",
            "| loss:   2.802435 |\n",
            "| loss:   2.831149 |\n",
            "| loss:   2.720211 |\n",
            "Batch 51, loss2.753659248352051\n",
            "| loss:   2.923751 |\n",
            "| loss:   2.872341 |\n",
            "| loss:   2.728516 |\n",
            "| loss:   2.828182 |\n",
            "| loss:   2.800687 |\n",
            "| loss:   2.778462 |\n",
            "| loss:   2.694298 |\n",
            "| loss:   2.737149 |\n",
            "| loss:   2.588923 |\n",
            "| loss:   2.783659 |\n",
            "| loss:   2.528827 |\n",
            "| loss:   2.787828 |\n",
            "| loss:   2.584394 |\n",
            "Batch 101, loss2.7519781589508057\n",
            "| loss:   2.751568 |\n",
            "| loss:   2.507075 |\n",
            "| loss:   2.766423 |\n",
            "| loss:   2.801934 |\n",
            "| loss:   2.643874 |\n",
            "| loss:   2.791219 |\n",
            "| loss:   2.708097 |\n",
            "| loss:   2.656520 |\n",
            "| loss:   2.755680 |\n",
            "| loss:   2.677395 |\n",
            "| loss:   2.664474 |\n",
            "| loss:   2.630650 |\n",
            "Batch 151, loss2.7006125450134277\n",
            "| loss:   2.688385 |\n",
            "| loss:   2.740944 |\n",
            "| loss:   2.800246 |\n",
            "| loss:   2.724353 |\n",
            "| loss:   2.650203 |\n",
            "| loss:   2.650234 |\n",
            "| loss:   2.685391 |\n",
            "| loss:   2.684107 |\n",
            "| loss:   2.671902 |\n",
            "| loss:   2.632329 |\n",
            "| loss:   2.559411 |\n",
            "| loss:   2.669165 |\n",
            "| loss:   2.669031 |\n",
            "Batch 201, loss2.519993782043457\n",
            "| loss:   2.616755 |\n",
            "| loss:   2.740378 |\n",
            "| loss:   2.737354 |\n",
            "| loss:   2.654202 |\n",
            "| loss:   2.591865 |\n",
            "| loss:   2.622885 |\n",
            "| loss:   2.369208 |\n",
            "| loss:   2.625199 |\n",
            "| loss:   2.726227 |\n",
            "| loss:   2.710283 |\n",
            "| loss:   2.543144 |\n",
            "| loss:   2.680225 |\n",
            "Batch 251, loss2.74112606048584\n",
            "| loss:   2.730840 |\n",
            "| loss:   2.707636 |\n",
            "| loss:   2.570976 |\n",
            "| loss:   2.684782 |\n",
            "| loss:   2.585220 |\n",
            "| loss:   2.549723 |\n",
            "| loss:   2.610735 |\n",
            "| loss:   2.582590 |\n",
            "| loss:   2.583655 |\n",
            "| loss:   2.650999 |\n",
            "| loss:   2.701956 |\n",
            "| loss:   2.747513 |\n",
            "| loss:   2.596457 |\n",
            "Batch 301, loss2.42464017868042\n",
            "| loss:   2.764752 |\n",
            "| loss:   2.669738 |\n",
            "| loss:   2.592270 |\n",
            "| loss:   2.694597 |\n",
            "| loss:   2.570761 |\n",
            "| loss:   2.614100 |\n",
            "| loss:   2.639615 |\n",
            "| loss:   2.466825 |\n",
            "| loss:   2.594804 |\n",
            "| loss:   2.605510 |\n",
            "| loss:   2.587023 |\n",
            "| loss:   2.620973 |\n",
            "Batch 351, loss2.6607935428619385\n",
            "| loss:   2.570676 |\n",
            "| loss:   2.567586 |\n",
            "| loss:   2.618195 |\n",
            "| loss:   2.613683 |\n",
            "| loss:   2.536413 |\n",
            "| loss:   2.645432 |\n",
            "| loss:   2.626697 |\n",
            "| loss:   2.505369 |\n",
            "| loss:   2.451298 |\n",
            "| loss:   2.296124 |\n",
            "| loss:   2.516882 |\n",
            "| loss:   2.664275 |\n",
            "| loss:   2.398450 |\n",
            "Batch 401, loss2.5006182193756104\n",
            "| loss:   2.486670 |\n",
            "| loss:   2.434923 |\n",
            "| loss:   2.443202 |\n",
            "| loss:   2.644214 |\n",
            "| loss:   2.637923 |\n",
            "| loss:   2.452805 |\n",
            "| loss:   2.478661 |\n",
            "| loss:   2.657689 |\n",
            "| loss:   2.562299 |\n",
            "| loss:   2.541831 |\n",
            "| loss:   2.466186 |\n",
            "| loss:   2.487035 |\n",
            "Batch 451, loss2.500119686126709\n",
            "| loss:   2.586727 |\n",
            "| loss:   2.591110 |\n",
            "| loss:   2.609764 |\n",
            "| loss:   2.731375 |\n",
            "| loss:   2.525362 |\n",
            "| loss:   2.476972 |\n",
            "| loss:   2.695593 |\n",
            "| loss:   2.513316 |\n",
            "| loss:   2.516990 |\n",
            "| loss:   2.553146 |\n",
            "| loss:   2.468516 |\n",
            "| loss:   2.632859 |\n",
            "| loss:   2.420815 |\n",
            "Batch 501, loss2.4718410968780518\n",
            "| loss:   2.401896 |\n",
            "| loss:   2.574750 |\n",
            "| loss:   2.588180 |\n",
            "| loss:   2.597516 |\n",
            "| loss:   2.395278 |\n",
            "| loss:   2.475370 |\n",
            "| loss:   2.438726 |\n",
            "| loss:   2.534388 |\n",
            "| loss:   2.552812 |\n",
            "| loss:   2.663200 |\n",
            "| loss:   2.603291 |\n",
            "| loss:   1.457988 |\n",
            "| val loss:   4.181898 |\n",
            "| val loss:   4.348701 |\n",
            "| val loss:   4.141191 |\n",
            "| val loss:   4.518428 |\n",
            "| val loss:   4.194343 |\n",
            "| val loss:   4.298267 |\n",
            "Epoch: 06 | Time: 3m 1s\n",
            "\tTrain Loss: 2.638 | Train PPL:  13.986\n",
            "\t Val. Loss: 4.336 |  Val. PPL:  76.415\n",
            "| epoch:   6.000000 |\n",
            "total_batches=549\n",
            "| loss:   2.501747 |\n",
            "Batch 1, loss2.5511486530303955\n",
            "| loss:   2.405655 |\n",
            "| loss:   2.437335 |\n",
            "| loss:   2.454507 |\n",
            "| loss:   2.351110 |\n",
            "| loss:   2.496359 |\n",
            "| loss:   2.565552 |\n",
            "| loss:   2.412438 |\n",
            "| loss:   2.398725 |\n",
            "| loss:   2.309150 |\n",
            "| loss:   2.489603 |\n",
            "| loss:   2.529869 |\n",
            "| loss:   2.401603 |\n",
            "Batch 51, loss2.4247145652770996\n",
            "| loss:   2.585553 |\n",
            "| loss:   2.537791 |\n",
            "| loss:   2.428697 |\n",
            "| loss:   2.550719 |\n",
            "| loss:   2.472057 |\n",
            "| loss:   2.494674 |\n",
            "| loss:   2.360327 |\n",
            "| loss:   2.456109 |\n",
            "| loss:   2.262500 |\n",
            "| loss:   2.505901 |\n",
            "| loss:   2.187922 |\n",
            "| loss:   2.456411 |\n",
            "| loss:   2.270801 |\n",
            "Batch 101, loss2.422834873199463\n",
            "| loss:   2.396066 |\n",
            "| loss:   2.168178 |\n",
            "| loss:   2.467016 |\n",
            "| loss:   2.452681 |\n",
            "| loss:   2.355784 |\n",
            "| loss:   2.428378 |\n",
            "| loss:   2.337806 |\n",
            "| loss:   2.338612 |\n",
            "| loss:   2.421381 |\n",
            "| loss:   2.347373 |\n",
            "| loss:   2.407656 |\n",
            "| loss:   2.363832 |\n",
            "Batch 151, loss2.486400842666626\n",
            "| loss:   2.417031 |\n",
            "| loss:   2.477311 |\n",
            "| loss:   2.497786 |\n",
            "| loss:   2.527149 |\n",
            "| loss:   2.314991 |\n",
            "| loss:   2.264562 |\n",
            "| loss:   2.393570 |\n",
            "| loss:   2.363822 |\n",
            "| loss:   2.333898 |\n",
            "| loss:   2.383266 |\n",
            "| loss:   2.228567 |\n",
            "| loss:   2.352617 |\n",
            "| loss:   2.391567 |\n",
            "Batch 201, loss2.2731666564941406\n",
            "| loss:   2.304591 |\n",
            "| loss:   2.444794 |\n",
            "| loss:   2.363150 |\n",
            "| loss:   2.382771 |\n",
            "| loss:   2.296413 |\n",
            "| loss:   2.348203 |\n",
            "| loss:   2.095714 |\n",
            "| loss:   2.304266 |\n",
            "| loss:   2.463562 |\n",
            "| loss:   2.425975 |\n",
            "| loss:   2.208741 |\n",
            "| loss:   2.352663 |\n",
            "Batch 251, loss2.4253127574920654\n",
            "| loss:   2.423021 |\n",
            "| loss:   2.386621 |\n",
            "| loss:   2.254894 |\n",
            "| loss:   2.384004 |\n",
            "| loss:   2.227504 |\n",
            "| loss:   2.215452 |\n",
            "| loss:   2.280338 |\n",
            "| loss:   2.275107 |\n",
            "| loss:   2.253209 |\n",
            "| loss:   2.328969 |\n",
            "| loss:   2.367825 |\n",
            "| loss:   2.456298 |\n",
            "| loss:   2.283371 |\n",
            "Batch 301, loss2.129971981048584\n",
            "| loss:   2.440427 |\n",
            "| loss:   2.404376 |\n",
            "| loss:   2.347583 |\n",
            "| loss:   2.360484 |\n",
            "| loss:   2.286030 |\n",
            "| loss:   2.310732 |\n",
            "| loss:   2.311732 |\n",
            "| loss:   2.224964 |\n",
            "| loss:   2.244979 |\n",
            "| loss:   2.306898 |\n",
            "| loss:   2.233497 |\n",
            "| loss:   2.354609 |\n",
            "Batch 351, loss2.419928789138794\n",
            "| loss:   2.201124 |\n",
            "| loss:   2.309855 |\n",
            "| loss:   2.359059 |\n",
            "| loss:   2.285341 |\n",
            "| loss:   2.223374 |\n",
            "| loss:   2.323506 |\n",
            "| loss:   2.348754 |\n",
            "| loss:   2.222230 |\n",
            "| loss:   2.154371 |\n",
            "| loss:   1.980866 |\n",
            "| loss:   2.216376 |\n",
            "| loss:   2.396923 |\n",
            "| loss:   2.093957 |\n",
            "Batch 401, loss2.1689465045928955\n",
            "| loss:   2.217442 |\n",
            "| loss:   2.114827 |\n",
            "| loss:   2.159670 |\n",
            "| loss:   2.368314 |\n",
            "| loss:   2.346723 |\n",
            "| loss:   2.158173 |\n",
            "| loss:   2.183202 |\n",
            "| loss:   2.288021 |\n",
            "| loss:   2.260565 |\n",
            "| loss:   2.242222 |\n",
            "| loss:   2.201968 |\n",
            "| loss:   2.188450 |\n",
            "Batch 451, loss2.2061703205108643\n",
            "| loss:   2.314540 |\n",
            "| loss:   2.318823 |\n",
            "| loss:   2.291256 |\n",
            "| loss:   2.447351 |\n",
            "| loss:   2.234379 |\n",
            "| loss:   2.164189 |\n",
            "| loss:   2.416103 |\n",
            "| loss:   2.196098 |\n",
            "| loss:   2.235206 |\n",
            "| loss:   2.266372 |\n",
            "| loss:   2.191018 |\n",
            "| loss:   2.322634 |\n",
            "| loss:   2.089614 |\n",
            "Batch 501, loss2.196074962615967\n",
            "| loss:   2.135784 |\n",
            "| loss:   2.327286 |\n",
            "| loss:   2.256750 |\n",
            "| loss:   2.283321 |\n",
            "| loss:   2.079434 |\n",
            "| loss:   2.178535 |\n",
            "| loss:   2.104743 |\n",
            "| loss:   2.218866 |\n",
            "| loss:   2.252699 |\n",
            "| loss:   2.400625 |\n",
            "| loss:   2.278861 |\n",
            "| loss:   1.104871 |\n",
            "| val loss:   4.311598 |\n",
            "| val loss:   4.501034 |\n",
            "| val loss:   4.254881 |\n",
            "| val loss:   4.485517 |\n",
            "| val loss:   4.502429 |\n",
            "| val loss:   4.494164 |\n",
            "Epoch: 07 | Time: 3m 1s\n",
            "\tTrain Loss: 2.334 | Train PPL:  10.317\n",
            "\t Val. Loss: 4.475 |  Val. PPL:  87.764\n",
            "| epoch:   7.000000 |\n",
            "total_batches=549\n",
            "| loss:   2.299466 |\n",
            "Batch 1, loss2.2776739597320557\n",
            "| loss:   2.111244 |\n",
            "| loss:   2.171979 |\n",
            "| loss:   2.146764 |\n",
            "| loss:   2.077681 |\n",
            "| loss:   2.260727 |\n",
            "| loss:   2.252995 |\n",
            "| loss:   2.126500 |\n",
            "| loss:   2.158678 |\n",
            "| loss:   1.958095 |\n",
            "| loss:   2.178637 |\n",
            "| loss:   2.270313 |\n",
            "| loss:   2.093714 |\n",
            "Batch 51, loss2.1503665447235107\n",
            "| loss:   2.370114 |\n",
            "| loss:   2.226720 |\n",
            "| loss:   2.198275 |\n",
            "| loss:   2.301714 |\n",
            "| loss:   2.207653 |\n",
            "| loss:   2.185051 |\n",
            "| loss:   2.141667 |\n",
            "| loss:   2.176718 |\n",
            "| loss:   2.013643 |\n",
            "| loss:   2.214207 |\n",
            "| loss:   1.935166 |\n",
            "| loss:   2.150859 |\n",
            "| loss:   1.970109 |\n",
            "Batch 101, loss2.1705775260925293\n",
            "| loss:   2.120792 |\n",
            "| loss:   1.865383 |\n",
            "| loss:   2.200953 |\n",
            "| loss:   2.187891 |\n",
            "| loss:   2.068964 |\n",
            "| loss:   2.138634 |\n",
            "| loss:   2.053863 |\n",
            "| loss:   2.064791 |\n",
            "| loss:   2.184851 |\n",
            "| loss:   2.077196 |\n",
            "| loss:   2.141197 |\n",
            "| loss:   2.069256 |\n",
            "Batch 151, loss2.212197780609131\n",
            "| loss:   2.182281 |\n",
            "| loss:   2.184701 |\n",
            "| loss:   2.236395 |\n",
            "| loss:   2.218993 |\n",
            "| loss:   2.008656 |\n",
            "| loss:   1.980401 |\n",
            "| loss:   2.077797 |\n",
            "| loss:   2.101146 |\n",
            "| loss:   2.050842 |\n",
            "| loss:   2.147304 |\n",
            "| loss:   1.983634 |\n",
            "| loss:   2.024873 |\n",
            "| loss:   2.107676 |\n",
            "Batch 201, loss1.9951225519180298\n",
            "| loss:   2.046216 |\n",
            "| loss:   2.163750 |\n",
            "| loss:   2.140583 |\n",
            "| loss:   2.094524 |\n",
            "| loss:   2.038746 |\n",
            "| loss:   2.074866 |\n",
            "| loss:   1.841097 |\n",
            "| loss:   2.029670 |\n",
            "| loss:   2.157305 |\n",
            "| loss:   2.145210 |\n",
            "| loss:   1.916732 |\n",
            "| loss:   2.096824 |\n",
            "Batch 251, loss2.13787841796875\n",
            "| loss:   2.121623 |\n",
            "| loss:   2.083922 |\n",
            "| loss:   1.987741 |\n",
            "| loss:   2.101610 |\n",
            "| loss:   2.019964 |\n",
            "| loss:   1.905092 |\n",
            "| loss:   1.978331 |\n",
            "| loss:   1.966300 |\n",
            "| loss:   1.995187 |\n",
            "| loss:   2.057518 |\n",
            "| loss:   2.065577 |\n",
            "| loss:   2.159679 |\n",
            "| loss:   1.986615 |\n",
            "Batch 301, loss1.8266174793243408\n",
            "| loss:   2.168012 |\n",
            "| loss:   2.101335 |\n",
            "| loss:   2.083099 |\n",
            "| loss:   2.116656 |\n",
            "| loss:   1.982590 |\n",
            "| loss:   2.030134 |\n",
            "| loss:   2.118376 |\n",
            "| loss:   1.946248 |\n",
            "| loss:   1.951732 |\n",
            "| loss:   2.042480 |\n",
            "| loss:   1.963329 |\n",
            "| loss:   2.102494 |\n",
            "Batch 351, loss2.188951253890991\n",
            "| loss:   1.950877 |\n",
            "| loss:   2.062036 |\n",
            "| loss:   2.075549 |\n",
            "| loss:   2.037237 |\n",
            "| loss:   1.930752 |\n",
            "| loss:   2.060722 |\n",
            "| loss:   2.075953 |\n",
            "| loss:   1.987064 |\n",
            "| loss:   1.893496 |\n",
            "| loss:   1.719887 |\n",
            "| loss:   1.911730 |\n",
            "| loss:   2.133137 |\n",
            "| loss:   1.829012 |\n",
            "Batch 401, loss1.921804428100586\n",
            "| loss:   1.977221 |\n",
            "| loss:   1.864978 |\n",
            "| loss:   1.960935 |\n",
            "| loss:   2.092237 |\n",
            "| loss:   2.185259 |\n",
            "| loss:   1.908799 |\n",
            "| loss:   1.983197 |\n",
            "| loss:   2.026191 |\n",
            "| loss:   2.007741 |\n",
            "| loss:   1.960763 |\n",
            "| loss:   1.921278 |\n",
            "| loss:   1.935486 |\n",
            "Batch 451, loss1.9034109115600586\n",
            "| loss:   2.064312 |\n",
            "| loss:   2.117618 |\n",
            "| loss:   2.062722 |\n",
            "| loss:   2.191432 |\n",
            "| loss:   2.037204 |\n",
            "| loss:   1.944517 |\n",
            "| loss:   2.171864 |\n",
            "| loss:   1.978679 |\n",
            "| loss:   1.981474 |\n",
            "| loss:   2.024684 |\n",
            "| loss:   1.924658 |\n",
            "| loss:   2.061231 |\n",
            "| loss:   1.865401 |\n",
            "Batch 501, loss1.9524970054626465\n",
            "| loss:   1.893026 |\n",
            "| loss:   2.094399 |\n",
            "| loss:   2.019413 |\n",
            "| loss:   2.068302 |\n",
            "| loss:   1.886827 |\n",
            "| loss:   1.917170 |\n",
            "| loss:   1.853308 |\n",
            "| loss:   1.974653 |\n",
            "| loss:   1.978442 |\n",
            "| loss:   2.157065 |\n",
            "| loss:   2.064670 |\n",
            "| loss:   0.808922 |\n",
            "| val loss:   4.354167 |\n",
            "| val loss:   4.612414 |\n",
            "| val loss:   4.327791 |\n",
            "| val loss:   4.743273 |\n",
            "| val loss:   4.433062 |\n",
            "| val loss:   4.508917 |\n",
            "Epoch: 08 | Time: 3m 1s\n",
            "\tTrain Loss: 2.070 | Train PPL:   7.922\n",
            "\t Val. Loss: 4.581 |  Val. PPL:  97.647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNPvKIdPYSfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "exp.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrFN7nTHZlBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sentence(sentence, inpLang, optLang, model, device, max_len = 50):\n",
        "    model.eval()\n",
        "    tokens = inpLang.tokenize(sentence)\n",
        "    src_indexes = inpLang.encode(sentence)\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
        "    src_mask = model.make_src_mask(src_tensor)\n",
        "    with torch.no_grad():\n",
        "        enc_src = model.encoder(src_tensor, src_mask)\n",
        "    trg_indexes = [optLang.word2idx[optLang.special[\"init_token\"]]]\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\n",
        "        with torch.no_grad():\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, src_mask)\n",
        "        pred_token = output.argmax(2)[:,-1].item()\n",
        "        trg_indexes.append(pred_token)\n",
        "        if pred_token == optLang.word2idx[optLang.special[\"eos_token\"]]:\n",
        "            break\n",
        "    trg_tokens = [optLang.idx2word[i] for i in trg_indexes]\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTeMBCzdcRHT",
        "colab_type": "code",
        "outputId": "69ec7d19-3340-475f-a1dd-690da8319e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "inp=[]\n",
        "opt=[]\n",
        "trg=[]\n",
        "i=0\n",
        "bleu=0\n",
        "from tqdm import tqdm\n",
        "print(len(test_dataloader))\n",
        "for i,(x,y) in tqdm(enumerate(test_dataloader)):\n",
        "    i+=1\n",
        "    for x_,y_ in zip(x,y):\n",
        "        o,a=translate_sentence(x_,inpLang,optLang,model,torch.device(\"cuda\"))\n",
        "        y_=inpLang.tokenize(y_)\n",
        "        inp.append(x_)\n",
        "        trg.append(y_)\n",
        "        opt.append(o)\n",
        "        # bleu+=bleu_score([o],[[y_]])\n",
        "bleu=bleu/i\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "68it [12:29, 11.03s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wajXFKMCu_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext.data.metrics import bleu_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwua1n5AYI4k",
        "colab_type": "code",
        "outputId": "3b385dcd-7001-4d6e-dd3d-b4c2cf6c3684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "bleu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34.051867093280244"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eb6g16EEiP68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame({\"input\":inp,\"target\":trg,\"output\":opt})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-kp56BliaVf",
        "colab_type": "code",
        "outputId": "35e2e24d-f89d-492d-ea4a-82a01781f81d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>clothing also provides protection from harmful...</td>\n",
              "      <td>[what, type, of, radiation, can, clothing, pro...</td>\n",
              "      <td>[what, does, sparking, limit, ?, &lt;EOS&gt;]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>soon a beam was traced to derby (which had bee...</td>\n",
              "      <td>[the, beam, was, traced, to, what, town, ?]</td>\n",
              "      <td>[who, did, the, romans, use, to, avoid, the, r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>the ownership of the spectre organisation—orig...</td>\n",
              "      <td>[what, did, spectre, originally, stand, for, ?]</td>\n",
              "      <td>[who, was, the, recipient, of, the, first, boo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>by contrast, a white wine contains lower pheno...</td>\n",
              "      <td>[what, type, of, wine, is, fermented, after, t...</td>\n",
              "      <td>[what, is, the, relationship, between, copper,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nasser's regional position changed unexpectedl...</td>\n",
              "      <td>[what, country, experienced, a, coup, in, 1962...</td>\n",
              "      <td>[what, conflict, did, nasser, and, the, genera...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               input  ...                                             output\n",
              "0  clothing also provides protection from harmful...  ...            [what, does, sparking, limit, ?, <EOS>]\n",
              "1  soon a beam was traced to derby (which had bee...  ...  [who, did, the, romans, use, to, avoid, the, r...\n",
              "2  the ownership of the spectre organisation—orig...  ...  [who, was, the, recipient, of, the, first, boo...\n",
              "3  by contrast, a white wine contains lower pheno...  ...  [what, is, the, relationship, between, copper,...\n",
              "4  nasser's regional position changed unexpectedl...  ...  [what, conflict, did, nasser, and, the, genera...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnqwFdXPibnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"transformer_qg_output.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnd6EU4Timuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}