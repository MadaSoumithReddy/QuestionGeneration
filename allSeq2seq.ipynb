{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "allSeq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hjcS6ex4OF7K",
        "p5CkzAODOMba",
        "aX7azUEHWhEZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKOHJfp-bHRC",
        "colab_type": "code",
        "outputId": "5794709f-96a1-42d6-9884-aca2c42fb539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "source": [
        "!pip install wget\n",
        "!pip install hyperdash\n",
        "!pip install tokenizers\n",
        "!pip install transformers\n",
        "!hd login --github\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Requirement already satisfied: hyperdash in /usr/local/lib/python3.6/dist-packages (0.15.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from hyperdash) (4.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from hyperdash) (2.21.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from hyperdash) (1.12.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->hyperdash) (1.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->hyperdash) (3.0.4)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.40)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.40)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.40->boto3->transformers) (2.8.1)\n",
            "Opening browser, please wait. If something goes wrong, press CTRL+C to cancel.\n",
            "\u001b[1m SSH'd into a remote machine, or just don't have access to a browser? Open this link in any browser and then copy/paste the provided access token: \u001b[4mhttps://hyperdash.io/oauth/github/start?state=client_cli_manual\u001b[0m \u001b[0m\n",
            "Waiting for Github OAuth to complete.\n",
            "If something goes wrong, press CTRL+C to cancel.\n",
            "Access token: DcGFHT5cFa4eikxp97lHR789au1i92kM8KgSLmbYq/E=\n",
            "Successfully logged in! We also installed: 6dqfQAL9Xij4kBZzoFO+iDTxNHszbaxsxhzaeg0f/DE= as your default API key\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9p_lKl7_yVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE= \"GPU\"\n",
        "testNMT=True\n",
        "SAMPLE=False\n",
        "MAX_LEN=100\n",
        "INPUT_VOCAB=10000\n",
        "OUTPUT_VOCAB=10000\n",
        "BATCH_SIZE=128\n",
        "USE_PRETRAINED=False\n",
        "TOKENIZER=\"spacy\"\n",
        "\n",
        "if TOKENIZER==\"BERT\":\n",
        "    trainNMT=False\n",
        "    USE_PRETRAINED=False\n",
        "if testNMT:\n",
        "    USE_PRETRAINED=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGdhs_5J62ik",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT0Fj0arbaU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "if DEVICE==\"CPU\":\n",
        "    device=torch.device('cpu')\n",
        "elif DEVICE==\"TPU\": \n",
        "    import torch_xla\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "    import torch_xla.distributed.parallel_loader as pl\n",
        "    device = xm.xla_device() \n",
        "else:\n",
        "    device=torch.device(('cuda' if torch.cuda.is_available() else 'cpu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saa-1mn9bL3J",
        "colab_type": "code",
        "outputId": "14fa72b0-7dd5-4442-90b1-02c9c6d5efc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(device)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--QrJIaa0PZN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load Glove\n",
        "def load_glove_embedding(vocab,glove):\n",
        "\n",
        "    nlp = glove\n",
        "    vocab_size = len(vocab.word2idx)\n",
        "    word_vec_size = 300\n",
        "    embedding = np.zeros((vocab_size, word_vec_size))\n",
        "    unk_count = 0\n",
        "    \n",
        "    print('='*100)\n",
        "    print('Loading spacy glove embedding:')\n",
        "    print('- Vocabulary size: {}'.format(vocab_size))\n",
        "    print('- Word vector size: {}'.format(word_vec_size))\n",
        "    \n",
        "    for token, index in tqdm(vocab.word2idx.items()):\n",
        "        if token == vocab.special[\"pad_token\"]: \n",
        "            continue\n",
        "        elif token in [vocab.special[\"eos_token\"], vocab.special[\"init_token\"], vocab.special[\"unk_token\"]]: \n",
        "            vector = np.random.rand(word_vec_size,)\n",
        "        elif token in nlp.keys():\n",
        "            vector = nlp[token]\n",
        "        else:\n",
        "            vector = embedding[vocab.word2idx[vocab.special[\"unk_token\"]]]\n",
        "            unk_count += 1\n",
        "            \n",
        "        embedding[index] = vector\n",
        "        \n",
        "    print('\\n- Unknown word count: {}'.format(unk_count))\n",
        "    print('='*100 + '\\n')\n",
        "        \n",
        "    return torch.from_numpy(embedding).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KojChtf0Vj2",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download Glove\n",
        "import os\n",
        "import wget\n",
        "import zipfile\n",
        "def download_glove(link=\"http://nlp.stanford.edu/data/glove.6B.zip\"):\n",
        "    print(\"Downloading glove\")\n",
        "    if not os.path.exists(\"./glove\"):\n",
        "        if not os.path.exists(\"glove.zip\"):\n",
        "            wget.download(link,\"glove.zip\")\n",
        "        print(\"Extracting Glove\")\n",
        "        with zipfile.ZipFile(\"./glove.zip\", 'r') as zip_ref:\n",
        "            zip_ref.extractall(\"glove\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "def loadGloveModel(gloveFile=\"./glove/glove.6B.300d.txt\"):\n",
        "    if not os.path.exists(gloveFile):\n",
        "        download_glove()\n",
        "    print(\"Loading Glove Model\")\n",
        "    with open(gloveFile,'r') as f:\n",
        "        model = {}\n",
        "        for line in tqdm(f,position=0,leave=False):\n",
        "            splitLine = line.split()\n",
        "            try:\n",
        "                word = \" \".join(splitLine[0:-300])\n",
        "                embedding = np.array([float(val) for val in splitLine[-300:]])\n",
        "                model[word] = embedding\n",
        "            except:\n",
        "                print(f\"Error in {splitLine[0]}\")\n",
        "                print(f\"Vect : \\n{splitLine[1:]}\")\n",
        "        print(\"Done.\",len(model),\" words loaded!\")\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clnf9nj1bVbB",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title QGenDataset\n",
        "import wget\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torchtext\n",
        "import spacy\n",
        "import zipfile\n",
        "import unicodedata\n",
        "import re\n",
        "class QGenDataset(object):\n",
        "    def __init__(self,squad=True,USE_ENTIRE_SENTENCE=True,test_nmt=False):\n",
        "        self.USE_ENTIRE_SENTENCE=USE_ENTIRE_SENTENCE\n",
        "        if squad and not test_nmt:\n",
        "            if not os.path.exists(\"./train-v2.0.json\"):\n",
        "                wget.download(\"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\")\n",
        "            with open(\"./train-v2.0.json\",'r') as f:\n",
        "                self.raw_data=json.load(f)\n",
        "            self.data=self._get_dataset()\n",
        "        if test_nmt:\n",
        "            with zipfile.ZipFile(\"/content/drive/My Drive/Thesis/spa-eng.zip\", 'r') as zip_ref:\n",
        "                zip_ref.extractall(\"./spa/\")\n",
        "            with open(\"./spa/spa.txt\",'r') as f:\n",
        "                self.nmt_raw=f.read().strip().split('\\n')\n",
        "            self.__get_NMT__()\n",
        "    def __get_NMT__(self):\n",
        "        original_word_pairs = [[w for w in l.split('\\t')] for l in self.nmt_raw]\n",
        "        self.eng=[i[0] for i in original_word_pairs]\n",
        "        self.spa=[i[1] for i in original_word_pairs]\n",
        "\n",
        "    def get_AQ(self,max_len=80,sample=True):\n",
        "        raw_data = {'ans' : [line[0] for line in self.data], 'que': [line[1] for line in self.data]}\n",
        "        df = pd.DataFrame(raw_data, columns=[\"ans\", \"que\"])\n",
        "        # remove very long sentences and sentences where translations are \n",
        "        # not of roughly equal length\n",
        "        df['ans_len'] = df['ans'].str.count(' ')\n",
        "        df['que_len'] = df['que'].str.count(' ')\n",
        "        df = df.query('ans_len <'+str(max_len)+' & que_len <'+str(max_len))\n",
        "        df = df.drop_duplicates()\n",
        "        if sample:\n",
        "            return df[\"ans\"].values[:2000],df[\"que\"].values[:2000]\n",
        "        return df[\"ans\"].values,df[\"que\"].values\n",
        "        \n",
        "\n",
        "    def get_NMT(self,sample=False):\n",
        "        if sample:\n",
        "            return self.eng[:2000],self.spa[:2000]   \n",
        "        return self.eng,self.spa \n",
        "\n",
        "    def _create_dataset(self,data,normalize=True):\n",
        "        load_failure=0\n",
        "        try:\n",
        "            if \"data\" in data.keys():\n",
        "                data=data[\"data\"]\n",
        "        except:\n",
        "            pass\n",
        "        que_ans=[]\n",
        "        for topic in data:\n",
        "            for para in topic[\"paragraphs\"]:\n",
        "                for qa in para[\"qas\"]:\n",
        "                    try:\n",
        "                        res=[]\n",
        "                        if normalize:\n",
        "                            res.append(self._normalize(self._get_sentence(para[\"context\"],qa[\"answers\"][0][\"answer_start\"],qa[\"answers\"][0][\"text\"])))\n",
        "                            res.append(self._normalize(qa[\"question\"]))\n",
        "                        else:\n",
        "                            res.append(self._get_sentence(para[\"context\"],qa[\"answers\"][0][\"answer_start\"],qa[\"answers\"][0][\"text\"]))\n",
        "                            res.append(qa[\"question\"])\n",
        "                        que_ans.append(res)\n",
        "                    except:\n",
        "                        load_failure+=1\n",
        "        print(\"Load Failure : \",load_failure)\n",
        "        return que_ans\n",
        "    @staticmethod\n",
        "    def _get_sentence(context,position,text):\n",
        "        if \".\" in text[:-1]:\n",
        "            return_2=True\n",
        "        else:\n",
        "            return_2=False\n",
        "        context=context.split(\".\")\n",
        "        count=0\n",
        "        for sent in range(len(context)):\n",
        "            if count+len(context[sent])>position:\n",
        "                if return_2:\n",
        "                    return \".\".join(context[sent:sent+2])\n",
        "                else:\n",
        "                    return context[sent]\n",
        "            else:\n",
        "                count+=len(context[sent])+1\n",
        "        return False\n",
        "\n",
        "    def _get_dataset(self,normalize=True):\n",
        "        data =  self._create_dataset(self.raw_data,normalize=normalize)\n",
        "        return data  \n",
        "            \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.data_len\n",
        "    def apply(self,function,all=True):\n",
        "        for i in tqdm(range(self.data_len),position=0,leave=True):\n",
        "            self.context[i]=function(self.context[i])\n",
        "            self.answers[i]=function(self.answers[i])\n",
        "            self.questions[i]=function(self.questions[i])\n",
        "\n",
        "    def bert_format(self):\n",
        "        X=[0 for i in range(self.data_len)]\n",
        "        Y=[0 for i in range(self.data_len)]\n",
        "        for i in range(self.data_len):\n",
        "            X[i]=\"[CLS] \" + self.context[i] +\"[SEP]\"+ self.answers[i] + \"[SEP]\"\n",
        "            Y[i]=self.questions[i]\n",
        "        return (X,Y)\n",
        "    @staticmethod\n",
        "    def unicodeToAscii(s):\n",
        "        return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "    def _normalize(self,s):\n",
        "        s = self.unicodeToAscii(s.lower().strip())\n",
        "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "        #s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "        return s\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omT8c2xubpZ_",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Language Index\n",
        "import spacy\n",
        "from collections import Counter \n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "class LanguageIndex():\n",
        "\n",
        "    def __init__(self, lang,tokenizer=\"spacy\", pad=\"<PAD>\",init_token=\"<SOS>\",eos_token=\"<EOS>\",unk_token=\"<UNK>\",max_len=None,vocab_size=None,lower_case=True):\n",
        "        \"\"\" lang are the list of phrases from each language\"\"\"\n",
        "        self.lang = lang\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.special={}\n",
        "        self.max_len=max_len\n",
        "        self.vocab_size=vocab_size-4 if vocab_size!=None else sys.maxsize\n",
        "        self.lower=lower_case\n",
        "        self.tokenizer=tokenizer\n",
        "        if self.tokenizer==\"BERT\":\n",
        "            model_type = 'bert-base-uncased'\n",
        "            self.bert_tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
        "\n",
        "        # add a padding token with index 0\n",
        "        self.word2idx[pad] = 0\n",
        "        self.special[\"pad_token\"]=pad\n",
        "\n",
        "        self.word2idx[init_token] = 1\n",
        "        self.special[\"init_token\"]=init_token\n",
        "\n",
        "        self.word2idx[eos_token] = 2\n",
        "        self.special[\"eos_token\"]=eos_token\n",
        "\n",
        "        self.word2idx[unk_token] = 3\n",
        "        self.special[\"unk_token\"]=unk_token\n",
        "\n",
        "        self.vocab = set()\n",
        "        self.counter=Counter()\n",
        "        self.spacy=None\n",
        "        self.create_index()\n",
        "        \n",
        "\n",
        "    @staticmethod\n",
        "    def unicode_to_ascii(s):\n",
        "        \"\"\"\n",
        "        Normalizes latin chars with accent to their canonical decomposition\n",
        "        \"\"\"\n",
        "        return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn')\n",
        "        \n",
        "    @staticmethod\n",
        "    def preprocess_sentence(w):\n",
        "        w = unicode_to_ascii(w.lower().strip())\n",
        "        \n",
        "        # creating a space between a word and the punctuation following it\n",
        "        # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "        # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "        w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "        w = re.sub(r'[\" \"]+', \" \", w)\n",
        "        \n",
        "        # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "        w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "        \n",
        "        w = w.rstrip().strip()\n",
        "        \n",
        "        # adding a start and an end token to the sentence\n",
        "        # so that the model know when to start and stop predicting.\n",
        "        return w\n",
        "        \n",
        "    def tokenize(self,phrase):\n",
        "        if self.tokenizer==\"spacy\":\n",
        "            if not self.spacy:\n",
        "                self.spacy = spacy.load('en')\n",
        "            return [tok.text for tok in self.spacy.tokenizer(phrase)]\n",
        "        if self.tokenizer==\"BERT\":\n",
        "            return self.bert_tokenizer.tokenize(phrase)\n",
        "        else:\n",
        "            return self.preprocess(phrase)\n",
        "\n",
        "    def create_index(self):\n",
        "        for phrase in self.lang:\n",
        "            # update with individual tokens\n",
        "            tokens=self.tokenize(phrase.lower() if self.lower else phrase)\n",
        "            self.vocab.update(tokens)\n",
        "            self.counter.update(tokens)\n",
        "            \n",
        "        # sort the vocab\n",
        "        self.vocab = sorted(self.vocab)\n",
        "        start_index = max(self.word2idx.values())+1\n",
        "        \n",
        "        # word to index mapping\n",
        "        for index, word in enumerate(self.counter.most_common(self.vocab_size)):\n",
        "            self.word2idx[word[0]] = index + start_index \n",
        "        \n",
        "        # index to word mapping\n",
        "        for word, index in self.word2idx.items():\n",
        "            self.idx2word[index] = word\n",
        "\n",
        "    def encode_batch(self,batch,special_tokens=True):\n",
        "        return np.array([self.encode(obj,special_tokens=special_tokens) for obj in batch],dtype=np.int64)\n",
        "    def decode_batch(self,batch):\n",
        "        return [self.decode(obj) for obj in batch]\n",
        "\n",
        "    def encode(self,input,special_tokens=True):\n",
        "        pad_len=self.max_len\n",
        "        input=input.lower() if self.lower else input\n",
        "        tokens=[tok for tok in self.tokenize(input)]\n",
        "        if pad_len!=None:\n",
        "            if len(tokens)>pad_len-(2 if special_tokens else 0):\n",
        "                if special_tokens:\n",
        "                    return [1]+[self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens][:pad_len-2]+[2]\n",
        "                else:\n",
        "                    return [self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens][:pad_len]\n",
        "            else:\n",
        "                return ([1] if special_tokens else []) + [self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens] +([2] if special_tokens else []) +[0 for i in range(pad_len-(2 if special_tokens else 0)-len(tokens))]\n",
        "        return ([1] if special_tokens else []) + [self.word2idx[s] if s in self.word2idx.keys() else 3 for s in tokens] +([2] if special_tokens else []) \n",
        "    def decode(self,input,to_string=False):\n",
        "        sent=[self.idx2word[s] if s in self.idx2word.keys() else self.special[\"unk_token\"] for s in input]\n",
        "        if self.tokenizer==\"BERT\" and to_string:\n",
        "            return self.bert_tokenizer.convert_tokens_to_string(sent)\n",
        "        return sent\n",
        "    def vocab_size_final(self):\n",
        "        return len(self.word2idx.keys())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0VuUdzAbtFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "qg=QGenDataset(test_nmt=testNMT)\n",
        "if testNMT:\n",
        "    input_,output_=qg.get_NMT(sample=SAMPLE)\n",
        "else:\n",
        "    input_,output_=qg.get_AQ(sample=SAMPLE,max_len=MAX_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwmDjERNb1o4",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "#@title Train Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_set_input,test_set_input,train_set_output,test_set_output=train_test_split(input_,output_,test_size=0.1)\n",
        "input_train,input_test,output_train,output_test=train_test_split(train_set_input,train_set_output,test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VuwVoNUb7QQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "inpLang=LanguageIndex(input_train,vocab_size=INPUT_VOCAB,max_len=MAX_LEN,tokenizer=TOKENIZER)\n",
        "optLang=LanguageIndex(output_train,vocab_size=OUTPUT_VOCAB,max_len=MAX_LEN,tokenizer=TOKENIZER)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAiPQbTr0j_Q",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Load Glove Embeddings\n",
        "if USE_PRETRAINED:\n",
        "    import pickle \n",
        "    with open(\"/content/drive/My Drive/glove.p\",'rb') as f:\n",
        "        glove=pickle.load(f)\n",
        "\n",
        "if USE_PRETRAINED:\n",
        "    glove_opt=load_glove_embedding(optLang,glove)\n",
        "    glove_inp=load_glove_embedding(inpLang,glove)\n",
        "    OUTPUT_VOCAB=glove_opt.shape[0]\n",
        "    INPUT_VOCAB=glove_inp.shape[0]\n",
        "else:\n",
        "    glove_inp=None\n",
        "    glove_opt=None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RjKgcWycLGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_train_tokens=inpLang.encode_batch(input_train)\n",
        "input_test_tokens=inpLang.encode_batch(input_test)\n",
        "ouptut_train_tokens=optLang.encode_batch(output_train)\n",
        "output_test_tokens=optLang.encode_batch(output_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKbJjwwicdt6",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Add data to torch dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class MyData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "        # TODO: convert this into torch code is possible\n",
        "        self.length = [ np.sum(1 - np.equal(x, 0)) for x in X]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        x_len = self.length[index]\n",
        "        return x,y,x_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67WheItKgQjT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Add data to torch test dataset\n",
        "class TestData(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.target = y\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.target[index]\n",
        "        return x,y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "TEST_BATCH_SIZE=20\n",
        "final_test_dataset=TestData(test_set_input,test_set_output)\n",
        "final_dataloader=DataLoader(final_test_dataset,batch_size=TEST_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FG3AX81ci8o",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Create Dataloader\n",
        "train_dataset = MyData(input_train_tokens,ouptut_train_tokens)\n",
        "test_dataset = MyData(input_test_tokens, output_test_tokens)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = BATCH_SIZE, \n",
        "                     drop_last=True,\n",
        "                     shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxB9FeCZclnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "import random\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7UE0cOPZchT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_param(path,**kwargs):\n",
        "    with open(path,'w') as f:\n",
        "        for key, value in kwargs.items(): \n",
        "            f.write(f\"{key} == {value}\\n\")\n",
        "    print(f\"Saved Hyperparameters at {path}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMS3zykTaOA5",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Get Params for model\n",
        "def get_params():\n",
        "    params={}\n",
        "    params_to_save=[\"BATCH_SIZE\",\"DEVICE\",\"testNMT\",\"SAMPLE\",\"MAX_LEN\",\n",
        "                    \"INPUT_VOCAB\",\"OUTPUT_VOCAB\",\"BATCH_SIZE\",\"USE_PRETRAINED\",\n",
        "                    \"INPUT_DIM\",\"OUTPUT_DIM\",\"ENC_EMB_DIM\",\"DEC_HID_DIM\",\"HID_DIM\",\n",
        "                    \"ENC_DROPOUT\",\"DEC_DROPOUT\",\"N_LAYERS\",\"LR\",\"EPOCHS\",\"HYPERDASH\",\"CLIP\",\"EPOCHS\",\"TOKENIZER\"]\n",
        "    for name in params_to_save:\n",
        "        try:\n",
        "            params[name]=globals().get(name,None)\n",
        "        except:\n",
        "            pass\n",
        "    return params\n",
        "def log_hyperdash(exp,**kwargs):\n",
        "    for k,v in kwargs.items():\n",
        "        exp.param(k,v)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjcS6ex4OF7K",
        "colab_type": "text"
      },
      "source": [
        "#Basic Seq2seq with 2 layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vW1BC2WzsrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENC_EMB_DIM = 300\n",
        "DEC_EMB_DIM = 300\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.3\n",
        "DEC_DROPOUT = 0.3\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fKLKbHXcx2e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_dim,emb_dim,hid_dim,n_layers,dropout,embedding=None):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        self.n_layers=n_layers\n",
        "        self.embedding=nn.Embedding(input_dim,emb_dim)\n",
        "        if embedding!=None:\n",
        "            self.embedding.load_state_dict({'weight': embedding})\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        self.rnn=nn.LSTM(emb_dim,hid_dim,n_layers,dropout=dropout)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,src):\n",
        "        src=src.transpose(0,1)\n",
        "        embedded=self.dropout(self.embedding(src))\n",
        "        output,(hidden,cell)=self.rnn(embedded)\n",
        "        return hidden,cell"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFoszRnRedyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_dim,emb_dim,hid_dim,n_layers,dropout,embedding=None):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.output_dim=output_dim\n",
        "        self.hid_dim=hid_dim\n",
        "        self.n_layers=n_layers\n",
        "        self.embedding=nn.Embedding(output_dim,emb_dim)\n",
        "        if embedding!=None:\n",
        "            self.embedding.load_state_dict({'weight': embedding})\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        self.rnn=nn.LSTM(emb_dim,hid_dim,n_layers,dropout=dropout)\n",
        "        self.fc=nn.Linear(hid_dim,output_dim)\n",
        "        # self.softmax=nn.Softmax(dim=1)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,input,hidden,cell):\n",
        "        input=input.unsqueeze(0)\n",
        "        embedded=self.dropout(self.embedding(input))\n",
        "        output,(hidden,cell)=self.rnn(embedded,(hidden,cell))\n",
        "        prediction=self.fc(output.squeeze(0))\n",
        "        # prediction=self.softmax(prediction)\n",
        "        return prediction,hidden,cell\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtxFQYzf5Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder,device):\n",
        "        super(Seq2seq,self).__init__()\n",
        "        self.encoder=encoder\n",
        "        self.decoder=decoder\n",
        "        self.device=device\n",
        "    def forward(self,src,trg,teacher_forcing_ratio=0.5):\n",
        "        batch_size=trg.shape[0]\n",
        "        trg_len=trg.shape[1]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        outputs=torch.zeros(trg_len,batch_size,trg_vocab_size).to(self.device)\n",
        "        hidden,cell=self.encoder(src)\n",
        "        input=trg[:,0]\n",
        "        for t in range(1,trg_len):\n",
        "            output,hidden,cell=self.decoder(input,hidden,cell)\n",
        "            outputs[t]=output\n",
        "            teacher_force=random.random()<teacher_forcing_ratio\n",
        "            top1=output.argmax(1)\n",
        "            input = trg[:,t] if teacher_force else top1\n",
        "        return outputs\n",
        "    def decode(self,input,inpLang,optLang,max_len=10):\n",
        "        stop_token=optLang.word2idx[optLang.special[\"eos_token\"]]\n",
        "        start_token=optLang.word2idx[optLang.special[\"init_token\"]]\n",
        "        src=torch.tensor(inpLang.encode(input),device=self.device).unsqueeze(1).transpose(0,1)\n",
        "        batch_size=src.shape[1]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        hidden,cell=self.encoder(src)\n",
        "        input=torch.tensor(start_token,device=self.device).unsqueeze(0)\n",
        "        stop=False\n",
        "        outputs=[]\n",
        "        while not stop:\n",
        "            output,hidden,cell=self.decoder(input,hidden,cell)\n",
        "            top1=output.argmax(1)\n",
        "            topk=torch.topk(output,5)\n",
        "            input=top1\n",
        "            if top1.item()==stop_token or len(outputs)>max_len:\n",
        "                stop=True\n",
        "            outputs.append(top1.item())\n",
        "        return \" \".join(optLang.decode(outputs))\n",
        "    def batch_decode(self,input,inpLang,optLang,max_len=10):\n",
        "        stop_token=optLang.word2idx[optLang.special[\"eos_token\"]]\n",
        "        start_token=optLang.word2idx[optLang.special[\"init_token\"]]\n",
        "        src=torch.tensor(inpLang.encode_batch(input),device=self.device)\n",
        "        batch_size=src.shape[0]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        hidden,cell=self.encoder(src)\n",
        "        input=torch.tensor([start_token]*batch_size,device=self.device)\n",
        "        stop=False\n",
        "        outputs=[]\n",
        "        while not stop:\n",
        "            output,hidden,cell=self.decoder(input,hidden,cell)\n",
        "            top1=output.argmax(1)\n",
        "            topk=torch.topk(output,5)\n",
        "            input=top1\n",
        "            if len(outputs)>max_len:\n",
        "                stop=True\n",
        "            outputs.append(top1.cpu().tolist())\n",
        "        outputs=(np.array(outputs).transpose().tolist())\n",
        "        return [\" \".join(i) for i in optLang.decode_batch(outputs)]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KPsw_uIia3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc=Encoder(INPUT_VOCAB,ENC_EMB_DIM,HID_DIM,N_LAYERS,ENC_DROPOUT,embedding=glove_inp)\n",
        "dec=Decoder(OUTPUT_VOCAB,DEC_EMB_DIM,HID_DIM,N_LAYERS,DEC_DROPOUT,embedding=glove_opt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EKaYvC7MEWTf",
        "colab": {}
      },
      "source": [
        "model = Seq2seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5iKFm-ujBh7",
        "colab_type": "code",
        "outputId": "ac6a1c8c-7352-40d1-bae0-648487e5d08b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(10000, 300)\n",
              "    (rnn): LSTM(300, 512, num_layers=2, dropout=0.3)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10000, 300)\n",
              "    (rnn): LSTM(300, 512, num_layers=2, dropout=0.3)\n",
              "    (fc): Linear(in_features=512, out_features=10000, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1jwMr2ojFmq",
        "colab_type": "code",
        "outputId": "519c918b-8fc0-4ecd-d925-c8bde179f108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 18,666,640 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj_8v18_jJhh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdVB_ckAjLSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = optLang.word2idx[optLang.special[\"pad_token\"]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zDJJdwnjiGq",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Train loop\n",
        "from tqdm import tqdm\n",
        "def train(model,iterator,optimizer,criterion,clip,exp):\n",
        "    model.train()\n",
        "    epoch_loss=0\n",
        "    print(\"Bathces: \",len(iterator))\n",
        "    for i,(x,y,_) in tqdm(enumerate(iterator)):\n",
        "        src=x.to(device)\n",
        "        trg=y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output=model(src,trg)\n",
        "        trg=trg.transpose(0,1)\n",
        "        output_dim=output.shape[-1]\n",
        "        output=output[1:].view(-1,output_dim)\n",
        "        trg=trg[1:].contiguous().view(-1)\n",
        "        loss=criterion(output,trg)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss+=loss.item()\n",
        "        if exp!=None:\n",
        "            exp.metric(\"batch\",i)\n",
        "            exp.metric(\"loss\",loss.item())\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG2iHXN6l8Gu",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Eval loop\n",
        "def eval(model,iterator,criterion,exp):\n",
        "    model.eval()\n",
        "    epoch_loss=0\n",
        "    print(\"Bathces: \",len(iterator))\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y,_) in enumerate(iterator):\n",
        "            src=x.to(device)\n",
        "            trg=y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output=model(src,trg)\n",
        "            trg=trg.transpose(0,1)\n",
        "            output_dim=output.shape[-1]\n",
        "            output=output[1:].view(-1,output_dim)\n",
        "            trg=trg[1:].contiguous().view(-1)\n",
        "            loss=criterion(output,trg)\n",
        "            epoch_loss+=loss.item()\n",
        "            if exp!=None:\n",
        "                exp.metric(\"batch\",i)\n",
        "                exp.metric(\"loss\",loss.item())\n",
        "        inp=random.choice(input_)\n",
        "        print(\"Input\",inp)\n",
        "        print(\"Output\",model.decode(inp,inpLang,optLang))\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjH-SAP0gKMY",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test loop\n",
        "def create_test_df(model,iterator,inpLang,optLang,save_path=\"./opt.csv\"):\n",
        "    model.eval()\n",
        "    all_data=[]\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(iterator):\n",
        "            pred=model.batch_decode(x,inpLang,optLang)\n",
        "            data=list(zip(x,y,pred))\n",
        "            all_data+=data\n",
        "    df=pd.DataFrame(all_data,columns=[\"input\",\"traget\",\"pred\"])\n",
        "    df.to_csv(save_path)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt5sEOQSlVvy",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title timer\n",
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Lbg16PplZSs",
        "colab_type": "code",
        "outputId": "dc46aad5-9eca-430b-ee8c-04c149a607dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        }
      },
      "source": [
        "model_path=\"/content/drive/My Drive/ALDA Project/seq2seq_vanilla/\"\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path)\n",
        "import math\n",
        "import pandas\n",
        "import pickle\n",
        "\n",
        "HYPERDASH=False\n",
        "if HYPERDASH==True:\n",
        "    from hyperdash import Experiment\n",
        "    exp = Experiment(\"Question Generation -seq2seq vanilla\")\n",
        "else:\n",
        "    exp=None\n",
        "EPOCHS=5\n",
        "CLIP=1\n",
        "best_valid_loss=float('inf')\n",
        "for epoch in range(EPOCHS):\n",
        "    if HYPERDASH:\n",
        "        exp.metric(\"epoch\",epoch)\n",
        "    st_time=time.time()\n",
        "    train_loss=train(model,train_dataloader,optimizer,criterion,CLIP,exp)\n",
        "    valid_loss=eval(model,test_dataloader,criterion,exp)\n",
        "    e_time=time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(st_time, e_time)\n",
        "    if valid_loss<best_valid_loss:\n",
        "        best_valid_loss=valid_loss\n",
        "        torch.save(model.state_dict(),model_path+\"model.pt\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "with open(model_path+\"inpLang.p\",'wb') as f:\n",
        "    pickle.dump(inpLang,f)\n",
        "with open(model_path+\"optLang.p\",'wb') as f:\n",
        "    pickle.dump(optLang,f)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bathces:  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12it [00:08,  1.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bathces:  1\n",
            "Input Look at us.\n",
            "Output tom . . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 9s\n",
            "\tTrain Loss: 6.294 | Train PPL: 541.366\n",
            "\t Val. Loss: 4.516 |  Val. PPL:  91.482\n",
            "Bathces:  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12it [00:08,  1.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bathces:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input Get away!\n",
            "Output ¡ . . <EOS>\n",
            "Epoch: 02 | Time: 0m 8s\n",
            "\tTrain Loss: 4.203 | Train PPL:  66.881\n",
            "\t Val. Loss: 4.471 |  Val. PPL:  87.476\n",
            "Bathces:  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "12it [00:08,  1.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Bathces:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Input I work.\n",
            "Output ¡ . . <EOS>\n",
            "Epoch: 03 | Time: 0m 8s\n",
            "\tTrain Loss: 3.989 | Train PPL:  54.021\n",
            "\t Val. Loss: 4.510 |  Val. PPL:  90.889\n",
            "Bathces:  12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2it [00:01,  1.21it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-a2536bcdac63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mst_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCLIP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0me_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-dc6fb1a160dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip, exp)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtrg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-8d6ff774fbb1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mteacher_force\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-93b83874524b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, cell)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0membedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprediction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m# prediction=self.softmax(prediction)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1368\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4hJjyQa5KkD",
        "colab_type": "code",
        "outputId": "5f670c2e-d890-4462-d7ba-732ca86bfa62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.decode(\"i grew up in india\",inpLang,optLang)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¡ . . <EOS>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBUc0rDwcT8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "create_test_df(model,final_dataloader,inpLang,optLang,save_path=model_path+\"output.csv\")\n",
        "if HYPERDASH:\n",
        "    exp.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5CkzAODOMba",
        "colab_type": "text"
      },
      "source": [
        "#Seq2seq Context Transfer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rfnx6aSYH0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = INPUT_VOCAB\n",
        "OUTPUT_DIM = OUTPUT_VOCAB\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkhtly_vOU85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_dim,emb_dim,hid_dim,dropout):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        self.input_dim=input_dim\n",
        "        self.embedding=nn.Embedding(input_dim,emb_dim)\n",
        "        self.rnn=nn.GRU(emb_dim,hid_dim)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,src):\n",
        "        src=src.transpose(0,1)\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        output,hidden=self.rnn(embedded)\n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YbNKT9gSJBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_dim,emb_dim,hid_dim,dropout):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.hid_dim=hid_dim\n",
        "        self.output_dim=output_dim\n",
        "        self.embedding=nn.Embedding(output_dim,emb_dim)\n",
        "        self.rnn=nn.GRU(emb_dim+hid_dim,hid_dim)\n",
        "        self.fc=nn.Linear(emb_dim+hid_dim*2,output_dim)\n",
        "        self.softmax=nn.Softmax(dim=1)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,input,hidden,context):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded=self.embedding(input)\n",
        "        emb_con = torch.cat((embedded, context), dim = 2)\n",
        "        output, hidden = self.rnn(emb_con, hidden)\n",
        "        output = torch.cat((embedded.squeeze(0), hidden.squeeze(0), context.squeeze(0)), dim = 1)\n",
        "        prediction = self.fc(output)\n",
        "        predcition=self.softmax(prediction)\n",
        "        return prediction, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDwSCGbEYoBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder,device):\n",
        "        super(Seq2seq,self).__init__()\n",
        "        self.encoder=encoder\n",
        "        self.decoder=decoder\n",
        "        self.device=device\n",
        "    def forward(self,src,trg,teacher_forcing_ratio=0.5):\n",
        "        trg=trg.transpose(0,1)\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        context = self.encoder(src)\n",
        "        hidden = context\n",
        "        input = trg[0,:]\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden = self.decoder(input, hidden, context)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1) \n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs\n",
        "    def decode(self,input,inpLang,optLang):\n",
        "        stop_token=optLang.word2idx[optLang.special[\"eos_token\"]]\n",
        "        start_token=optLang.word2idx[optLang.special[\"init_token\"]]\n",
        "        src=torch.tensor(inpLang.encode(input),device=self.device).unsqueeze(1).transpose(0,1)\n",
        "        batch_size=src.shape[1]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        hidden=self.encoder(src)\n",
        "        context=hidden\n",
        "        input=torch.tensor(start_token,device=self.device).unsqueeze(0)\n",
        "        stop=False\n",
        "        outputs=[]\n",
        "        while not stop:\n",
        "            output,hidden=self.decoder(input,hidden,context)\n",
        "            top1=output.argmax(1)\n",
        "            topk=torch.topk(output,5)\n",
        "            input=top1\n",
        "            if top1.item()==stop_token or len(outputs)>10:\n",
        "                stop=True\n",
        "            outputs.append(top1.item())\n",
        "        return \" \".join(optLang.decode(outputs))\n",
        "    def batch_decode(self,input,inpLang,optLang,max_len=10):\n",
        "        stop_token=optLang.word2idx[optLang.special[\"eos_token\"]]\n",
        "        start_token=optLang.word2idx[optLang.special[\"init_token\"]]\n",
        "        src=torch.tensor(inpLang.encode_batch(input),device=self.device)\n",
        "        batch_size=src.shape[0]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        hidden=self.encoder(src)\n",
        "        context=hidden\n",
        "        input=torch.tensor([start_token]*batch_size,device=self.device)\n",
        "        stop=False\n",
        "        outputs=[]\n",
        "        while not stop:\n",
        "            output,hidden=self.decoder(input,hidden,context)\n",
        "            top1=output.argmax(1)\n",
        "            topk=torch.topk(output,5)\n",
        "            input=top1\n",
        "            if len(outputs)>max_len:\n",
        "                stop=True\n",
        "            outputs.append(top1.cpu().tolist())\n",
        "        outputs=(np.array(outputs).transpose().tolist())\n",
        "        return [\" \".join(i) for i in optLang.decode_batch(outputs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1c1-m8NW26J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = Seq2seq(enc, dec, device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFGa14oid3uX",
        "colab_type": "code",
        "outputId": "f1f1bee6-e9ca-4f7a-ca2a-57054b64369e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(45000, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(30000, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc): Linear(in_features=1280, out_features=30000, bias=True)\n",
              "    (softmax): Softmax(dim=1)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OYljiFceChe",
        "colab_type": "code",
        "outputId": "dbd7a462-670d-4f63-c1dd-df98c458536c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 60,781,872 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aISfyr2eFok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nm0yXjEeSt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = optLang.word2idx[optLang.special[\"pad_token\"]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeEcB_W0e_j9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Train loop\n",
        "def train(model,iterator,optimizer,criterion,clip,exp):\n",
        "    model.train()\n",
        "    epoch_loss=0\n",
        "    print(\"Total Batches - \",len(iterator))\n",
        "    for i,(x,y,_) in enumerate(iterator):\n",
        "        src=x.to(device)\n",
        "        trg=y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output=model(src,trg)\n",
        "        trg=trg.transpose(0,1)\n",
        "        output_dim=output.shape[-1]\n",
        "        output=output[1:].view(-1,output_dim)\n",
        "        trg=trg[1:].contiguous().view(-1)\n",
        "        loss=criterion(output,trg)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss+=loss.item()\n",
        "        exp.metric(\"batch\",i)\n",
        "        exp.metric(\"loss\",loss.item())\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHxEl2SjfAOE",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Eval loop\n",
        "\n",
        "def eval(model,iterator,criterion,exp):\n",
        "    model.eval()\n",
        "    epoch_loss=0\n",
        "    print(\"Total Batches - \",len(iterator))\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y,_) in enumerate(iterator):\n",
        "            src=x.to(device)\n",
        "            trg=y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output=model(src,trg)\n",
        "            trg=trg.transpose(0,1)\n",
        "            output_dim=output.shape[-1]\n",
        "            output=output[1:].view(-1,output_dim)\n",
        "            trg=trg[1:].contiguous().view(-1)\n",
        "            loss=criterion(output,trg)\n",
        "            epoch_loss+=loss.item()\n",
        "            exp.metric(\"val_batch\",i)\n",
        "            exp.metric(\"val_loss\",loss.item())\n",
        "        inp=random.choice(input_)\n",
        "        print(\"Input:\",inp)\n",
        "        print(\"Output:\",model.decode(inp,inpLang,optLang))\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ta3NS5CK_d",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Test loop\n",
        "def create_test_df(model,iterator,inpLang,optLang,save_path=\"./opt.csv\"):\n",
        "    model.eval()\n",
        "    all_data=[]\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(iterator):\n",
        "            pred=model.batch_decode(x,inpLang,optLang)\n",
        "            data=list(zip(x,y,pred))\n",
        "            all_data+=data\n",
        "    df=pd.DataFrame(all_data,columns=[\"input\",\"traget\",\"pred\"])\n",
        "    df.to_csv(save_path)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4OxcFvqfCIf",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title timer\n",
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99EVjAJUfENP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path=\"/content/drive/My Drive/ALDA Project/seq2seq_with_context/\"\n",
        "if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path)\n",
        "import math\n",
        "import pandas\n",
        "import pickle\n",
        "HYPERDASH=True\n",
        "if HYPERDASH==True:\n",
        "    from hyperdash import Experiment\n",
        "    exp = Experiment(\"Question Generation -seq2seq context\")\n",
        "else:\n",
        "    exp=None\n",
        "EPOCHS=1\n",
        "CLIP=1\n",
        "best_valid_loss=float('inf')\n",
        "for epoch in range(EPOCHS):\n",
        "    if HYPERDASH:\n",
        "        exp.metric(\"epoch\",epoch)\n",
        "    st_time=time.time()\n",
        "    train_loss=train(model,train_dataloader,optimizer,criterion,CLIP,exp)\n",
        "    valid_loss=eval(model,test_dataloader,criterion,exp)\n",
        "    e_time=time.time()\n",
        "    epoch_mins, epoch_secs = epoch_time(st_time, e_time)\n",
        "    if valid_loss<best_valid_loss:\n",
        "        best_valid_loss=valid_loss\n",
        "        torch.save(model.state_dict(),model_path+\"model.pt\")\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "with open(model_path+\"inpLang.p\",'wb') as f:\n",
        "    pickle.dump(inpLang,f)\n",
        "with open(model_path+\"optLang.p\",'wb') as f:\n",
        "    pickle.dump(optLang,f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQH6togBfHHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "create_test_df(model,final_dataloader,inpLang,optLang,save_path=model_path+\"output.csv\")\n",
        "if HYPERDASH:\n",
        "    exp.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY9SnfdIQ-v3",
        "colab_type": "code",
        "outputId": "6e65cb33-5bf2-4f89-d8e4-ace107b63325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.decode(\"We have presented a fully data-driven neural networks approach to automatic question generation for reading comprehension\",inpLang,optLang)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'what was the name of the ? <EOS>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX7azUEHWhEZ",
        "colab_type": "text"
      },
      "source": [
        "# Seq2seq with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIzg9Gx2vOb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = INPUT_VOCAB\n",
        "OUTPUT_DIM = OUTPUT_VOCAB\n",
        "ENC_EMB_DIM = 300\n",
        "DEC_EMB_DIM = 300\n",
        "ENC_HID_DIM = 600\n",
        "DEC_HID_DIM = 600\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "N_LAYERS=1\n",
        "LR=1e-3\n",
        "EPOCHS=8\n",
        "HYPERDASH=True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOsPoZ2Ft3N_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,inp_dim,emb_dim,hid_dim_enc,\n",
        "                 hid_dim_dec,dropout=0.5,embedding=None):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.hid_dim=hid_dim_enc\n",
        "        self.emb_dim=emb_dim\n",
        "        self.embedding=nn.Embedding(inp_dim,emb_dim)\n",
        "        if embedding!=None:\n",
        "            self.embedding.load_state_dict({'weight': embedding})\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        self.rnn=nn.GRU(emb_dim,hid_dim_enc,bidirectional=True)\n",
        "        self.fc=nn.Linear(2*hid_dim_enc,hid_dim_dec)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self,src,src_len):\n",
        "        src=src.transpose(0,1) #(seq len , batch)\n",
        "        embedded=self.dropout(self.embedding(src)) #(seq_len , batch , emb_dim)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len)\n",
        "        packed_outputs,hidden=self.rnn(packed_embedded)  # (seq len , batch , 2* enc_hidden_dec) , (2, batch , enc_hid_dim)\n",
        "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs,)\n",
        "        hidden = torch.tanh(self.fc(\n",
        "                torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1))) # (batch , dec_hid_dim)\n",
        "        return output,hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLYzgg881l77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,hid_dim_enc,hid_dim_dec):\n",
        "        super(Attention,self).__init__()\n",
        "        self.attention=nn.Linear(2*hid_dim_enc+hid_dim_dec,hid_dim_dec)\n",
        "        self.v=nn.Linear(hid_dim_dec,1,bias=False)\n",
        "    def forward(self,enc_opt,dec_hid,mask):\n",
        "        batch_size=enc_opt.shape[1]\n",
        "        src_len=enc_opt.shape[0]\n",
        "\n",
        "        dec_hid=dec_hid.unsqueeze(1).repeat(1,src_len,1) #(batch, src_len , dec_hid_dim)\n",
        "        enc_opt=enc_opt.permute(1,0,2) #(batch, src len, 2*enc_hid_dim)\n",
        "\n",
        "        energy = torch.tanh(self.attention(torch.cat((enc_opt,dec_hid),dim=2))) #(batch, src_len , dec_hid_dim)\n",
        "        attention=self.v(energy).squeeze(2) #(batch , srclen)\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        attention=torch.softmax(attention,dim=1) #(batch , srclen)\n",
        "        return attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5uoJiC3FkGF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,opt_dim, emb_dim, hid_dim_enc, hid_dim_dec,dropout=0.5,embedding=None  ):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.output_dim=opt_dim\n",
        "        self.embedding=nn.Embedding(opt_dim,emb_dim)\n",
        "        if embedding!=None:\n",
        "            self.embedding.load_state_dict({'weight': embedding})\n",
        "            self.embedding.weight.requires_grad = False\n",
        "        self.rnn=nn.GRU(emb_dim+2*hid_dim_enc,hid_dim_dec)\n",
        "        self.fc=nn.Linear(emb_dim+2*hid_dim_enc+hid_dim_dec,opt_dim)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "        self.attention=Attention(hid_dim_enc,hid_dim_dec)\n",
        "    def forward(self,trg,enc_opt,enc_hid,mask):\n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
        "        embedded=self.dropout(self.embedding(trg)).unsqueeze(0) #(1, batch, emb_dim)\n",
        "        att=self.attention(enc_opt,enc_hid,mask).unsqueeze(1) # (batch, 1, src_len)\n",
        "        enc_opt = enc_opt.permute(1, 0, 2)     #(batch, src len, 2*enc_hid_dim)    \n",
        "        weighted = torch.bmm(att, enc_opt)    #(batch, 1 , 2*enc_hid_dim)\n",
        "        weighted = weighted.permute(1, 0, 2)    #(1, batch, 2*enc_hid_dim)\n",
        "        rnn_inp=torch.cat((embedded,weighted),dim=2) #(1, batch, 2*enc_hid_dim+emb_dim)\n",
        "        opt,hid=self.rnn(rnn_inp,enc_hid.unsqueeze(0)) # (1,batch, dec_hid_dim)  (1,batch,dec_hid_dim)\n",
        "        pred=self.fc((torch.cat((opt,weighted,embedded),dim=2).squeeze(0)))\n",
        "        return pred,hid.squeeze(0),att.squeeze(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pnj_i3xVPDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder,src_pad_idx,device):\n",
        "        super(Seq2seq,self).__init__()\n",
        "        self.encoder=encoder\n",
        "        self.decoder=decoder\n",
        "        self.device=device\n",
        "        self.src_pad_idx=src_pad_idx\n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.src_pad_idx)\n",
        "        return mask\n",
        "    def forward(self,src,trg,src_len,teacher_forcing_ratio=0.75):\n",
        "        trg=trg.transpose(0,1)\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        enc_opt,hidden = self.encoder(src,src_len)\n",
        "        input = trg[0,:]\n",
        "        mask = self.create_mask(src[:,:torch.max(src_len)])\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, _ = self.decoder(input, enc_opt, hidden,mask)\n",
        "            outputs[t] = output\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1) \n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs\n",
        "    def decode(self,input,inpLang,optLang,max_len=MAX_LEN):\n",
        "        stop_token=optLang.word2idx[optLang.special[\"eos_token\"]]\n",
        "        start_token=optLang.word2idx[optLang.special[\"init_token\"]]\n",
        "        src=torch.tensor(inpLang.encode(input),device=self.device).unsqueeze(1).transpose(0,1)\n",
        "        batch_size=src.shape[0]\n",
        "        src_len=src.shape[1]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        src_len_enc = torch.LongTensor([src_len]).to(self.device)\n",
        "        enc_out,hidden = self.encoder(src,src_len_enc)\n",
        "        input=torch.tensor(start_token,device=self.device).unsqueeze(0)\n",
        "        mask=self.create_mask(src)\n",
        "        stop=False\n",
        "        outputs=[]\n",
        "        attentions = torch.zeros(max_len+1, 1, src_len).to(self.device)\n",
        "        i=0\n",
        "        while not stop:\n",
        "            output, hidden, attention = self.decoder(input, enc_out,hidden,mask)\n",
        "            attentions[i] = attention\n",
        "            top1=output.argmax(1)\n",
        "            topk=torch.topk(output,5)\n",
        "            input=top1\n",
        "            if top1.item()==stop_token or len(outputs)>=max_len:\n",
        "                stop=True\n",
        "            outputs.append(top1.item())\n",
        "            i+=1\n",
        "        return \" \".join(optLang.decode(outputs)),attentions\n",
        "    def batch_decode(self,input,inpLang,optLang,max_len=MAX_LEN):\n",
        "        stop_token=optLang.word2idx[optLang.special[\"eos_token\"]]\n",
        "        start_token=optLang.word2idx[optLang.special[\"init_token\"]]\n",
        "        src=torch.tensor(inpLang.encode_batch(input),device=self.device)\n",
        "        batch_size=src.shape[0]\n",
        "        src_len=max_len\n",
        "        src_len_enc = torch.LongTensor([src_len]*batch_size).to(self.device)\n",
        "        mask=self.create_mask(src)\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        enc_opt,hidden = self.encoder(src,src_len_enc)\n",
        "        input=torch.tensor([start_token]*batch_size,device=self.device)\n",
        "        stop=False\n",
        "        outputs=[]\n",
        "        while not stop:\n",
        "            output, hidden, _ = self.decoder(input, enc_opt,hidden,mask)\n",
        "            top1=output.argmax(1)\n",
        "            topk=torch.topk(output,5)\n",
        "            input=top1\n",
        "            if len(outputs)>=max_len:\n",
        "                stop=True\n",
        "            outputs.append(top1.cpu().tolist())\n",
        "        outputs=(np.array(outputs).transpose().tolist())\n",
        "        return [\" \".join(i) for i in optLang.decode_batch(outputs)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVa6bs5GYWKa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, dropout=ENC_DROPOUT,embedding=glove_inp)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, dropout=DEC_DROPOUT,embedding=glove_opt)\n",
        "\n",
        "model = Seq2seq(enc, dec,inpLang.word2idx[inpLang.special[\"pad_token\"]], device).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yx1UrOSICPX",
        "colab_type": "code",
        "outputId": "39d96351-420a-4bc1-897c-fee0c2bc68fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "            \n",
        "model.apply(init_weights)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(10000, 300)\n",
              "    (rnn): GRU(300, 600, bidirectional=True)\n",
              "    (fc): Linear(in_features=1200, out_features=600, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10000, 300)\n",
              "    (rnn): GRU(1500, 600)\n",
              "    (fc): Linear(in_features=2100, out_features=10000, bias=True)\n",
              "    (dropout): Dropout(p=0.2, inplace=False)\n",
              "    (attention): Attention(\n",
              "      (attention): Linear(in_features=1800, out_features=600, bias=True)\n",
              "      (v): Linear(in_features=600, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAiaEtmeYqmb",
        "colab_type": "code",
        "outputId": "d6ff7163-51f8-49ca-e98e-b20b27c17297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 35,842,600 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoI6ewVTYwuW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=LR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGODTgGrY1lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = optLang.word2idx[optLang.special[\"pad_token\"]])\n",
        "def sort_batch(X, y, lengths):\n",
        "    lengths, indx = lengths.sort(dim=0, descending=True)\n",
        "    X = X[indx]\n",
        "    y = y[indx]\n",
        "    return X, y, lengths # transpose (batch x seq) to (seq x batch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XA3d611Y2Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,iterator,optimizer,criterion,clip,exp):\n",
        "    model.train()\n",
        "    epoch_loss=0\n",
        "    print(\"Total Batches - \",len(iterator))\n",
        "    for i,(x,y,x_l) in enumerate(iterator):\n",
        "        x,y,x_l=sort_batch(x,y,x_l)\n",
        "        src=x.to(device)\n",
        "        trg=y.to(device)\n",
        "        src_len=x_l.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output=model(src,trg,src_len)\n",
        "        trg=trg.transpose(0,1)\n",
        "        output_dim=output.shape[-1]\n",
        "        output=output[1:].view(-1,output_dim)\n",
        "        trg=trg[1:].contiguous().view(-1)\n",
        "        loss=criterion(output,trg)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss+=loss.item()\n",
        "        if exp!=None:\n",
        "            exp.metric(\"batch\",i)\n",
        "            exp.metric(\"loss\",loss.item())\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOtXnYfQY5Pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model,iterator,criterion,exp):\n",
        "    model.eval()\n",
        "    epoch_loss=0\n",
        "    print(\"Total Batches - \",len(iterator))\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y,x_l) in enumerate(iterator):\n",
        "            x,y,x_l=sort_batch(x,y,x_l)\n",
        "            src=x.to(device)\n",
        "            trg=y.to(device)\n",
        "            src_len=x_l.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output=model(src,trg,src_len)\n",
        "            trg=trg.transpose(0,1)\n",
        "            output_dim=output.shape[-1]\n",
        "            output=output[1:].view(-1,output_dim)\n",
        "            trg=trg[1:].contiguous().view(-1)\n",
        "            loss=criterion(output,trg)\n",
        "            epoch_loss+=loss.item()\n",
        "            if exp!=None:\n",
        "                exp.metric(\"val_batch\",i)\n",
        "                exp.metric(\"val_loss\",loss.item())\n",
        "        inp=random.choice(input_)\n",
        "        print(\"Input:\",inp)\n",
        "        print(\"Output:\",model.decode(inp,inpLang,optLang)[0])\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_xg01-SY8Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_test_df(model,iterator,inpLang,optLang,save_path=\"./opt.csv\"):\n",
        "    model.eval()\n",
        "    all_data=[]\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y) in enumerate(iterator):\n",
        "            pred=model.batch_decode(x,inpLang,optLang)\n",
        "            data=list(zip(x,y,pred))\n",
        "            all_data+=data\n",
        "    df=pd.DataFrame(all_data,columns=[\"input\",\"traget\",\"pred\"])\n",
        "    df.to_csv(save_path)\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMqLlOelY_Ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFsmSgrzZB93",
        "colab_type": "code",
        "outputId": "ab93b248-b925-4671-d004-11e9da65c6df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "try:\n",
        "    nmt_path=\"_translation\" if testNMT else \"\" \n",
        "    current_run= os.walk(f\"/content/drive/My Drive/ALDA Project/seq2seq_with_attention{nmt_path}/\")\n",
        "    folders=[int(i) for i in list(current_run)[0][1] if i.isdigit()]\n",
        "    cur_run=max(folders)+1 if folders!=[] else 0\n",
        "    model_path=f\"/content/drive/My Drive/ALDA Project/seq2seq_with_attention{nmt_path}/{str(cur_run)}/\"\n",
        "    if not os.path.exists(model_path):\n",
        "        os.mkdir(model_path)\n",
        "    import math\n",
        "    import pandas\n",
        "    import pickle\n",
        "    if HYPERDASH==True:\n",
        "        from hyperdash import Experiment\n",
        "        exp = Experiment(\"Question Generation -seq2seq attention\")\n",
        "        log_hyperdash(exp,**get_params())\n",
        "    else:\n",
        "        exp=None\n",
        "    CLIP=1\n",
        "    best_valid_loss=float('inf')\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "        if HYPERDASH:\n",
        "            exp.metric(\"epoch\",epoch)\n",
        "        st_time=time.time()\n",
        "        train_loss=train(model,train_dataloader,optimizer,criterion,CLIP,exp)\n",
        "        valid_loss=eval(model,test_dataloader,criterion,exp)\n",
        "        e_time=time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(st_time, e_time)\n",
        "        if valid_loss<best_valid_loss:\n",
        "            best_valid_loss=valid_loss\n",
        "            torch.save(model.state_dict(),model_path+\"model.pt\")\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "    with open(model_path+\"inpLang.p\",'wb') as f:\n",
        "        pickle.dump(inpLang,f)\n",
        "    with open(model_path+\"optLang.p\",'wb') as f:\n",
        "        pickle.dump(optLang,f)\n",
        "    log_param(model_path+\"param.txt\",**get_params())\n",
        "    \n",
        "except:\n",
        "    print(f\"Training stopped at Epoch {epoch}\")\n",
        "finally:\n",
        "    if HYPERDASH:\n",
        "        exp.end()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "| loss:   0.997820 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   0.862271 |\n",
            "| batch: 573.000000 |\n",
            "| loss:   0.871379 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   0.715866 |\n",
            "| batch: 575.000000 |\n",
            "| loss:   0.715259 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   0.743179 |\n",
            "| batch: 577.000000 |\n",
            "| loss:   0.865101 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   0.764572 |\n",
            "| batch: 579.000000 |\n",
            "| loss:   0.871656 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   0.704666 |\n",
            "| batch: 581.000000 |\n",
            "| loss:   1.339308 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   0.877910 |\n",
            "| batch: 583.000000 |\n",
            "| loss:   0.971847 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   0.741154 |\n",
            "| batch: 585.000000 |\n",
            "| loss:   0.882445 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   0.721776 |\n",
            "| batch: 587.000000 |\n",
            "| loss:   0.840740 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   0.827918 |\n",
            "| batch: 589.000000 |\n",
            "| loss:   0.802070 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   0.907488 |\n",
            "| batch: 591.000000 |\n",
            "| loss:   0.808083 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   0.717908 |\n",
            "| batch: 593.000000 |\n",
            "| loss:   0.910171 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   0.804243 |\n",
            "| batch: 595.000000 |\n",
            "| loss:   0.802594 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   0.806810 |\n",
            "| batch: 597.000000 |\n",
            "| loss:   0.862274 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   0.840095 |\n",
            "| batch: 599.000000 |\n",
            "| loss:   0.777316 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   1.050304 |\n",
            "| batch: 601.000000 |\n",
            "| loss:   1.088766 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   1.175253 |\n",
            "| batch: 603.000000 |\n",
            "| loss:   0.956926 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   0.705737 |\n",
            "| batch: 605.000000 |\n",
            "| loss:   0.868174 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   0.751421 |\n",
            "| batch: 607.000000 |\n",
            "| loss:   0.836587 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   0.870009 |\n",
            "| batch: 609.000000 |\n",
            "| loss:   1.272603 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   0.885168 |\n",
            "| batch: 611.000000 |\n",
            "| loss:   0.818344 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   1.006614 |\n",
            "| batch: 613.000000 |\n",
            "| loss:   0.910299 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   0.713548 |\n",
            "| batch: 615.000000 |\n",
            "| loss:   1.024112 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   0.969032 |\n",
            "| batch: 617.000000 |\n",
            "| loss:   0.877339 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   1.040149 |\n",
            "| batch: 619.000000 |\n",
            "| loss:   0.788912 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   0.956188 |\n",
            "| batch: 621.000000 |\n",
            "| loss:   0.992781 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   0.928027 |\n",
            "| batch: 623.000000 |\n",
            "| loss:   0.938588 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   0.732912 |\n",
            "| batch: 625.000000 |\n",
            "| loss:   0.779306 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   0.891666 |\n",
            "| batch: 627.000000 |\n",
            "| loss:   0.850295 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   1.016586 |\n",
            "| batch: 629.000000 |\n",
            "| loss:   0.944932 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   0.916168 |\n",
            "| batch: 631.000000 |\n",
            "| loss:   0.942646 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   0.719976 |\n",
            "| batch: 633.000000 |\n",
            "| loss:   0.911555 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   0.932766 |\n",
            "| batch: 635.000000 |\n",
            "| loss:   0.792437 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   0.788060 |\n",
            "| batch: 637.000000 |\n",
            "| loss:   0.817194 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   0.882303 |\n",
            "| batch: 639.000000 |\n",
            "| loss:   0.934531 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   0.697136 |\n",
            "| batch: 641.000000 |\n",
            "| loss:   1.082114 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   0.825438 |\n",
            "| batch: 643.000000 |\n",
            "| loss:   0.967877 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   0.918236 |\n",
            "| batch: 645.000000 |\n",
            "| loss:   0.882030 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   0.678328 |\n",
            "| batch: 647.000000 |\n",
            "| loss:   0.809878 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   0.855631 |\n",
            "| batch: 649.000000 |\n",
            "| loss:   0.934978 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   0.636379 |\n",
            "| batch: 651.000000 |\n",
            "| loss:   0.936721 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   0.975460 |\n",
            "| batch: 653.000000 |\n",
            "| loss:   0.841654 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   0.866906 |\n",
            "| batch: 655.000000 |\n",
            "| loss:   0.776976 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   0.810826 |\n",
            "| batch: 657.000000 |\n",
            "| loss:   0.674880 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   1.269859 |\n",
            "| batch: 659.000000 |\n",
            "| loss:   0.771774 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   0.982854 |\n",
            "| batch: 661.000000 |\n",
            "| loss:   0.779297 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   0.890860 |\n",
            "| batch: 663.000000 |\n",
            "| loss:   0.711097 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   0.850235 |\n",
            "| batch: 665.000000 |\n",
            "| loss:   0.733674 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   0.817802 |\n",
            "| batch: 667.000000 |\n",
            "| loss:   0.953817 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   0.975563 |\n",
            "| batch: 669.000000 |\n",
            "| loss:   0.987312 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   1.944199 |\n",
            "| batch: 671.000000 |\n",
            "| loss:   0.714365 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   1.208298 |\n",
            "| batch: 673.000000 |\n",
            "| loss:   0.909268 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   0.920396 |\n",
            "| batch: 675.000000 |\n",
            "| loss:   0.814593 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   0.848359 |\n",
            "| batch: 677.000000 |\n",
            "| loss:   0.864835 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   1.080656 |\n",
            "| batch: 679.000000 |\n",
            "| loss:   0.746454 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   0.902484 |\n",
            "| batch: 681.000000 |\n",
            "| loss:   0.849913 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   0.909414 |\n",
            "| batch: 683.000000 |\n",
            "| loss:   1.014317 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   0.866766 |\n",
            "| batch: 685.000000 |\n",
            "| loss:   0.789889 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   0.866284 |\n",
            "| batch: 687.000000 |\n",
            "| loss:   0.997435 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   1.182962 |\n",
            "| batch: 689.000000 |\n",
            "| loss:   0.914311 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   1.003837 |\n",
            "| batch: 691.000000 |\n",
            "| loss:   0.641628 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   0.789038 |\n",
            "| batch: 693.000000 |\n",
            "| loss:   0.821978 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   0.922479 |\n",
            "| batch: 695.000000 |\n",
            "| loss:   0.887942 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   1.069380 |\n",
            "| batch: 697.000000 |\n",
            "| loss:   0.756911 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   0.955162 |\n",
            "| batch: 699.000000 |\n",
            "| loss:   0.884750 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   0.860173 |\n",
            "| batch: 701.000000 |\n",
            "| loss:   0.968786 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   0.843269 |\n",
            "| batch: 703.000000 |\n",
            "| loss:   1.051835 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   0.781160 |\n",
            "| batch: 705.000000 |\n",
            "| loss:   1.096568 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   1.303381 |\n",
            "| batch: 707.000000 |\n",
            "| loss:   0.877442 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   0.715822 |\n",
            "| batch: 709.000000 |\n",
            "| loss:   0.844315 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   0.755570 |\n",
            "| batch: 711.000000 |\n",
            "| loss:   0.911573 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   0.788405 |\n",
            "| batch: 713.000000 |\n",
            "| loss:   0.990328 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   0.877337 |\n",
            "| batch: 715.000000 |\n",
            "| loss:   0.949999 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   1.051517 |\n",
            "| batch: 717.000000 |\n",
            "| loss:   0.778878 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   1.084523 |\n",
            "| batch: 719.000000 |\n",
            "| loss:   0.710037 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   0.829357 |\n",
            "| batch: 721.000000 |\n",
            "| loss:   0.734389 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   1.368951 |\n",
            "| batch: 723.000000 |\n",
            "| loss:   1.041217 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   0.715787 |\n",
            "| batch: 725.000000 |\n",
            "| loss:   1.003153 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   1.052996 |\n",
            "| batch: 727.000000 |\n",
            "| loss:   0.932079 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   0.906111 |\n",
            "| batch: 729.000000 |\n",
            "| loss:   0.986713 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   0.746668 |\n",
            "| batch: 731.000000 |\n",
            "| loss:   0.925288 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   0.857977 |\n",
            "| batch: 733.000000 |\n",
            "| loss:   1.036635 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   0.719541 |\n",
            "| batch: 735.000000 |\n",
            "| loss:   0.853158 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   0.861095 |\n",
            "| batch: 737.000000 |\n",
            "| loss:   0.840931 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   1.075375 |\n",
            "| batch: 739.000000 |\n",
            "| loss:   0.881725 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   0.972831 |\n",
            "| batch: 741.000000 |\n",
            "| loss:   0.913303 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   1.112135 |\n",
            "| batch: 743.000000 |\n",
            "| loss:   1.072018 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   0.751786 |\n",
            "| batch: 745.000000 |\n",
            "| loss:   0.839696 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   0.814527 |\n",
            "| batch: 747.000000 |\n",
            "| loss:   0.774921 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   1.088470 |\n",
            "| batch: 749.000000 |\n",
            "| loss:   0.827563 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   0.878626 |\n",
            "| batch: 751.000000 |\n",
            "| loss:   0.971383 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   0.781579 |\n",
            "| batch: 753.000000 |\n",
            "| loss:   1.030515 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   0.844826 |\n",
            "| batch: 755.000000 |\n",
            "| loss:   0.788640 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   1.156732 |\n",
            "| batch: 757.000000 |\n",
            "| loss:   0.907259 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   0.870176 |\n",
            "| batch: 759.000000 |\n",
            "| loss:   1.288812 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   1.031939 |\n",
            "| batch: 761.000000 |\n",
            "| loss:   0.777932 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   0.913066 |\n",
            "| batch: 763.000000 |\n",
            "| loss:   0.922399 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   1.032587 |\n",
            "| batch: 765.000000 |\n",
            "| loss:   0.945801 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   1.055774 |\n",
            "| batch: 767.000000 |\n",
            "| loss:   0.773862 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   1.241849 |\n",
            "| batch: 769.000000 |\n",
            "| loss:   0.999945 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   0.945140 |\n",
            "| batch: 771.000000 |\n",
            "| loss:   1.099358 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   1.065881 |\n",
            "| batch: 773.000000 |\n",
            "| loss:   0.879564 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   0.989000 |\n",
            "| batch: 775.000000 |\n",
            "| loss:   0.807686 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   0.866480 |\n",
            "| batch: 777.000000 |\n",
            "| loss:   0.785519 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   0.845775 |\n",
            "| batch: 779.000000 |\n",
            "| loss:   0.826402 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   0.781520 |\n",
            "| batch: 781.000000 |\n",
            "| loss:   0.655309 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   0.998953 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   1.874509 |\n",
            "| val_batch:   3.000000 |\n",
            "| val_loss:   1.463852 |\n",
            "| val_batch:   6.000000 |\n",
            "| val_loss:   1.488952 |\n",
            "| val_batch:   9.000000 |\n",
            "| val_loss:   1.578858 |\n",
            "| val_batch:  12.000000 |\n",
            "| val_loss:   1.139488 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   1.233559 |\n",
            "| val_batch:  18.000000 |\n",
            "| val_loss:   1.308693 |\n",
            "| val_batch:  21.000000 |\n",
            "| val_loss:   1.304982 |\n",
            "| val_batch:  24.000000 |\n",
            "| val_loss:   1.332098 |\n",
            "| val_batch:  27.000000 |\n",
            "| val_loss:   1.494968 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   1.402401 |\n",
            "| val_batch:  33.000000 |\n",
            "| val_loss:   1.842180 |\n",
            "| val_batch:  36.000000 |\n",
            "| val_loss:   1.475488 |\n",
            "| val_batch:  39.000000 |\n",
            "| val_loss:   1.496609 |\n",
            "| val_batch:  42.000000 |\n",
            "| val_loss:   1.403495 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   1.373350 |\n",
            "| val_batch:  48.000000 |\n",
            "| val_loss:   1.747440 |\n",
            "| val_batch:  51.000000 |\n",
            "| val_loss:   1.477520 |\n",
            "| val_batch:  54.000000 |\n",
            "| val_loss:   1.414172 |\n",
            "| val_batch:  57.000000 |\n",
            "| val_loss:   1.423149 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   1.622626 |\n",
            "| val_batch:  63.000000 |\n",
            "| val_loss:   1.499336 |\n",
            "| val_batch:  66.000000 |\n",
            "| val_loss:   1.725539 |\n",
            "| val_batch:  69.000000 |\n",
            "| val_loss:   1.604424 |\n",
            "| val_batch:  72.000000 |\n",
            "| val_loss:   1.595686 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   1.683217 |\n",
            "| val_batch:  78.000000 |\n",
            "| val_loss:   1.635455 |\n",
            "| val_batch:  81.000000 |\n",
            "| val_loss:   1.545435 |\n",
            "| val_batch:  83.000000 |\n",
            "| val_loss:   1.755663 |\n",
            "| val_batch:  86.000000 |\n",
            "| val_loss:   1.369610 |\n",
            "Input: We're famished.\n",
            "Output: estamos <UNK> fácilmente . estamos <UNK> . estamos <UNK> . <UNK> . <UNK> . estamos <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . . estamos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "Epoch: 05 | Time: 16m 47s\n",
            "\tTrain Loss: 0.861 | Train PPL:   2.367\n",
            "\t Val. Loss: 1.554 |  Val. PPL:   4.732\n",
            "| epoch:   5.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   0.592119 |\n",
            "| batch:   1.000000 |\n",
            "| loss:   0.774992 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   0.554719 |\n",
            "| batch:   3.000000 |\n",
            "| loss:   0.766773 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   0.579313 |\n",
            "| batch:   5.000000 |\n",
            "| loss:   0.534202 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   0.614958 |\n",
            "| batch:   7.000000 |\n",
            "| loss:   0.654495 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   0.776261 |\n",
            "| batch:   9.000000 |\n",
            "| loss:   0.579348 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   0.677801 |\n",
            "| batch:  11.000000 |\n",
            "| loss:   0.713441 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   0.613096 |\n",
            "| batch:  13.000000 |\n",
            "| loss:   0.587197 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   0.622767 |\n",
            "| batch:  15.000000 |\n",
            "| loss:   0.477911 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   0.649399 |\n",
            "| batch:  17.000000 |\n",
            "| loss:   0.548479 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   0.548552 |\n",
            "| batch:  19.000000 |\n",
            "| loss:   0.704309 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   0.740542 |\n",
            "| batch:  21.000000 |\n",
            "| loss:   0.699188 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   0.811660 |\n",
            "| batch:  23.000000 |\n",
            "| loss:   0.551167 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   0.659976 |\n",
            "| batch:  25.000000 |\n",
            "| loss:   0.693167 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   0.457309 |\n",
            "| batch:  27.000000 |\n",
            "| loss:   0.681135 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   0.764991 |\n",
            "| batch:  29.000000 |\n",
            "| loss:   0.645988 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   0.671165 |\n",
            "| batch:  31.000000 |\n",
            "| loss:   0.675048 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   0.683912 |\n",
            "| batch:  33.000000 |\n",
            "| loss:   0.694648 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   0.691702 |\n",
            "| batch:  35.000000 |\n",
            "| loss:   0.855820 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   0.545734 |\n",
            "| batch:  37.000000 |\n",
            "| loss:   0.804452 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   0.587836 |\n",
            "| batch:  39.000000 |\n",
            "| loss:   1.004098 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   0.717917 |\n",
            "| batch:  41.000000 |\n",
            "| loss:   0.555878 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   0.576519 |\n",
            "| batch:  43.000000 |\n",
            "| loss:   0.666492 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   0.604527 |\n",
            "| batch:  45.000000 |\n",
            "| loss:   0.656582 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   0.735776 |\n",
            "| batch:  47.000000 |\n",
            "| loss:   0.674436 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   0.519096 |\n",
            "| batch:  49.000000 |\n",
            "| loss:   0.506581 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   0.706925 |\n",
            "| batch:  51.000000 |\n",
            "| loss:   0.615398 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   0.533993 |\n",
            "| batch:  53.000000 |\n",
            "| loss:   0.988962 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   0.611139 |\n",
            "| batch:  55.000000 |\n",
            "| loss:   0.567382 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   0.607265 |\n",
            "| batch:  57.000000 |\n",
            "| loss:   1.012764 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   0.684791 |\n",
            "| batch:  59.000000 |\n",
            "| loss:   0.584488 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   0.571015 |\n",
            "| batch:  61.000000 |\n",
            "| loss:   0.812708 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   0.597508 |\n",
            "| batch:  63.000000 |\n",
            "| loss:   0.654196 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   0.922301 |\n",
            "| batch:  65.000000 |\n",
            "| loss:   0.552770 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   0.585675 |\n",
            "| batch:  67.000000 |\n",
            "| loss:   0.764752 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   0.562153 |\n",
            "| batch:  69.000000 |\n",
            "| loss:   0.664237 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   0.600154 |\n",
            "| batch:  71.000000 |\n",
            "| loss:   0.897734 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   0.523088 |\n",
            "| batch:  73.000000 |\n",
            "| loss:   0.701140 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   0.717757 |\n",
            "| batch:  75.000000 |\n",
            "| loss:   0.618437 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   0.825388 |\n",
            "| batch:  77.000000 |\n",
            "| loss:   0.626899 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   0.670704 |\n",
            "| batch:  79.000000 |\n",
            "| loss:   0.636104 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   0.625716 |\n",
            "| batch:  81.000000 |\n",
            "| loss:   0.877998 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   0.607287 |\n",
            "| batch:  83.000000 |\n",
            "| loss:   0.609317 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   0.566137 |\n",
            "| batch:  85.000000 |\n",
            "| loss:   0.692522 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   0.699479 |\n",
            "| batch:  87.000000 |\n",
            "| loss:   0.581326 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   0.901036 |\n",
            "| batch:  89.000000 |\n",
            "| loss:   0.639190 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   0.746154 |\n",
            "| batch:  91.000000 |\n",
            "| loss:   0.676825 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   0.563353 |\n",
            "| batch:  93.000000 |\n",
            "| loss:   0.734594 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   0.706280 |\n",
            "| batch:  95.000000 |\n",
            "| loss:   0.793674 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   0.593707 |\n",
            "| batch:  97.000000 |\n",
            "| loss:   0.675163 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   0.552993 |\n",
            "| batch:  99.000000 |\n",
            "| loss:   0.461816 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   0.608178 |\n",
            "| batch: 101.000000 |\n",
            "| loss:   0.540557 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   0.704578 |\n",
            "| batch: 103.000000 |\n",
            "| loss:   0.587019 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   0.641939 |\n",
            "| batch: 105.000000 |\n",
            "| loss:   0.724938 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   0.697776 |\n",
            "| batch: 107.000000 |\n",
            "| loss:   0.647457 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   0.780242 |\n",
            "| batch: 109.000000 |\n",
            "| loss:   0.576902 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   1.142823 |\n",
            "| batch: 111.000000 |\n",
            "| loss:   0.634996 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   0.674451 |\n",
            "| batch: 113.000000 |\n",
            "| loss:   0.621386 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   0.645271 |\n",
            "| batch: 115.000000 |\n",
            "| loss:   0.824577 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   0.523751 |\n",
            "| batch: 117.000000 |\n",
            "| loss:   0.601519 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   0.596877 |\n",
            "| batch: 119.000000 |\n",
            "| loss:   0.690385 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   0.617049 |\n",
            "| batch: 121.000000 |\n",
            "| loss:   0.834763 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   0.790523 |\n",
            "| batch: 123.000000 |\n",
            "| loss:   0.647723 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   0.536144 |\n",
            "| batch: 125.000000 |\n",
            "| loss:   0.894742 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   0.508270 |\n",
            "| batch: 127.000000 |\n",
            "| loss:   0.622081 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   0.706576 |\n",
            "| batch: 129.000000 |\n",
            "| loss:   0.690247 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   0.583880 |\n",
            "| batch: 131.000000 |\n",
            "| loss:   0.560478 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   0.731881 |\n",
            "| batch: 133.000000 |\n",
            "| loss:   0.711305 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   0.536720 |\n",
            "| batch: 135.000000 |\n",
            "| loss:   0.689601 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   0.613658 |\n",
            "| batch: 137.000000 |\n",
            "| loss:   0.542941 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   0.860445 |\n",
            "| batch: 139.000000 |\n",
            "| loss:   0.684686 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   0.741915 |\n",
            "| batch: 141.000000 |\n",
            "| loss:   0.626357 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   0.640507 |\n",
            "| batch: 143.000000 |\n",
            "| loss:   0.620602 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   0.603084 |\n",
            "| batch: 145.000000 |\n",
            "| loss:   0.682703 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   0.579415 |\n",
            "| batch: 147.000000 |\n",
            "| loss:   0.858901 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   0.647587 |\n",
            "| batch: 149.000000 |\n",
            "| loss:   0.636937 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   0.472791 |\n",
            "| batch: 151.000000 |\n",
            "| loss:   0.617125 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   0.623713 |\n",
            "| batch: 153.000000 |\n",
            "| loss:   0.745459 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   0.684264 |\n",
            "| batch: 155.000000 |\n",
            "| loss:   0.601829 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   0.530628 |\n",
            "| batch: 157.000000 |\n",
            "| loss:   0.607975 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   0.893272 |\n",
            "| batch: 159.000000 |\n",
            "| loss:   0.768516 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   0.737288 |\n",
            "| batch: 161.000000 |\n",
            "| loss:   0.667050 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   0.591594 |\n",
            "| batch: 163.000000 |\n",
            "| loss:   0.816315 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   0.528775 |\n",
            "| batch: 165.000000 |\n",
            "| loss:   0.646766 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   0.612583 |\n",
            "| batch: 167.000000 |\n",
            "| loss:   0.566590 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   1.064134 |\n",
            "| batch: 169.000000 |\n",
            "| loss:   0.673313 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   0.714587 |\n",
            "| batch: 171.000000 |\n",
            "| loss:   0.577688 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   0.722451 |\n",
            "| batch: 173.000000 |\n",
            "| loss:   0.824155 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   0.841331 |\n",
            "| batch: 175.000000 |\n",
            "| loss:   0.486757 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   0.656175 |\n",
            "| batch: 177.000000 |\n",
            "| loss:   0.550019 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   0.682484 |\n",
            "| batch: 179.000000 |\n",
            "| loss:   0.798647 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   0.756919 |\n",
            "| batch: 181.000000 |\n",
            "| loss:   0.789689 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   0.625596 |\n",
            "| batch: 183.000000 |\n",
            "| loss:   0.662731 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   0.743793 |\n",
            "| batch: 185.000000 |\n",
            "| loss:   0.650097 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   0.603076 |\n",
            "| batch: 187.000000 |\n",
            "| loss:   0.831894 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   0.706318 |\n",
            "| batch: 189.000000 |\n",
            "| loss:   0.752817 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   0.561807 |\n",
            "| batch: 191.000000 |\n",
            "| loss:   0.635884 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   0.778008 |\n",
            "| batch: 193.000000 |\n",
            "| loss:   0.466724 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   0.693214 |\n",
            "| batch: 195.000000 |\n",
            "| loss:   0.603331 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   0.784594 |\n",
            "| batch: 197.000000 |\n",
            "| loss:   0.522771 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   0.960345 |\n",
            "| batch: 199.000000 |\n",
            "| loss:   1.026728 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   0.691813 |\n",
            "| batch: 201.000000 |\n",
            "| loss:   0.718678 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   0.593750 |\n",
            "| batch: 203.000000 |\n",
            "| loss:   0.625909 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   0.534573 |\n",
            "| batch: 205.000000 |\n",
            "| loss:   0.763001 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   0.912119 |\n",
            "| batch: 207.000000 |\n",
            "| loss:   0.863631 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   0.919270 |\n",
            "| batch: 209.000000 |\n",
            "| loss:   1.018766 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   0.962505 |\n",
            "| batch: 211.000000 |\n",
            "| loss:   0.608318 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   0.812738 |\n",
            "| batch: 213.000000 |\n",
            "| loss:   0.680828 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   0.765880 |\n",
            "| batch: 215.000000 |\n",
            "| loss:   0.793185 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   0.557704 |\n",
            "| batch: 217.000000 |\n",
            "| loss:   0.847399 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   0.632846 |\n",
            "| batch: 219.000000 |\n",
            "| loss:   0.597803 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   0.694152 |\n",
            "| batch: 221.000000 |\n",
            "| loss:   0.647412 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   0.684920 |\n",
            "| batch: 223.000000 |\n",
            "| loss:   0.712841 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   0.636093 |\n",
            "| batch: 225.000000 |\n",
            "| loss:   0.710258 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   0.861124 |\n",
            "| batch: 227.000000 |\n",
            "| loss:   0.762162 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   0.678063 |\n",
            "| batch: 229.000000 |\n",
            "| loss:   0.559398 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   0.686876 |\n",
            "| batch: 231.000000 |\n",
            "| loss:   0.702354 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   0.648489 |\n",
            "| batch: 233.000000 |\n",
            "| loss:   0.746840 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   0.709782 |\n",
            "| batch: 235.000000 |\n",
            "| loss:   0.578484 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   0.768559 |\n",
            "| batch: 237.000000 |\n",
            "| loss:   0.829541 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   0.602675 |\n",
            "| batch: 239.000000 |\n",
            "| loss:   0.743800 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   0.778595 |\n",
            "| batch: 241.000000 |\n",
            "| loss:   0.615335 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   0.608261 |\n",
            "| batch: 243.000000 |\n",
            "| loss:   0.540181 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   0.736618 |\n",
            "| batch: 245.000000 |\n",
            "| loss:   0.649709 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   0.815401 |\n",
            "| batch: 247.000000 |\n",
            "| loss:   0.625104 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   0.569539 |\n",
            "| batch: 249.000000 |\n",
            "| loss:   0.760574 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   0.583559 |\n",
            "| batch: 251.000000 |\n",
            "| loss:   0.873885 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   0.596802 |\n",
            "| batch: 253.000000 |\n",
            "| loss:   0.557590 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   0.835049 |\n",
            "| batch: 255.000000 |\n",
            "| loss:   0.735955 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   0.600215 |\n",
            "| batch: 257.000000 |\n",
            "| loss:   0.716439 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   0.995874 |\n",
            "| batch: 259.000000 |\n",
            "| loss:   1.036358 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   0.679333 |\n",
            "| batch: 261.000000 |\n",
            "| loss:   0.691138 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   0.669618 |\n",
            "| batch: 263.000000 |\n",
            "| loss:   0.679104 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   0.655787 |\n",
            "| batch: 265.000000 |\n",
            "| loss:   0.651796 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   0.732354 |\n",
            "| batch: 267.000000 |\n",
            "| loss:   0.609568 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   0.802949 |\n",
            "| batch: 269.000000 |\n",
            "| loss:   0.695807 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   0.666511 |\n",
            "| batch: 271.000000 |\n",
            "| loss:   0.618000 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   0.645389 |\n",
            "| batch: 273.000000 |\n",
            "| loss:   0.614545 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   0.908015 |\n",
            "| batch: 275.000000 |\n",
            "| loss:   0.887911 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   0.791179 |\n",
            "| batch: 277.000000 |\n",
            "| loss:   0.613772 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   0.636847 |\n",
            "| batch: 279.000000 |\n",
            "| loss:   0.705005 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   0.723102 |\n",
            "| batch: 281.000000 |\n",
            "| loss:   1.105190 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   0.712323 |\n",
            "| batch: 283.000000 |\n",
            "| loss:   0.710334 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   1.020663 |\n",
            "| batch: 285.000000 |\n",
            "| loss:   0.622965 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   0.636640 |\n",
            "| batch: 287.000000 |\n",
            "| loss:   0.577311 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   0.697718 |\n",
            "| batch: 289.000000 |\n",
            "| loss:   0.750727 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   0.617336 |\n",
            "| batch: 291.000000 |\n",
            "| loss:   0.576480 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   0.721009 |\n",
            "| batch: 293.000000 |\n",
            "| loss:   0.699838 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   0.684480 |\n",
            "| batch: 295.000000 |\n",
            "| loss:   0.675587 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   0.900951 |\n",
            "| batch: 297.000000 |\n",
            "| loss:   0.714626 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   0.768290 |\n",
            "| batch: 299.000000 |\n",
            "| loss:   0.634063 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   0.703381 |\n",
            "| batch: 301.000000 |\n",
            "| loss:   0.666159 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   0.760797 |\n",
            "| batch: 303.000000 |\n",
            "| loss:   0.550990 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   0.636723 |\n",
            "| batch: 305.000000 |\n",
            "| loss:   0.572643 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   0.999436 |\n",
            "| batch: 307.000000 |\n",
            "| loss:   0.876492 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   0.554172 |\n",
            "| batch: 309.000000 |\n",
            "| loss:   0.822245 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   0.765861 |\n",
            "| batch: 311.000000 |\n",
            "| loss:   0.896588 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   0.804154 |\n",
            "| batch: 313.000000 |\n",
            "| loss:   0.966080 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   0.650863 |\n",
            "| batch: 315.000000 |\n",
            "| loss:   0.646520 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   0.803321 |\n",
            "| batch: 317.000000 |\n",
            "| loss:   0.743687 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   0.606422 |\n",
            "| batch: 319.000000 |\n",
            "| loss:   0.640590 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   0.533804 |\n",
            "| batch: 321.000000 |\n",
            "| loss:   0.976389 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   0.681780 |\n",
            "| batch: 323.000000 |\n",
            "| loss:   0.625456 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   0.742353 |\n",
            "| batch: 325.000000 |\n",
            "| loss:   0.682288 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   0.654769 |\n",
            "| batch: 327.000000 |\n",
            "| loss:   0.893364 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   0.770298 |\n",
            "| batch: 329.000000 |\n",
            "| loss:   0.877297 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   0.709880 |\n",
            "| batch: 331.000000 |\n",
            "| loss:   1.183637 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   0.695281 |\n",
            "| batch: 333.000000 |\n",
            "| loss:   0.571940 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   0.840199 |\n",
            "| batch: 335.000000 |\n",
            "| loss:   0.564911 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   0.908710 |\n",
            "| batch: 337.000000 |\n",
            "| loss:   0.703090 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   1.035498 |\n",
            "| batch: 339.000000 |\n",
            "| loss:   0.629219 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   0.724823 |\n",
            "| batch: 341.000000 |\n",
            "| loss:   0.731249 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   0.863963 |\n",
            "| batch: 343.000000 |\n",
            "| loss:   0.812898 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   0.548070 |\n",
            "| batch: 345.000000 |\n",
            "| loss:   0.705033 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   0.735570 |\n",
            "| batch: 347.000000 |\n",
            "| loss:   0.665713 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   0.684176 |\n",
            "| batch: 349.000000 |\n",
            "| loss:   0.766404 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   0.589085 |\n",
            "| batch: 351.000000 |\n",
            "| loss:   1.038426 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   0.739812 |\n",
            "| batch: 353.000000 |\n",
            "| loss:   0.718176 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   0.660994 |\n",
            "| batch: 355.000000 |\n",
            "| loss:   0.754890 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   0.628015 |\n",
            "| batch: 357.000000 |\n",
            "| loss:   0.688267 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   0.635904 |\n",
            "| batch: 359.000000 |\n",
            "| loss:   0.648222 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   0.699906 |\n",
            "| batch: 361.000000 |\n",
            "| loss:   0.810463 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   0.748179 |\n",
            "| batch: 363.000000 |\n",
            "| loss:   0.775227 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   1.011803 |\n",
            "| batch: 365.000000 |\n",
            "| loss:   0.791379 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   0.987711 |\n",
            "| batch: 367.000000 |\n",
            "| loss:   0.629533 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   0.729622 |\n",
            "| batch: 369.000000 |\n",
            "| loss:   0.870830 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   0.522494 |\n",
            "| batch: 371.000000 |\n",
            "| loss:   0.693445 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   0.697354 |\n",
            "| batch: 373.000000 |\n",
            "| loss:   0.623966 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   0.668324 |\n",
            "| batch: 375.000000 |\n",
            "| loss:   0.807189 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   0.915706 |\n",
            "| batch: 377.000000 |\n",
            "| loss:   0.796561 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   0.779476 |\n",
            "| batch: 379.000000 |\n",
            "| loss:   0.614912 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   0.838968 |\n",
            "| batch: 381.000000 |\n",
            "| loss:   0.708821 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   0.581501 |\n",
            "| batch: 383.000000 |\n",
            "| loss:   0.653965 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   0.818897 |\n",
            "| batch: 385.000000 |\n",
            "| loss:   0.728066 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   0.907452 |\n",
            "| batch: 387.000000 |\n",
            "| loss:   0.612657 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   0.828348 |\n",
            "| batch: 389.000000 |\n",
            "| loss:   0.741477 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   0.776236 |\n",
            "| batch: 391.000000 |\n",
            "| loss:   0.818977 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   0.630748 |\n",
            "| batch: 393.000000 |\n",
            "| loss:   0.714000 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   0.806042 |\n",
            "| batch: 395.000000 |\n",
            "| loss:   0.791417 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   0.686486 |\n",
            "| batch: 397.000000 |\n",
            "| loss:   0.608595 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   1.277798 |\n",
            "| batch: 399.000000 |\n",
            "| loss:   0.660518 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   0.680447 |\n",
            "| batch: 401.000000 |\n",
            "| loss:   0.697980 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   0.714502 |\n",
            "| batch: 403.000000 |\n",
            "| loss:   0.562783 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   0.661829 |\n",
            "| batch: 405.000000 |\n",
            "| loss:   0.619934 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   0.723727 |\n",
            "| batch: 407.000000 |\n",
            "| loss:   0.700063 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   0.903782 |\n",
            "| batch: 409.000000 |\n",
            "| loss:   0.886270 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   0.784963 |\n",
            "| batch: 411.000000 |\n",
            "| loss:   0.896173 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   1.118199 |\n",
            "| batch: 413.000000 |\n",
            "| loss:   0.676618 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   1.942608 |\n",
            "| batch: 415.000000 |\n",
            "| loss:   0.746179 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   0.733772 |\n",
            "| batch: 417.000000 |\n",
            "| loss:   0.651629 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   0.652050 |\n",
            "| batch: 419.000000 |\n",
            "| loss:   0.654320 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   0.637918 |\n",
            "| batch: 421.000000 |\n",
            "| loss:   0.567827 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   0.753968 |\n",
            "| batch: 423.000000 |\n",
            "| loss:   0.699141 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   0.847571 |\n",
            "| batch: 425.000000 |\n",
            "| loss:   0.891337 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   1.060419 |\n",
            "| batch: 427.000000 |\n",
            "| loss:   0.682471 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   0.656794 |\n",
            "| batch: 429.000000 |\n",
            "| loss:   0.721758 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   0.730541 |\n",
            "| batch: 431.000000 |\n",
            "| loss:   0.686153 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   0.665641 |\n",
            "| batch: 433.000000 |\n",
            "| loss:   0.793708 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   0.878226 |\n",
            "| batch: 435.000000 |\n",
            "| loss:   0.601307 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   0.588487 |\n",
            "| batch: 437.000000 |\n",
            "| loss:   0.678185 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   0.732674 |\n",
            "| batch: 439.000000 |\n",
            "| loss:   0.783695 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   0.582151 |\n",
            "| batch: 441.000000 |\n",
            "| loss:   0.818708 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   0.662999 |\n",
            "| batch: 443.000000 |\n",
            "| loss:   0.594185 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   0.659500 |\n",
            "| batch: 445.000000 |\n",
            "| loss:   0.640706 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   0.776741 |\n",
            "| batch: 447.000000 |\n",
            "| loss:   0.803878 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   1.116239 |\n",
            "| batch: 449.000000 |\n",
            "| loss:   0.597212 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   0.671122 |\n",
            "| batch: 451.000000 |\n",
            "| loss:   0.756186 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   0.750245 |\n",
            "| batch: 453.000000 |\n",
            "| loss:   0.622864 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   0.645155 |\n",
            "| batch: 455.000000 |\n",
            "| loss:   0.528823 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   0.860276 |\n",
            "| batch: 457.000000 |\n",
            "| loss:   0.847580 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   0.910422 |\n",
            "| batch: 459.000000 |\n",
            "| loss:   1.209695 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   0.623226 |\n",
            "| batch: 461.000000 |\n",
            "| loss:   0.688318 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   0.709794 |\n",
            "| batch: 463.000000 |\n",
            "| loss:   0.691433 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   0.726066 |\n",
            "| batch: 465.000000 |\n",
            "| loss:   0.710573 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   0.639191 |\n",
            "| batch: 467.000000 |\n",
            "| loss:   0.902622 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   0.886369 |\n",
            "| batch: 469.000000 |\n",
            "| loss:   0.805601 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   0.923023 |\n",
            "| batch: 471.000000 |\n",
            "| loss:   1.075479 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   0.626719 |\n",
            "| batch: 473.000000 |\n",
            "| loss:   0.588279 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   0.635413 |\n",
            "| batch: 475.000000 |\n",
            "| loss:   0.918998 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   0.723910 |\n",
            "| batch: 477.000000 |\n",
            "| loss:   0.588137 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   0.875127 |\n",
            "| batch: 479.000000 |\n",
            "| loss:   0.590962 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   1.008693 |\n",
            "| batch: 481.000000 |\n",
            "| loss:   1.048435 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   0.667453 |\n",
            "| batch: 483.000000 |\n",
            "| loss:   0.561923 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   0.809630 |\n",
            "| batch: 485.000000 |\n",
            "| loss:   0.866464 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   1.088001 |\n",
            "| batch: 487.000000 |\n",
            "| loss:   0.770292 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   0.757951 |\n",
            "| batch: 489.000000 |\n",
            "| loss:   1.000589 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   0.768676 |\n",
            "| batch: 491.000000 |\n",
            "| loss:   0.723319 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   0.615251 |\n",
            "| batch: 493.000000 |\n",
            "| loss:   0.797071 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   0.864516 |\n",
            "| batch: 495.000000 |\n",
            "| loss:   0.597980 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   0.830325 |\n",
            "| batch: 497.000000 |\n",
            "| loss:   0.674224 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   0.648558 |\n",
            "| batch: 499.000000 |\n",
            "| loss:   0.586662 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   0.623921 |\n",
            "| batch: 501.000000 |\n",
            "| loss:   0.631776 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   0.681301 |\n",
            "| batch: 503.000000 |\n",
            "| loss:   0.628787 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   1.170421 |\n",
            "| batch: 505.000000 |\n",
            "| loss:   0.764334 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   0.776357 |\n",
            "| batch: 507.000000 |\n",
            "| loss:   0.661095 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   0.860834 |\n",
            "| batch: 509.000000 |\n",
            "| loss:   0.822781 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   0.748072 |\n",
            "| batch: 511.000000 |\n",
            "| loss:   0.684927 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   0.865068 |\n",
            "| batch: 513.000000 |\n",
            "| loss:   1.037715 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   0.622541 |\n",
            "| batch: 515.000000 |\n",
            "| loss:   0.742456 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   0.671612 |\n",
            "| batch: 517.000000 |\n",
            "| loss:   0.744908 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   0.858207 |\n",
            "| batch: 519.000000 |\n",
            "| loss:   0.766674 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   0.903871 |\n",
            "| batch: 521.000000 |\n",
            "| loss:   1.001251 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   0.812690 |\n",
            "| batch: 523.000000 |\n",
            "| loss:   0.698410 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   0.644186 |\n",
            "| batch: 525.000000 |\n",
            "| loss:   0.717541 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   0.761793 |\n",
            "| batch: 527.000000 |\n",
            "| loss:   0.670353 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   1.087727 |\n",
            "| batch: 529.000000 |\n",
            "| loss:   0.733349 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   1.101832 |\n",
            "| batch: 531.000000 |\n",
            "| loss:   0.835975 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   0.846900 |\n",
            "| batch: 533.000000 |\n",
            "| loss:   1.134756 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   0.713464 |\n",
            "| batch: 535.000000 |\n",
            "| loss:   0.697594 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   0.735817 |\n",
            "| batch: 537.000000 |\n",
            "| loss:   0.985603 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   0.993868 |\n",
            "| batch: 539.000000 |\n",
            "| loss:   0.682224 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   0.728498 |\n",
            "| batch: 541.000000 |\n",
            "| loss:   0.640720 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   0.730557 |\n",
            "| batch: 543.000000 |\n",
            "| loss:   0.848534 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   0.695152 |\n",
            "| batch: 545.000000 |\n",
            "| loss:   0.782445 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   0.718612 |\n",
            "| batch: 547.000000 |\n",
            "| loss:   0.758693 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   0.693228 |\n",
            "| batch: 549.000000 |\n",
            "| loss:   0.670176 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   0.949890 |\n",
            "| batch: 551.000000 |\n",
            "| loss:   0.776544 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   0.708862 |\n",
            "| batch: 553.000000 |\n",
            "| loss:   0.704570 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   0.799508 |\n",
            "| batch: 555.000000 |\n",
            "| loss:   0.749072 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   0.817177 |\n",
            "| batch: 557.000000 |\n",
            "| loss:   0.785064 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   0.830842 |\n",
            "| batch: 559.000000 |\n",
            "| loss:   0.795488 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   0.719920 |\n",
            "| batch: 561.000000 |\n",
            "| loss:   0.763483 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   1.132794 |\n",
            "| batch: 563.000000 |\n",
            "| loss:   0.631514 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   0.814886 |\n",
            "| batch: 565.000000 |\n",
            "| loss:   0.669700 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   0.685438 |\n",
            "| batch: 567.000000 |\n",
            "| loss:   0.712244 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   0.837032 |\n",
            "| batch: 569.000000 |\n",
            "| loss:   0.747031 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   0.744178 |\n",
            "| batch: 571.000000 |\n",
            "| loss:   0.772424 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   0.638601 |\n",
            "| batch: 573.000000 |\n",
            "| loss:   0.642358 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   0.810173 |\n",
            "| batch: 575.000000 |\n",
            "| loss:   0.770344 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   0.849740 |\n",
            "| batch: 577.000000 |\n",
            "| loss:   1.207812 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   0.614670 |\n",
            "| batch: 579.000000 |\n",
            "| loss:   0.658927 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   0.968658 |\n",
            "| batch: 581.000000 |\n",
            "| loss:   0.952257 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   0.730660 |\n",
            "| batch: 583.000000 |\n",
            "| loss:   0.713600 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   0.850652 |\n",
            "| batch: 585.000000 |\n",
            "| loss:   0.900520 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   1.605207 |\n",
            "| batch: 587.000000 |\n",
            "| loss:   0.835321 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   0.961967 |\n",
            "| batch: 589.000000 |\n",
            "| loss:   0.645721 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   0.699902 |\n",
            "| batch: 591.000000 |\n",
            "| loss:   1.003280 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   0.665895 |\n",
            "| batch: 593.000000 |\n",
            "| loss:   0.932198 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   0.945250 |\n",
            "| batch: 595.000000 |\n",
            "| loss:   0.774412 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   0.693677 |\n",
            "| batch: 597.000000 |\n",
            "| loss:   0.775242 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   0.770478 |\n",
            "| batch: 599.000000 |\n",
            "| loss:   0.964563 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   0.725269 |\n",
            "| batch: 601.000000 |\n",
            "| loss:   0.617129 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   0.818145 |\n",
            "| batch: 603.000000 |\n",
            "| loss:   0.787560 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   1.026629 |\n",
            "| batch: 605.000000 |\n",
            "| loss:   0.816047 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   0.906957 |\n",
            "| batch: 607.000000 |\n",
            "| loss:   0.899218 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   0.695606 |\n",
            "| batch: 609.000000 |\n",
            "| loss:   0.693057 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   0.760566 |\n",
            "| batch: 611.000000 |\n",
            "| loss:   0.537891 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   0.592102 |\n",
            "| batch: 613.000000 |\n",
            "| loss:   0.629244 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   0.702821 |\n",
            "| batch: 615.000000 |\n",
            "| loss:   0.655821 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   0.851501 |\n",
            "| batch: 617.000000 |\n",
            "| loss:   0.786710 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   0.694951 |\n",
            "| batch: 619.000000 |\n",
            "| loss:   0.757910 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   0.657105 |\n",
            "| batch: 621.000000 |\n",
            "| loss:   0.733540 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   1.085141 |\n",
            "| batch: 623.000000 |\n",
            "| loss:   0.927770 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   0.761648 |\n",
            "| batch: 625.000000 |\n",
            "| loss:   0.649953 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   0.961146 |\n",
            "| batch: 627.000000 |\n",
            "| loss:   0.745225 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   0.890713 |\n",
            "| batch: 629.000000 |\n",
            "| loss:   1.006521 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   0.689429 |\n",
            "| batch: 631.000000 |\n",
            "| loss:   0.599754 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   0.824019 |\n",
            "| batch: 633.000000 |\n",
            "| loss:   0.653477 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   1.083600 |\n",
            "| batch: 635.000000 |\n",
            "| loss:   0.749227 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   0.798133 |\n",
            "| batch: 637.000000 |\n",
            "| loss:   0.962013 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   0.694796 |\n",
            "| batch: 639.000000 |\n",
            "| loss:   0.957931 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   0.611753 |\n",
            "| batch: 641.000000 |\n",
            "| loss:   0.845380 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   0.748027 |\n",
            "| batch: 643.000000 |\n",
            "| loss:   0.711941 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   0.747040 |\n",
            "| batch: 645.000000 |\n",
            "| loss:   0.807164 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   0.706961 |\n",
            "| batch: 647.000000 |\n",
            "| loss:   0.717223 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   0.883849 |\n",
            "| batch: 649.000000 |\n",
            "| loss:   0.746905 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   0.771352 |\n",
            "| batch: 651.000000 |\n",
            "| loss:   0.835061 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   0.664697 |\n",
            "| batch: 653.000000 |\n",
            "| loss:   0.655921 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   0.995186 |\n",
            "| batch: 655.000000 |\n",
            "| loss:   0.758715 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   0.658866 |\n",
            "| batch: 657.000000 |\n",
            "| loss:   0.811650 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   0.846614 |\n",
            "| batch: 659.000000 |\n",
            "| loss:   0.798899 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   0.776595 |\n",
            "| batch: 661.000000 |\n",
            "| loss:   1.037137 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   0.709891 |\n",
            "| batch: 663.000000 |\n",
            "| loss:   0.742612 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   0.659423 |\n",
            "| batch: 665.000000 |\n",
            "| loss:   0.971563 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   0.690910 |\n",
            "| batch: 667.000000 |\n",
            "| loss:   0.849865 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   1.085483 |\n",
            "| batch: 669.000000 |\n",
            "| loss:   0.952775 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   0.764679 |\n",
            "| batch: 671.000000 |\n",
            "| loss:   0.987875 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   0.809760 |\n",
            "| batch: 673.000000 |\n",
            "| loss:   0.776885 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   0.704348 |\n",
            "| batch: 675.000000 |\n",
            "| loss:   0.644349 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   0.750919 |\n",
            "| batch: 677.000000 |\n",
            "| loss:   0.662797 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   0.706194 |\n",
            "| batch: 679.000000 |\n",
            "| loss:   0.808312 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   0.782797 |\n",
            "| batch: 681.000000 |\n",
            "| loss:   0.664385 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   0.768068 |\n",
            "| batch: 683.000000 |\n",
            "| loss:   0.928797 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   0.723370 |\n",
            "| batch: 685.000000 |\n",
            "| loss:   0.721399 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   0.614657 |\n",
            "| batch: 687.000000 |\n",
            "| loss:   0.797873 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   0.780764 |\n",
            "| batch: 689.000000 |\n",
            "| loss:   0.613665 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   0.846456 |\n",
            "| batch: 691.000000 |\n",
            "| loss:   0.860480 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   0.801631 |\n",
            "| batch: 693.000000 |\n",
            "| loss:   0.810576 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   0.649470 |\n",
            "| batch: 695.000000 |\n",
            "| loss:   1.093466 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   0.722841 |\n",
            "| batch: 697.000000 |\n",
            "| loss:   1.581005 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   0.972350 |\n",
            "| batch: 699.000000 |\n",
            "| loss:   1.296124 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   1.018460 |\n",
            "| batch: 701.000000 |\n",
            "| loss:   0.712608 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   0.821786 |\n",
            "| batch: 703.000000 |\n",
            "| loss:   0.941421 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   0.831552 |\n",
            "| batch: 705.000000 |\n",
            "| loss:   0.786904 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   0.872860 |\n",
            "| batch: 707.000000 |\n",
            "| loss:   0.728921 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   0.612533 |\n",
            "| batch: 709.000000 |\n",
            "| loss:   0.939390 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   1.003883 |\n",
            "| batch: 711.000000 |\n",
            "| loss:   0.875609 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   0.917171 |\n",
            "| batch: 713.000000 |\n",
            "| loss:   0.743009 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   0.723984 |\n",
            "| batch: 715.000000 |\n",
            "| loss:   0.747816 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   1.064418 |\n",
            "| batch: 717.000000 |\n",
            "| loss:   0.627666 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   0.797769 |\n",
            "| batch: 719.000000 |\n",
            "| loss:   0.656238 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   0.794468 |\n",
            "| batch: 721.000000 |\n",
            "| loss:   0.821484 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   0.967365 |\n",
            "| batch: 723.000000 |\n",
            "| loss:   0.726550 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   0.849204 |\n",
            "| batch: 725.000000 |\n",
            "| loss:   0.743187 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   1.043754 |\n",
            "| batch: 727.000000 |\n",
            "| loss:   1.034110 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   0.756727 |\n",
            "| batch: 729.000000 |\n",
            "| loss:   0.824959 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   0.712189 |\n",
            "| batch: 731.000000 |\n",
            "| loss:   0.629080 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   0.710261 |\n",
            "| batch: 733.000000 |\n",
            "| loss:   0.698106 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   0.741527 |\n",
            "| batch: 735.000000 |\n",
            "| loss:   0.894038 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   0.705174 |\n",
            "| batch: 737.000000 |\n",
            "| loss:   0.884336 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   0.817182 |\n",
            "| batch: 739.000000 |\n",
            "| loss:   0.724170 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   0.823908 |\n",
            "| batch: 741.000000 |\n",
            "| loss:   0.841875 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   0.813307 |\n",
            "| batch: 743.000000 |\n",
            "| loss:   0.870170 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   0.707876 |\n",
            "| batch: 745.000000 |\n",
            "| loss:   0.702399 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   0.858980 |\n",
            "| batch: 747.000000 |\n",
            "| loss:   0.763907 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   0.870295 |\n",
            "| batch: 749.000000 |\n",
            "| loss:   0.766471 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   0.618226 |\n",
            "| batch: 751.000000 |\n",
            "| loss:   0.858386 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   0.669909 |\n",
            "| batch: 753.000000 |\n",
            "| loss:   0.737029 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   0.855263 |\n",
            "| batch: 755.000000 |\n",
            "| loss:   0.779806 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   0.724857 |\n",
            "| batch: 757.000000 |\n",
            "| loss:   0.896465 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   0.851104 |\n",
            "| batch: 759.000000 |\n",
            "| loss:   0.766551 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   0.682992 |\n",
            "| batch: 761.000000 |\n",
            "| loss:   0.686683 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   0.707254 |\n",
            "| batch: 763.000000 |\n",
            "| loss:   0.692694 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   0.790074 |\n",
            "| batch: 765.000000 |\n",
            "| loss:   0.639211 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   0.863737 |\n",
            "| batch: 767.000000 |\n",
            "| loss:   0.880981 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   0.822885 |\n",
            "| batch: 769.000000 |\n",
            "| loss:   0.794661 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   0.917940 |\n",
            "| batch: 771.000000 |\n",
            "| loss:   0.639670 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   0.927429 |\n",
            "| batch: 773.000000 |\n",
            "| loss:   0.712233 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   0.684768 |\n",
            "| batch: 775.000000 |\n",
            "| loss:   0.842756 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   1.000833 |\n",
            "| batch: 777.000000 |\n",
            "| loss:   0.662208 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   0.763055 |\n",
            "| batch: 779.000000 |\n",
            "| loss:   0.990922 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   0.961198 |\n",
            "| batch: 781.000000 |\n",
            "| loss:   0.852796 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   0.789269 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   1.491184 |\n",
            "| val_batch:   3.000000 |\n",
            "| val_loss:   1.770612 |\n",
            "| val_batch:   6.000000 |\n",
            "| val_loss:   1.872771 |\n",
            "| val_batch:   9.000000 |\n",
            "| val_loss:   1.749582 |\n",
            "| val_batch:  12.000000 |\n",
            "| val_loss:   1.603155 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   1.223585 |\n",
            "| val_batch:  18.000000 |\n",
            "| val_loss:   1.533374 |\n",
            "| val_batch:  21.000000 |\n",
            "| val_loss:   1.536421 |\n",
            "| val_batch:  24.000000 |\n",
            "| val_loss:   1.490337 |\n",
            "| val_batch:  27.000000 |\n",
            "| val_loss:   1.938821 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   1.684416 |\n",
            "| val_batch:  33.000000 |\n",
            "| val_loss:   1.808968 |\n",
            "| val_batch:  36.000000 |\n",
            "| val_loss:   1.426418 |\n",
            "| val_batch:  39.000000 |\n",
            "| val_loss:   1.376163 |\n",
            "| val_batch:  42.000000 |\n",
            "| val_loss:   1.422509 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   1.509588 |\n",
            "| val_batch:  48.000000 |\n",
            "| val_loss:   1.388025 |\n",
            "| val_batch:  51.000000 |\n",
            "| val_loss:   1.636637 |\n",
            "| val_batch:  54.000000 |\n",
            "| val_loss:   1.449914 |\n",
            "| val_batch:  57.000000 |\n",
            "| val_loss:   1.385251 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   1.544744 |\n",
            "| val_batch:  63.000000 |\n",
            "| val_loss:   1.463431 |\n",
            "| val_batch:  66.000000 |\n",
            "| val_loss:   1.472172 |\n",
            "| val_batch:  69.000000 |\n",
            "| val_loss:   1.374092 |\n",
            "| val_batch:  72.000000 |\n",
            "| val_loss:   1.914444 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   1.281359 |\n",
            "| val_batch:  78.000000 |\n",
            "| val_loss:   1.368100 |\n",
            "| val_batch:  81.000000 |\n",
            "| val_loss:   2.056947 |\n",
            "| val_batch:  84.000000 |\n",
            "| val_loss:   1.400808 |\n",
            "Input: That's true, too.\n",
            "Output: \" , , , , roger <UNK> , , , demasiado , , , es demasiado , \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . \" . vainilla . \" . \" . \" . \" . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            "Epoch: 06 | Time: 16m 48s\n",
            "\tTrain Loss: 0.745 | Train PPL:   2.107\n",
            "\t Val. Loss: 1.568 |  Val. PPL:   4.797\n",
            "| epoch:   6.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   0.675858 |\n",
            "| batch:   1.000000 |\n",
            "| loss:   0.605777 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   0.608737 |\n",
            "| batch:   3.000000 |\n",
            "| loss:   0.607809 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   0.639974 |\n",
            "| batch:   5.000000 |\n",
            "| loss:   0.697379 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   0.541660 |\n",
            "| batch:   7.000000 |\n",
            "| loss:   0.610836 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   0.523964 |\n",
            "| batch:   9.000000 |\n",
            "| loss:   0.635118 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   0.531713 |\n",
            "| batch:  11.000000 |\n",
            "| loss:   0.581780 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   0.548215 |\n",
            "| batch:  13.000000 |\n",
            "| loss:   0.444380 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   0.465641 |\n",
            "| batch:  15.000000 |\n",
            "| loss:   0.535230 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   0.496281 |\n",
            "| batch:  17.000000 |\n",
            "| loss:   0.648012 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   0.514096 |\n",
            "| batch:  19.000000 |\n",
            "| loss:   0.542915 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   0.513120 |\n",
            "| batch:  21.000000 |\n",
            "| loss:   0.596576 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   0.720820 |\n",
            "| batch:  23.000000 |\n",
            "| loss:   0.559739 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   0.574541 |\n",
            "| batch:  25.000000 |\n",
            "| loss:   0.620152 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   0.574734 |\n",
            "| batch:  27.000000 |\n",
            "| loss:   0.495384 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   0.726259 |\n",
            "| batch:  29.000000 |\n",
            "| loss:   0.467034 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   0.542890 |\n",
            "| batch:  31.000000 |\n",
            "| loss:   0.623123 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   0.570902 |\n",
            "| batch:  33.000000 |\n",
            "| loss:   0.527383 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   0.627569 |\n",
            "| batch:  35.000000 |\n",
            "| loss:   0.469096 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   0.422033 |\n",
            "| batch:  37.000000 |\n",
            "| loss:   0.590742 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   0.495330 |\n",
            "| batch:  39.000000 |\n",
            "| loss:   0.662048 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   0.777712 |\n",
            "| batch:  41.000000 |\n",
            "| loss:   0.518486 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   0.971066 |\n",
            "| batch:  43.000000 |\n",
            "| loss:   0.790555 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   0.616451 |\n",
            "| batch:  45.000000 |\n",
            "| loss:   0.599633 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   0.557650 |\n",
            "| batch:  47.000000 |\n",
            "| loss:   0.499147 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   0.590713 |\n",
            "| batch:  49.000000 |\n",
            "| loss:   0.640590 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   0.624190 |\n",
            "| batch:  51.000000 |\n",
            "| loss:   0.641357 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   0.431285 |\n",
            "| batch:  53.000000 |\n",
            "| loss:   0.511209 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   0.525448 |\n",
            "| batch:  55.000000 |\n",
            "| loss:   0.580068 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   0.473784 |\n",
            "| batch:  57.000000 |\n",
            "| loss:   0.557627 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   0.631381 |\n",
            "| batch:  59.000000 |\n",
            "| loss:   0.504064 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   0.508471 |\n",
            "| batch:  61.000000 |\n",
            "| loss:   0.594240 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   0.553345 |\n",
            "| batch:  63.000000 |\n",
            "| loss:   0.555434 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   0.568109 |\n",
            "| batch:  65.000000 |\n",
            "| loss:   0.615628 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   0.419588 |\n",
            "| batch:  67.000000 |\n",
            "| loss:   0.463736 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   0.461389 |\n",
            "| batch:  69.000000 |\n",
            "| loss:   0.502226 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   0.565914 |\n",
            "| batch:  71.000000 |\n",
            "| loss:   0.498151 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   0.600273 |\n",
            "| batch:  73.000000 |\n",
            "| loss:   0.543787 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   0.624403 |\n",
            "| batch:  75.000000 |\n",
            "| loss:   1.156133 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   0.574683 |\n",
            "| batch:  77.000000 |\n",
            "| loss:   0.486289 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   0.426024 |\n",
            "| batch:  79.000000 |\n",
            "| loss:   0.658349 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   0.561510 |\n",
            "| batch:  81.000000 |\n",
            "| loss:   0.559164 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   0.696052 |\n",
            "| batch:  83.000000 |\n",
            "| loss:   0.502037 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   0.533729 |\n",
            "| batch:  85.000000 |\n",
            "| loss:   0.569539 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   0.634348 |\n",
            "| batch:  87.000000 |\n",
            "| loss:   0.615616 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   0.549788 |\n",
            "| batch:  89.000000 |\n",
            "| loss:   0.526780 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   0.485769 |\n",
            "| batch:  91.000000 |\n",
            "| loss:   0.740172 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   0.576182 |\n",
            "| batch:  93.000000 |\n",
            "| loss:   0.739966 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   0.542252 |\n",
            "| batch:  95.000000 |\n",
            "| loss:   0.598735 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   0.692050 |\n",
            "| batch:  97.000000 |\n",
            "| loss:   0.561397 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   0.612144 |\n",
            "| batch:  99.000000 |\n",
            "| loss:   0.607903 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   0.583977 |\n",
            "| batch: 101.000000 |\n",
            "| loss:   0.673241 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   0.817943 |\n",
            "| batch: 103.000000 |\n",
            "| loss:   0.598107 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   0.406237 |\n",
            "| batch: 105.000000 |\n",
            "| loss:   0.581659 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   0.646842 |\n",
            "| batch: 107.000000 |\n",
            "| loss:   0.794237 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   0.690934 |\n",
            "| batch: 109.000000 |\n",
            "| loss:   0.432255 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   0.566832 |\n",
            "| batch: 111.000000 |\n",
            "| loss:   0.868511 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   0.651894 |\n",
            "| batch: 113.000000 |\n",
            "| loss:   0.592980 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   0.558037 |\n",
            "| batch: 115.000000 |\n",
            "| loss:   0.729180 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   0.441377 |\n",
            "| batch: 117.000000 |\n",
            "| loss:   0.603816 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   0.490150 |\n",
            "| batch: 119.000000 |\n",
            "| loss:   0.598937 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   0.838382 |\n",
            "| batch: 121.000000 |\n",
            "| loss:   0.496105 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   0.568478 |\n",
            "| batch: 123.000000 |\n",
            "| loss:   0.753718 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   0.449676 |\n",
            "| batch: 125.000000 |\n",
            "| loss:   0.522193 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   0.590939 |\n",
            "| batch: 127.000000 |\n",
            "| loss:   0.532277 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   0.733946 |\n",
            "| batch: 129.000000 |\n",
            "| loss:   0.596846 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   0.534565 |\n",
            "| batch: 131.000000 |\n",
            "| loss:   0.760571 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   0.479202 |\n",
            "| batch: 133.000000 |\n",
            "| loss:   0.501905 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   0.540597 |\n",
            "| batch: 135.000000 |\n",
            "| loss:   0.704799 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   0.534045 |\n",
            "| batch: 137.000000 |\n",
            "| loss:   0.569159 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   0.540041 |\n",
            "| batch: 139.000000 |\n",
            "| loss:   0.536839 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   0.721711 |\n",
            "| batch: 141.000000 |\n",
            "| loss:   0.686806 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   0.572831 |\n",
            "| batch: 143.000000 |\n",
            "| loss:   0.699254 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   0.646774 |\n",
            "| batch: 145.000000 |\n",
            "| loss:   0.591076 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   0.514296 |\n",
            "| batch: 147.000000 |\n",
            "| loss:   0.504451 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   0.622279 |\n",
            "| batch: 149.000000 |\n",
            "| loss:   0.496600 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   0.566313 |\n",
            "| batch: 151.000000 |\n",
            "| loss:   0.690770 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   0.447936 |\n",
            "| batch: 153.000000 |\n",
            "| loss:   0.663203 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   0.482202 |\n",
            "| batch: 155.000000 |\n",
            "| loss:   0.659278 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   0.448408 |\n",
            "| batch: 157.000000 |\n",
            "| loss:   0.624417 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   0.523805 |\n",
            "| batch: 159.000000 |\n",
            "| loss:   0.475880 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   0.532823 |\n",
            "| batch: 161.000000 |\n",
            "| loss:   0.546752 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   0.474119 |\n",
            "| batch: 163.000000 |\n",
            "| loss:   0.540688 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   0.729074 |\n",
            "| batch: 165.000000 |\n",
            "| loss:   0.653134 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   0.495008 |\n",
            "| batch: 167.000000 |\n",
            "| loss:   0.530280 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   0.626238 |\n",
            "| batch: 169.000000 |\n",
            "| loss:   0.650855 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   0.560626 |\n",
            "| batch: 171.000000 |\n",
            "| loss:   0.741982 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   0.501978 |\n",
            "| batch: 173.000000 |\n",
            "| loss:   0.853556 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   0.513011 |\n",
            "| batch: 175.000000 |\n",
            "| loss:   0.522778 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   0.568823 |\n",
            "| batch: 177.000000 |\n",
            "| loss:   0.761512 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   0.493290 |\n",
            "| batch: 179.000000 |\n",
            "| loss:   0.541785 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   0.462991 |\n",
            "| batch: 181.000000 |\n",
            "| loss:   0.740731 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   0.780580 |\n",
            "| batch: 183.000000 |\n",
            "| loss:   0.558684 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   0.595942 |\n",
            "| batch: 185.000000 |\n",
            "| loss:   0.510897 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   0.425264 |\n",
            "| batch: 187.000000 |\n",
            "| loss:   0.629350 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   0.616462 |\n",
            "| batch: 189.000000 |\n",
            "| loss:   0.623289 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   0.509992 |\n",
            "| batch: 191.000000 |\n",
            "| loss:   0.529630 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   1.187420 |\n",
            "| batch: 193.000000 |\n",
            "| loss:   0.641231 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   0.584655 |\n",
            "| batch: 195.000000 |\n",
            "| loss:   0.591580 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   0.761455 |\n",
            "| batch: 197.000000 |\n",
            "| loss:   0.540490 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   0.595756 |\n",
            "| batch: 199.000000 |\n",
            "| loss:   0.531664 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   0.482996 |\n",
            "| batch: 201.000000 |\n",
            "| loss:   0.613511 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   0.634450 |\n",
            "| batch: 203.000000 |\n",
            "| loss:   0.716959 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   0.562972 |\n",
            "| batch: 205.000000 |\n",
            "| loss:   0.611332 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   0.625945 |\n",
            "| batch: 207.000000 |\n",
            "| loss:   0.635172 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   0.658349 |\n",
            "| batch: 209.000000 |\n",
            "| loss:   0.585121 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   0.978188 |\n",
            "| batch: 211.000000 |\n",
            "| loss:   0.673604 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   0.511360 |\n",
            "| batch: 213.000000 |\n",
            "| loss:   0.477286 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   0.551825 |\n",
            "| batch: 215.000000 |\n",
            "| loss:   0.497786 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   0.468677 |\n",
            "| batch: 217.000000 |\n",
            "| loss:   0.643664 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   0.521269 |\n",
            "| batch: 219.000000 |\n",
            "| loss:   0.556510 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   0.582409 |\n",
            "| batch: 221.000000 |\n",
            "| loss:   0.668836 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   0.510420 |\n",
            "| batch: 223.000000 |\n",
            "| loss:   0.475356 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   0.737904 |\n",
            "| batch: 225.000000 |\n",
            "| loss:   0.408920 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   0.575283 |\n",
            "| batch: 227.000000 |\n",
            "| loss:   0.588275 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   0.727964 |\n",
            "| batch: 229.000000 |\n",
            "| loss:   0.561201 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   1.084321 |\n",
            "| batch: 231.000000 |\n",
            "| loss:   0.579711 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   0.458397 |\n",
            "| batch: 233.000000 |\n",
            "| loss:   0.707947 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   0.577729 |\n",
            "| batch: 235.000000 |\n",
            "| loss:   0.475903 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   0.501919 |\n",
            "| batch: 237.000000 |\n",
            "| loss:   0.539748 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   0.428542 |\n",
            "| batch: 239.000000 |\n",
            "| loss:   0.725766 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   0.496462 |\n",
            "| batch: 241.000000 |\n",
            "| loss:   0.515150 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   0.523030 |\n",
            "| batch: 243.000000 |\n",
            "| loss:   0.638048 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   0.515923 |\n",
            "| batch: 245.000000 |\n",
            "| loss:   0.587571 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   0.519941 |\n",
            "| batch: 247.000000 |\n",
            "| loss:   0.543616 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   0.652800 |\n",
            "| batch: 249.000000 |\n",
            "| loss:   0.501278 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   0.581379 |\n",
            "| batch: 251.000000 |\n",
            "| loss:   0.700927 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   0.640156 |\n",
            "| batch: 253.000000 |\n",
            "| loss:   0.632517 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   0.540062 |\n",
            "| batch: 255.000000 |\n",
            "| loss:   0.651251 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   0.676320 |\n",
            "| batch: 257.000000 |\n",
            "| loss:   0.525927 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   0.585798 |\n",
            "| batch: 259.000000 |\n",
            "| loss:   0.720807 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   0.549189 |\n",
            "| batch: 261.000000 |\n",
            "| loss:   0.539800 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   0.595316 |\n",
            "| batch: 263.000000 |\n",
            "| loss:   0.696112 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   0.631653 |\n",
            "| batch: 265.000000 |\n",
            "| loss:   0.728081 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   0.886156 |\n",
            "| batch: 267.000000 |\n",
            "| loss:   0.827761 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   0.652647 |\n",
            "| batch: 269.000000 |\n",
            "| loss:   0.563158 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   0.667519 |\n",
            "| batch: 271.000000 |\n",
            "| loss:   0.573504 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   0.540502 |\n",
            "| batch: 273.000000 |\n",
            "| loss:   0.555471 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   0.659800 |\n",
            "| batch: 275.000000 |\n",
            "| loss:   0.525450 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   0.627143 |\n",
            "| batch: 277.000000 |\n",
            "| loss:   0.803010 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   0.623846 |\n",
            "| batch: 279.000000 |\n",
            "| loss:   0.447634 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   0.839812 |\n",
            "| batch: 281.000000 |\n",
            "| loss:   0.570354 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   0.470679 |\n",
            "| batch: 283.000000 |\n",
            "| loss:   0.734699 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   0.620158 |\n",
            "| batch: 285.000000 |\n",
            "| loss:   0.619796 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   0.564195 |\n",
            "| batch: 287.000000 |\n",
            "| loss:   0.604052 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   0.529276 |\n",
            "| batch: 289.000000 |\n",
            "| loss:   0.595122 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   0.488332 |\n",
            "| batch: 291.000000 |\n",
            "| loss:   0.488464 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   0.645547 |\n",
            "| batch: 293.000000 |\n",
            "| loss:   0.619737 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   0.566352 |\n",
            "| batch: 295.000000 |\n",
            "| loss:   0.617360 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   0.691781 |\n",
            "| batch: 297.000000 |\n",
            "| loss:   0.708779 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   0.586404 |\n",
            "| batch: 299.000000 |\n",
            "| loss:   0.561505 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   0.487426 |\n",
            "| batch: 301.000000 |\n",
            "| loss:   0.782439 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   0.610735 |\n",
            "| batch: 303.000000 |\n",
            "| loss:   0.787655 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   0.407919 |\n",
            "| batch: 305.000000 |\n",
            "| loss:   0.544625 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   0.518885 |\n",
            "| batch: 307.000000 |\n",
            "| loss:   0.674933 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   0.752339 |\n",
            "| batch: 309.000000 |\n",
            "| loss:   0.665005 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   0.566270 |\n",
            "| batch: 311.000000 |\n",
            "| loss:   0.537513 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   0.811799 |\n",
            "| batch: 313.000000 |\n",
            "| loss:   0.620689 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   0.667692 |\n",
            "| batch: 315.000000 |\n",
            "| loss:   0.754689 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   0.773276 |\n",
            "| batch: 317.000000 |\n",
            "| loss:   0.466862 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   0.688965 |\n",
            "| batch: 319.000000 |\n",
            "| loss:   0.536489 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   0.637355 |\n",
            "| batch: 321.000000 |\n",
            "| loss:   0.727790 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   0.583153 |\n",
            "| batch: 323.000000 |\n",
            "| loss:   0.630826 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   0.633682 |\n",
            "| batch: 325.000000 |\n",
            "| loss:   0.672503 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   0.539230 |\n",
            "| batch: 327.000000 |\n",
            "| loss:   0.495661 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   0.524381 |\n",
            "| batch: 329.000000 |\n",
            "| loss:   0.549925 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   0.807027 |\n",
            "| batch: 331.000000 |\n",
            "| loss:   0.695528 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   0.785917 |\n",
            "| batch: 333.000000 |\n",
            "| loss:   0.758094 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   0.836892 |\n",
            "| batch: 335.000000 |\n",
            "| loss:   0.706218 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   0.671699 |\n",
            "| batch: 337.000000 |\n",
            "| loss:   0.542933 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   0.590799 |\n",
            "| batch: 339.000000 |\n",
            "| loss:   0.690395 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   0.792603 |\n",
            "| batch: 341.000000 |\n",
            "| loss:   0.447022 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   0.594088 |\n",
            "| batch: 343.000000 |\n",
            "| loss:   0.639638 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   0.547202 |\n",
            "| batch: 345.000000 |\n",
            "| loss:   0.730554 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   0.472872 |\n",
            "| batch: 347.000000 |\n",
            "| loss:   0.647062 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   0.746633 |\n",
            "| batch: 349.000000 |\n",
            "| loss:   0.699462 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   0.535967 |\n",
            "| batch: 351.000000 |\n",
            "| loss:   0.697770 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   0.630438 |\n",
            "| batch: 353.000000 |\n",
            "| loss:   0.692873 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   0.524613 |\n",
            "| batch: 355.000000 |\n",
            "| loss:   0.514694 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   0.725701 |\n",
            "| batch: 357.000000 |\n",
            "| loss:   0.931559 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   0.470619 |\n",
            "| batch: 359.000000 |\n",
            "| loss:   0.496078 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   0.560571 |\n",
            "| batch: 361.000000 |\n",
            "| loss:   0.472307 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   0.731271 |\n",
            "| batch: 363.000000 |\n",
            "| loss:   0.675828 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   0.571009 |\n",
            "| batch: 365.000000 |\n",
            "| loss:   0.616943 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   0.608417 |\n",
            "| batch: 367.000000 |\n",
            "| loss:   0.678509 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   0.672178 |\n",
            "| batch: 369.000000 |\n",
            "| loss:   0.687419 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   0.540956 |\n",
            "| batch: 371.000000 |\n",
            "| loss:   0.614282 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   0.579584 |\n",
            "| batch: 373.000000 |\n",
            "| loss:   0.767146 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   0.820590 |\n",
            "| batch: 375.000000 |\n",
            "| loss:   0.663932 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   0.622132 |\n",
            "| batch: 377.000000 |\n",
            "| loss:   0.678516 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   0.730396 |\n",
            "| batch: 379.000000 |\n",
            "| loss:   0.717996 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   0.571052 |\n",
            "| batch: 381.000000 |\n",
            "| loss:   0.545406 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   0.736851 |\n",
            "| batch: 383.000000 |\n",
            "| loss:   0.855812 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   0.608225 |\n",
            "| batch: 385.000000 |\n",
            "| loss:   0.682317 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   0.522346 |\n",
            "| batch: 387.000000 |\n",
            "| loss:   0.738025 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   0.589222 |\n",
            "| batch: 389.000000 |\n",
            "| loss:   0.649405 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   0.577201 |\n",
            "| batch: 391.000000 |\n",
            "| loss:   0.457912 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   0.556283 |\n",
            "| batch: 393.000000 |\n",
            "| loss:   0.586138 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   0.515362 |\n",
            "| batch: 395.000000 |\n",
            "| loss:   0.482161 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   0.533063 |\n",
            "| batch: 397.000000 |\n",
            "| loss:   0.520733 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   0.681051 |\n",
            "| batch: 399.000000 |\n",
            "| loss:   0.666883 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   0.889098 |\n",
            "| batch: 401.000000 |\n",
            "| loss:   0.658824 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   0.675092 |\n",
            "| batch: 403.000000 |\n",
            "| loss:   0.680128 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   0.591849 |\n",
            "| batch: 405.000000 |\n",
            "| loss:   0.508775 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   0.718776 |\n",
            "| batch: 407.000000 |\n",
            "| loss:   0.751460 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   0.673209 |\n",
            "| batch: 409.000000 |\n",
            "| loss:   0.523988 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   0.687103 |\n",
            "| batch: 411.000000 |\n",
            "| loss:   0.867289 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   0.605282 |\n",
            "| batch: 413.000000 |\n",
            "| loss:   0.813474 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   0.555944 |\n",
            "| batch: 415.000000 |\n",
            "| loss:   0.641516 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   0.583240 |\n",
            "| batch: 417.000000 |\n",
            "| loss:   0.590491 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   0.550151 |\n",
            "| batch: 419.000000 |\n",
            "| loss:   0.570057 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   0.474389 |\n",
            "| batch: 421.000000 |\n",
            "| loss:   0.856696 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   0.695035 |\n",
            "| batch: 423.000000 |\n",
            "| loss:   0.526937 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   0.949175 |\n",
            "| batch: 425.000000 |\n",
            "| loss:   0.659270 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   0.718420 |\n",
            "| batch: 427.000000 |\n",
            "| loss:   0.580734 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   0.667731 |\n",
            "| batch: 429.000000 |\n",
            "| loss:   0.540265 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   0.537689 |\n",
            "| batch: 431.000000 |\n",
            "| loss:   0.609274 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   0.602083 |\n",
            "| batch: 433.000000 |\n",
            "| loss:   0.813492 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   0.568491 |\n",
            "| batch: 435.000000 |\n",
            "| loss:   0.846840 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   0.603173 |\n",
            "| batch: 437.000000 |\n",
            "| loss:   0.637637 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   0.603878 |\n",
            "| batch: 439.000000 |\n",
            "| loss:   0.593126 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   0.683782 |\n",
            "| batch: 441.000000 |\n",
            "| loss:   0.742092 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   0.617476 |\n",
            "| batch: 443.000000 |\n",
            "| loss:   0.650183 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   0.478779 |\n",
            "| batch: 445.000000 |\n",
            "| loss:   0.635959 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   0.603875 |\n",
            "| batch: 447.000000 |\n",
            "| loss:   0.618791 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   0.681990 |\n",
            "| batch: 449.000000 |\n",
            "| loss:   0.683984 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   0.535927 |\n",
            "| batch: 451.000000 |\n",
            "| loss:   0.602850 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   0.737354 |\n",
            "| batch: 453.000000 |\n",
            "| loss:   0.584381 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   0.606633 |\n",
            "| batch: 455.000000 |\n",
            "| loss:   0.632270 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   0.770170 |\n",
            "| batch: 457.000000 |\n",
            "| loss:   0.782714 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   0.771712 |\n",
            "| batch: 459.000000 |\n",
            "| loss:   0.520575 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   0.563027 |\n",
            "| batch: 461.000000 |\n",
            "| loss:   0.589228 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   0.668011 |\n",
            "| batch: 463.000000 |\n",
            "| loss:   0.554554 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   0.859357 |\n",
            "| batch: 465.000000 |\n",
            "| loss:   0.532568 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   0.536279 |\n",
            "| batch: 467.000000 |\n",
            "| loss:   0.624672 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   0.686561 |\n",
            "| batch: 469.000000 |\n",
            "| loss:   0.576388 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   0.622452 |\n",
            "| batch: 471.000000 |\n",
            "| loss:   0.931903 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   0.647046 |\n",
            "| batch: 473.000000 |\n",
            "| loss:   0.565862 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   0.709110 |\n",
            "| batch: 475.000000 |\n",
            "| loss:   0.541912 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   0.856186 |\n",
            "| batch: 477.000000 |\n",
            "| loss:   0.618732 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   0.813574 |\n",
            "| batch: 479.000000 |\n",
            "| loss:   0.642666 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   0.605432 |\n",
            "| batch: 481.000000 |\n",
            "| loss:   0.639012 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   0.669690 |\n",
            "| batch: 483.000000 |\n",
            "| loss:   0.941902 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   0.765260 |\n",
            "| batch: 485.000000 |\n",
            "| loss:   1.116461 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   0.774783 |\n",
            "| batch: 487.000000 |\n",
            "| loss:   0.823063 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   0.696484 |\n",
            "| batch: 489.000000 |\n",
            "| loss:   0.548856 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   0.590570 |\n",
            "| batch: 491.000000 |\n",
            "| loss:   0.628134 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   0.642793 |\n",
            "| batch: 493.000000 |\n",
            "| loss:   0.609526 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   0.609609 |\n",
            "| batch: 495.000000 |\n",
            "| loss:   0.718352 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   0.889914 |\n",
            "| batch: 497.000000 |\n",
            "| loss:   0.646033 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   0.646484 |\n",
            "| batch: 499.000000 |\n",
            "| loss:   0.549607 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   0.487103 |\n",
            "| batch: 501.000000 |\n",
            "| loss:   0.655191 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   0.509075 |\n",
            "| batch: 503.000000 |\n",
            "| loss:   0.611409 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   0.533756 |\n",
            "| batch: 505.000000 |\n",
            "| loss:   0.749533 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   0.572463 |\n",
            "| batch: 507.000000 |\n",
            "| loss:   0.663069 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   0.623982 |\n",
            "| batch: 509.000000 |\n",
            "| loss:   1.051478 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   0.703328 |\n",
            "| batch: 511.000000 |\n",
            "| loss:   0.526047 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   0.598632 |\n",
            "| batch: 513.000000 |\n",
            "| loss:   0.798729 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   0.655365 |\n",
            "| batch: 515.000000 |\n",
            "| loss:   0.688736 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   0.524656 |\n",
            "| batch: 517.000000 |\n",
            "| loss:   0.589971 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   0.544723 |\n",
            "| batch: 519.000000 |\n",
            "| loss:   0.488663 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   0.784239 |\n",
            "| batch: 521.000000 |\n",
            "| loss:   0.568021 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   0.718447 |\n",
            "| batch: 523.000000 |\n",
            "| loss:   0.890465 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   0.584821 |\n",
            "| batch: 525.000000 |\n",
            "| loss:   0.749972 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   0.553897 |\n",
            "| batch: 527.000000 |\n",
            "| loss:   0.898459 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   0.720336 |\n",
            "| batch: 529.000000 |\n",
            "| loss:   0.662436 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   0.520596 |\n",
            "| batch: 531.000000 |\n",
            "| loss:   0.839543 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   0.765243 |\n",
            "| batch: 533.000000 |\n",
            "| loss:   0.716310 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   0.648094 |\n",
            "| batch: 535.000000 |\n",
            "| loss:   0.676162 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   0.804023 |\n",
            "| batch: 537.000000 |\n",
            "| loss:   0.579814 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   0.585647 |\n",
            "| batch: 539.000000 |\n",
            "| loss:   0.678461 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   0.698303 |\n",
            "| batch: 541.000000 |\n",
            "| loss:   0.503997 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   0.524585 |\n",
            "| batch: 543.000000 |\n",
            "| loss:   0.576809 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   0.848522 |\n",
            "| batch: 545.000000 |\n",
            "| loss:   0.665824 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   0.660533 |\n",
            "| batch: 547.000000 |\n",
            "| loss:   0.569602 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   0.677032 |\n",
            "| batch: 549.000000 |\n",
            "| loss:   0.600043 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   0.548114 |\n",
            "| batch: 551.000000 |\n",
            "| loss:   0.796693 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   0.670032 |\n",
            "| batch: 553.000000 |\n",
            "| loss:   0.535435 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   0.554074 |\n",
            "| batch: 555.000000 |\n",
            "| loss:   0.838838 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   0.637259 |\n",
            "| batch: 557.000000 |\n",
            "| loss:   0.728699 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   0.681105 |\n",
            "| batch: 559.000000 |\n",
            "| loss:   0.573411 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   0.545815 |\n",
            "| batch: 561.000000 |\n",
            "| loss:   0.598986 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   0.499005 |\n",
            "| batch: 563.000000 |\n",
            "| loss:   0.692443 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   0.782487 |\n",
            "| batch: 565.000000 |\n",
            "| loss:   0.585736 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   0.696832 |\n",
            "| batch: 567.000000 |\n",
            "| loss:   0.807036 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   0.609509 |\n",
            "| batch: 569.000000 |\n",
            "| loss:   0.613764 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   0.791542 |\n",
            "| batch: 571.000000 |\n",
            "| loss:   0.520454 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   0.788109 |\n",
            "| batch: 573.000000 |\n",
            "| loss:   0.614427 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   0.510875 |\n",
            "| batch: 575.000000 |\n",
            "| loss:   0.616456 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   0.633583 |\n",
            "| batch: 577.000000 |\n",
            "| loss:   0.798052 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   0.889343 |\n",
            "| batch: 579.000000 |\n",
            "| loss:   0.574553 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   0.757076 |\n",
            "| batch: 581.000000 |\n",
            "| loss:   0.783186 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   0.926312 |\n",
            "| batch: 583.000000 |\n",
            "| loss:   0.738906 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   0.662918 |\n",
            "| batch: 585.000000 |\n",
            "| loss:   0.678037 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   0.472700 |\n",
            "| batch: 587.000000 |\n",
            "| loss:   0.958037 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   0.667588 |\n",
            "| batch: 589.000000 |\n",
            "| loss:   0.694793 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   0.641127 |\n",
            "| batch: 591.000000 |\n",
            "| loss:   0.687187 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   0.767697 |\n",
            "| batch: 593.000000 |\n",
            "| loss:   0.546849 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   0.660254 |\n",
            "| batch: 595.000000 |\n",
            "| loss:   0.706518 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   0.586875 |\n",
            "| batch: 597.000000 |\n",
            "| loss:   0.691256 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   0.529761 |\n",
            "| batch: 599.000000 |\n",
            "| loss:   0.738333 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   0.598018 |\n",
            "| batch: 601.000000 |\n",
            "| loss:   0.564341 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   0.599067 |\n",
            "| batch: 603.000000 |\n",
            "| loss:   0.750559 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   0.558622 |\n",
            "| batch: 605.000000 |\n",
            "| loss:   0.612058 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   0.844047 |\n",
            "| batch: 607.000000 |\n",
            "| loss:   0.586094 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   0.548888 |\n",
            "| batch: 609.000000 |\n",
            "| loss:   0.485434 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   0.778707 |\n",
            "| batch: 611.000000 |\n",
            "| loss:   1.026280 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   0.608504 |\n",
            "| batch: 613.000000 |\n",
            "| loss:   0.818597 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   0.590654 |\n",
            "| batch: 615.000000 |\n",
            "| loss:   0.593986 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   0.631206 |\n",
            "| batch: 617.000000 |\n",
            "| loss:   0.727216 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   0.739018 |\n",
            "| batch: 619.000000 |\n",
            "| loss:   0.648696 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   0.620453 |\n",
            "| batch: 621.000000 |\n",
            "| loss:   0.831933 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   0.636973 |\n",
            "| batch: 623.000000 |\n",
            "| loss:   0.543083 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   0.599505 |\n",
            "| batch: 625.000000 |\n",
            "| loss:   0.516347 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   0.508781 |\n",
            "| batch: 627.000000 |\n",
            "| loss:   0.578232 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   0.746012 |\n",
            "| batch: 629.000000 |\n",
            "| loss:   0.776671 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   0.660600 |\n",
            "| batch: 631.000000 |\n",
            "| loss:   0.621530 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   0.654334 |\n",
            "| batch: 633.000000 |\n",
            "| loss:   0.940446 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   0.585197 |\n",
            "| batch: 635.000000 |\n",
            "| loss:   0.603505 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   0.797223 |\n",
            "| batch: 637.000000 |\n",
            "| loss:   0.699189 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   0.716794 |\n",
            "| batch: 639.000000 |\n",
            "| loss:   0.924022 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   0.628837 |\n",
            "| batch: 641.000000 |\n",
            "| loss:   0.592719 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   0.699643 |\n",
            "| batch: 643.000000 |\n",
            "| loss:   0.543996 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   1.017387 |\n",
            "| batch: 645.000000 |\n",
            "| loss:   0.604352 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   0.737160 |\n",
            "| batch: 647.000000 |\n",
            "| loss:   0.759120 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   0.732269 |\n",
            "| batch: 649.000000 |\n",
            "| loss:   0.535240 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   0.828596 |\n",
            "| batch: 651.000000 |\n",
            "| loss:   0.671053 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   0.569728 |\n",
            "| batch: 653.000000 |\n",
            "| loss:   0.714410 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   0.732858 |\n",
            "| batch: 655.000000 |\n",
            "| loss:   0.557043 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   0.554746 |\n",
            "| batch: 657.000000 |\n",
            "| loss:   0.717254 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   0.531892 |\n",
            "| batch: 659.000000 |\n",
            "| loss:   0.755936 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   0.783912 |\n",
            "| batch: 661.000000 |\n",
            "| loss:   0.719218 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   0.757391 |\n",
            "| batch: 663.000000 |\n",
            "| loss:   1.059547 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   0.566410 |\n",
            "| batch: 665.000000 |\n",
            "| loss:   0.629463 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   0.645982 |\n",
            "| batch: 667.000000 |\n",
            "| loss:   0.657152 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   0.773505 |\n",
            "| batch: 669.000000 |\n",
            "| loss:   0.696398 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   0.762056 |\n",
            "| batch: 671.000000 |\n",
            "| loss:   0.630463 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   0.966465 |\n",
            "| batch: 673.000000 |\n",
            "| loss:   0.595673 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   0.654045 |\n",
            "| batch: 675.000000 |\n",
            "| loss:   0.622728 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   0.690958 |\n",
            "| batch: 677.000000 |\n",
            "| loss:   0.677152 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   0.585180 |\n",
            "| batch: 679.000000 |\n",
            "| loss:   0.495495 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   0.677035 |\n",
            "| batch: 681.000000 |\n",
            "| loss:   0.732895 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   0.728919 |\n",
            "| batch: 683.000000 |\n",
            "| loss:   0.741790 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   0.605507 |\n",
            "| batch: 685.000000 |\n",
            "| loss:   0.597067 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   0.589968 |\n",
            "| batch: 687.000000 |\n",
            "| loss:   0.485270 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   0.669585 |\n",
            "| batch: 689.000000 |\n",
            "| loss:   0.657037 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   0.746610 |\n",
            "| batch: 691.000000 |\n",
            "| loss:   0.807799 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   0.547533 |\n",
            "| batch: 693.000000 |\n",
            "| loss:   0.545329 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   0.748363 |\n",
            "| batch: 695.000000 |\n",
            "| loss:   0.696168 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   0.603481 |\n",
            "| batch: 697.000000 |\n",
            "| loss:   0.619540 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   0.567799 |\n",
            "| batch: 699.000000 |\n",
            "| loss:   0.899460 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   0.849784 |\n",
            "| batch: 701.000000 |\n",
            "| loss:   0.726894 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   0.631129 |\n",
            "| batch: 703.000000 |\n",
            "| loss:   0.693804 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   0.616589 |\n",
            "| batch: 705.000000 |\n",
            "| loss:   0.660352 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   0.646333 |\n",
            "| batch: 707.000000 |\n",
            "| loss:   0.735908 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   0.703038 |\n",
            "| batch: 709.000000 |\n",
            "| loss:   0.616076 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   0.951175 |\n",
            "| batch: 711.000000 |\n",
            "| loss:   0.689173 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   0.577844 |\n",
            "| batch: 713.000000 |\n",
            "| loss:   0.602972 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   0.434246 |\n",
            "| batch: 715.000000 |\n",
            "| loss:   0.771787 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   0.610883 |\n",
            "| batch: 717.000000 |\n",
            "| loss:   0.639814 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   0.598695 |\n",
            "| batch: 719.000000 |\n",
            "| loss:   0.634914 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   0.838881 |\n",
            "| batch: 721.000000 |\n",
            "| loss:   0.553446 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   0.757651 |\n",
            "| batch: 723.000000 |\n",
            "| loss:   0.762688 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   0.586308 |\n",
            "| batch: 725.000000 |\n",
            "| loss:   0.664386 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   0.549798 |\n",
            "| batch: 727.000000 |\n",
            "| loss:   0.661773 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   0.987706 |\n",
            "| batch: 729.000000 |\n",
            "| loss:   0.569164 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   0.721714 |\n",
            "| batch: 731.000000 |\n",
            "| loss:   0.663761 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   0.662255 |\n",
            "| batch: 733.000000 |\n",
            "| loss:   0.644197 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   0.573869 |\n",
            "| batch: 735.000000 |\n",
            "| loss:   0.720491 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   1.388157 |\n",
            "| batch: 737.000000 |\n",
            "| loss:   0.834641 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   0.630747 |\n",
            "| batch: 739.000000 |\n",
            "| loss:   0.775844 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   0.698613 |\n",
            "| batch: 741.000000 |\n",
            "| loss:   0.630008 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   0.534467 |\n",
            "| batch: 743.000000 |\n",
            "| loss:   0.778287 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   0.655222 |\n",
            "| batch: 745.000000 |\n",
            "| loss:   0.572429 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   0.681896 |\n",
            "| batch: 747.000000 |\n",
            "| loss:   0.700230 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   0.695149 |\n",
            "| batch: 749.000000 |\n",
            "| loss:   0.638310 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   0.567563 |\n",
            "| batch: 751.000000 |\n",
            "| loss:   0.662787 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   0.841944 |\n",
            "| batch: 753.000000 |\n",
            "| loss:   0.719957 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   0.913743 |\n",
            "| batch: 755.000000 |\n",
            "| loss:   0.774226 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   0.769254 |\n",
            "| batch: 757.000000 |\n",
            "| loss:   0.747138 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   0.830331 |\n",
            "| batch: 759.000000 |\n",
            "| loss:   0.824622 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   0.878276 |\n",
            "| batch: 761.000000 |\n",
            "| loss:   0.602314 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   0.699627 |\n",
            "| batch: 763.000000 |\n",
            "| loss:   0.667215 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   0.687633 |\n",
            "| batch: 765.000000 |\n",
            "| loss:   0.632375 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   0.611739 |\n",
            "| batch: 767.000000 |\n",
            "| loss:   0.847529 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   0.699295 |\n",
            "| batch: 769.000000 |\n",
            "| loss:   0.618313 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   0.682666 |\n",
            "| batch: 771.000000 |\n",
            "| loss:   0.635080 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   0.636384 |\n",
            "| batch: 773.000000 |\n",
            "| loss:   0.629574 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   0.547470 |\n",
            "| batch: 775.000000 |\n",
            "| loss:   0.644261 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   0.745710 |\n",
            "| batch: 777.000000 |\n",
            "| loss:   0.938404 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   0.871077 |\n",
            "| batch: 779.000000 |\n",
            "| loss:   0.618904 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   0.775679 |\n",
            "| batch: 781.000000 |\n",
            "| loss:   0.672995 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   0.541875 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   1.687874 |\n",
            "| val_batch:   3.000000 |\n",
            "| val_loss:   1.671150 |\n",
            "| val_batch:   6.000000 |\n",
            "| val_loss:   1.828000 |\n",
            "| val_batch:   9.000000 |\n",
            "| val_loss:   1.673973 |\n",
            "| val_batch:  12.000000 |\n",
            "| val_loss:   1.384078 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   1.362569 |\n",
            "| val_batch:  18.000000 |\n",
            "| val_loss:   1.681402 |\n",
            "| val_batch:  21.000000 |\n",
            "| val_loss:   1.534112 |\n",
            "| val_batch:  24.000000 |\n",
            "| val_loss:   1.164487 |\n",
            "| val_batch:  26.000000 |\n",
            "| val_loss:   1.762888 |\n",
            "| val_batch:  29.000000 |\n",
            "| val_loss:   1.461750 |\n",
            "| val_batch:  32.000000 |\n",
            "| val_loss:   1.480265 |\n",
            "| val_batch:  34.000000 |\n",
            "| val_loss:   1.398454 |\n",
            "| val_batch:  37.000000 |\n",
            "| val_loss:   1.618636 |\n",
            "| val_batch:  40.000000 |\n",
            "| val_loss:   1.549130 |\n",
            "| val_batch:  43.000000 |\n",
            "| val_loss:   1.614624 |\n",
            "| val_batch:  46.000000 |\n",
            "| val_loss:   1.509317 |\n",
            "| val_batch:  49.000000 |\n",
            "| val_loss:   1.843548 |\n",
            "| val_batch:  51.000000 |\n",
            "| val_loss:   1.398508 |\n",
            "| val_batch:  54.000000 |\n",
            "| val_loss:   1.926519 |\n",
            "| val_batch:  56.000000 |\n",
            "| val_loss:   1.399920 |\n",
            "| val_batch:  59.000000 |\n",
            "| val_loss:   1.716926 |\n",
            "| val_batch:  62.000000 |\n",
            "| val_loss:   1.723176 |\n",
            "| val_batch:  65.000000 |\n",
            "| val_loss:   1.378404 |\n",
            "| val_batch:  68.000000 |\n",
            "| val_loss:   1.495443 |\n",
            "| val_batch:  71.000000 |\n",
            "| val_loss:   1.430462 |\n",
            "| val_batch:  74.000000 |\n",
            "| val_loss:   2.089260 |\n",
            "| val_batch:  77.000000 |\n",
            "| val_loss:   1.339351 |\n",
            "| val_batch:  79.000000 |\n",
            "| val_loss:   1.909229 |\n",
            "| val_batch:  82.000000 |\n",
            "| val_loss:   1.723919 |\n",
            "| val_batch:  85.000000 |\n",
            "| val_loss:   1.753876 |\n",
            "Input: The steak is well done.\n",
            "Output: el <UNK> llamada bien <UNK> . llamada . llamada . llamada . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . <UNK> . . <UNK> . <UNK> . . . <UNK> . <UNK> . . . <UNK> . <UNK> . . . . . . . . . . . . . . . . . . . . .\n",
            "Epoch: 07 | Time: 16m 49s\n",
            "\tTrain Loss: 0.639 | Train PPL:   1.895\n",
            "\t Val. Loss: 1.582 |  Val. PPL:   4.863\n",
            "| epoch:   7.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   0.434400 |\n",
            "| batch:   1.000000 |\n",
            "| loss:   0.450758 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   0.578039 |\n",
            "| batch:   3.000000 |\n",
            "| loss:   0.478423 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   0.701927 |\n",
            "| batch:   5.000000 |\n",
            "| loss:   0.467383 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   0.405333 |\n",
            "| batch:   7.000000 |\n",
            "| loss:   0.578604 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   0.429163 |\n",
            "| batch:   9.000000 |\n",
            "| loss:   0.525649 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   0.596144 |\n",
            "| batch:  11.000000 |\n",
            "| loss:   0.475047 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   0.529415 |\n",
            "| batch:  13.000000 |\n",
            "| loss:   0.682548 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   0.398209 |\n",
            "| batch:  15.000000 |\n",
            "| loss:   0.455844 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   0.420812 |\n",
            "| batch:  17.000000 |\n",
            "| loss:   0.395995 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   0.543015 |\n",
            "| batch:  19.000000 |\n",
            "| loss:   0.456518 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   0.453681 |\n",
            "| batch:  21.000000 |\n",
            "| loss:   0.514513 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   0.437410 |\n",
            "| batch:  23.000000 |\n",
            "| loss:   0.527291 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   0.398906 |\n",
            "| batch:  25.000000 |\n",
            "| loss:   0.403825 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   0.503626 |\n",
            "| batch:  27.000000 |\n",
            "| loss:   0.512381 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   0.591312 |\n",
            "| batch:  29.000000 |\n",
            "| loss:   0.658112 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   0.572070 |\n",
            "| batch:  31.000000 |\n",
            "| loss:   0.547267 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   0.564836 |\n",
            "| batch:  33.000000 |\n",
            "| loss:   0.716078 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   0.409947 |\n",
            "| batch:  35.000000 |\n",
            "| loss:   0.576365 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   0.418600 |\n",
            "| batch:  37.000000 |\n",
            "| loss:   0.649401 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   0.402948 |\n",
            "| batch:  39.000000 |\n",
            "| loss:   0.498027 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   0.391263 |\n",
            "| batch:  41.000000 |\n",
            "| loss:   0.458781 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   0.500478 |\n",
            "| batch:  43.000000 |\n",
            "| loss:   0.409327 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   0.808245 |\n",
            "| batch:  45.000000 |\n",
            "| loss:   0.384827 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   0.426273 |\n",
            "| batch:  47.000000 |\n",
            "| loss:   0.570224 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   0.419825 |\n",
            "| batch:  49.000000 |\n",
            "| loss:   0.569515 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   0.605062 |\n",
            "| batch:  51.000000 |\n",
            "| loss:   0.622894 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   0.781224 |\n",
            "| batch:  53.000000 |\n",
            "| loss:   0.520389 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   0.436385 |\n",
            "| batch:  55.000000 |\n",
            "| loss:   0.569456 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   0.431175 |\n",
            "| batch:  57.000000 |\n",
            "| loss:   0.487144 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   0.388539 |\n",
            "| batch:  59.000000 |\n",
            "| loss:   0.426520 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   0.511027 |\n",
            "| batch:  61.000000 |\n",
            "| loss:   0.450581 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   0.551798 |\n",
            "| batch:  63.000000 |\n",
            "| loss:   0.528761 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   0.926284 |\n",
            "| batch:  65.000000 |\n",
            "| loss:   0.481872 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   0.427312 |\n",
            "| batch:  67.000000 |\n",
            "| loss:   0.445238 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   0.435220 |\n",
            "| batch:  69.000000 |\n",
            "| loss:   0.462896 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   0.668605 |\n",
            "| batch:  71.000000 |\n",
            "| loss:   0.560163 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   0.488509 |\n",
            "| batch:  73.000000 |\n",
            "| loss:   0.489850 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   0.595320 |\n",
            "| batch:  75.000000 |\n",
            "| loss:   0.483845 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   0.443776 |\n",
            "| batch:  77.000000 |\n",
            "| loss:   0.403051 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   0.499641 |\n",
            "| batch:  79.000000 |\n",
            "| loss:   0.449756 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   0.506989 |\n",
            "| batch:  81.000000 |\n",
            "| loss:   0.496338 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   0.617808 |\n",
            "| batch:  83.000000 |\n",
            "| loss:   0.448339 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   0.551560 |\n",
            "| batch:  85.000000 |\n",
            "| loss:   0.427664 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   0.499546 |\n",
            "| batch:  87.000000 |\n",
            "| loss:   0.353884 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   0.579671 |\n",
            "| batch:  89.000000 |\n",
            "| loss:   0.526848 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   0.548012 |\n",
            "| batch:  91.000000 |\n",
            "| loss:   0.562944 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   0.382676 |\n",
            "| batch:  93.000000 |\n",
            "| loss:   0.380712 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   0.619674 |\n",
            "| batch:  95.000000 |\n",
            "| loss:   0.432751 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   0.641814 |\n",
            "| batch:  97.000000 |\n",
            "| loss:   0.273887 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   0.429338 |\n",
            "| batch:  99.000000 |\n",
            "| loss:   0.545120 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   0.438232 |\n",
            "| batch: 101.000000 |\n",
            "| loss:   0.437162 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   0.551446 |\n",
            "| batch: 103.000000 |\n",
            "| loss:   0.514757 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   0.532757 |\n",
            "| batch: 105.000000 |\n",
            "| loss:   0.378872 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   0.397306 |\n",
            "| batch: 107.000000 |\n",
            "| loss:   0.559778 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   0.475904 |\n",
            "| batch: 109.000000 |\n",
            "| loss:   0.485629 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   0.424068 |\n",
            "| batch: 111.000000 |\n",
            "| loss:   0.407010 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   0.390683 |\n",
            "| batch: 113.000000 |\n",
            "| loss:   0.481178 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   0.399474 |\n",
            "| batch: 115.000000 |\n",
            "| loss:   0.614253 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   0.725905 |\n",
            "| batch: 117.000000 |\n",
            "| loss:   0.468479 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   0.485414 |\n",
            "| batch: 119.000000 |\n",
            "| loss:   0.382698 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   0.708843 |\n",
            "| batch: 121.000000 |\n",
            "| loss:   0.464638 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   0.668432 |\n",
            "| batch: 123.000000 |\n",
            "| loss:   0.429800 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   0.466496 |\n",
            "| batch: 125.000000 |\n",
            "| loss:   0.536071 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   0.482513 |\n",
            "| batch: 127.000000 |\n",
            "| loss:   0.411147 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   0.523451 |\n",
            "| batch: 129.000000 |\n",
            "| loss:   0.561870 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   0.711302 |\n",
            "| batch: 131.000000 |\n",
            "| loss:   0.464121 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   0.472925 |\n",
            "| batch: 133.000000 |\n",
            "| loss:   0.721765 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   0.540409 |\n",
            "| batch: 135.000000 |\n",
            "| loss:   0.676211 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   0.395381 |\n",
            "| batch: 137.000000 |\n",
            "| loss:   0.592654 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   0.444546 |\n",
            "| batch: 139.000000 |\n",
            "| loss:   0.427544 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   0.469628 |\n",
            "| batch: 141.000000 |\n",
            "| loss:   0.634217 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   0.444689 |\n",
            "| batch: 143.000000 |\n",
            "| loss:   0.442782 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   0.340033 |\n",
            "| batch: 145.000000 |\n",
            "| loss:   0.579682 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   0.410802 |\n",
            "| batch: 147.000000 |\n",
            "| loss:   0.480561 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   0.361510 |\n",
            "| batch: 149.000000 |\n",
            "| loss:   0.468792 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   0.449146 |\n",
            "| batch: 151.000000 |\n",
            "| loss:   0.531536 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   0.557339 |\n",
            "| batch: 153.000000 |\n",
            "| loss:   0.583547 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   0.459838 |\n",
            "| batch: 155.000000 |\n",
            "| loss:   0.885704 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   0.608919 |\n",
            "| batch: 157.000000 |\n",
            "| loss:   0.589144 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   0.462630 |\n",
            "| batch: 159.000000 |\n",
            "| loss:   0.533210 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   0.476164 |\n",
            "| batch: 161.000000 |\n",
            "| loss:   0.587971 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   0.648870 |\n",
            "| batch: 163.000000 |\n",
            "| loss:   0.792609 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   0.562759 |\n",
            "| batch: 165.000000 |\n",
            "| loss:   0.446109 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   0.449288 |\n",
            "| batch: 167.000000 |\n",
            "| loss:   0.694685 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   0.397537 |\n",
            "| batch: 169.000000 |\n",
            "| loss:   0.449467 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   0.439334 |\n",
            "| batch: 171.000000 |\n",
            "| loss:   0.548391 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   0.585494 |\n",
            "| batch: 173.000000 |\n",
            "| loss:   0.451395 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   0.518827 |\n",
            "| batch: 175.000000 |\n",
            "| loss:   0.459223 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   0.387808 |\n",
            "| batch: 177.000000 |\n",
            "| loss:   0.578490 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   0.389211 |\n",
            "| batch: 179.000000 |\n",
            "| loss:   0.431930 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   0.561677 |\n",
            "| batch: 181.000000 |\n",
            "| loss:   0.513940 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   0.388221 |\n",
            "| batch: 183.000000 |\n",
            "| loss:   0.464445 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   0.635123 |\n",
            "| batch: 185.000000 |\n",
            "| loss:   0.443077 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   0.477802 |\n",
            "| batch: 187.000000 |\n",
            "| loss:   0.482426 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   0.450368 |\n",
            "| batch: 189.000000 |\n",
            "| loss:   0.655445 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   0.677946 |\n",
            "| batch: 191.000000 |\n",
            "| loss:   0.648267 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   0.531436 |\n",
            "| batch: 193.000000 |\n",
            "| loss:   0.450384 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   0.569961 |\n",
            "| batch: 195.000000 |\n",
            "| loss:   0.539832 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   0.538964 |\n",
            "| batch: 197.000000 |\n",
            "| loss:   0.643417 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   0.483354 |\n",
            "| batch: 199.000000 |\n",
            "| loss:   0.617129 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   0.394360 |\n",
            "| batch: 201.000000 |\n",
            "| loss:   0.456277 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   0.392169 |\n",
            "| batch: 203.000000 |\n",
            "| loss:   0.473434 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   0.670176 |\n",
            "| batch: 205.000000 |\n",
            "| loss:   0.441995 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   0.586551 |\n",
            "| batch: 207.000000 |\n",
            "| loss:   0.610650 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   0.496677 |\n",
            "| batch: 209.000000 |\n",
            "| loss:   0.567216 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   0.703397 |\n",
            "| batch: 211.000000 |\n",
            "| loss:   0.601519 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   0.778114 |\n",
            "| batch: 213.000000 |\n",
            "| loss:   0.673596 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   0.421415 |\n",
            "| batch: 215.000000 |\n",
            "| loss:   0.607779 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   0.523240 |\n",
            "| batch: 217.000000 |\n",
            "| loss:   0.528815 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   0.556216 |\n",
            "| batch: 219.000000 |\n",
            "| loss:   0.449670 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   0.429469 |\n",
            "| batch: 221.000000 |\n",
            "| loss:   0.459351 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   0.405558 |\n",
            "| batch: 223.000000 |\n",
            "| loss:   0.474126 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   0.426251 |\n",
            "| batch: 225.000000 |\n",
            "| loss:   0.452807 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   0.575310 |\n",
            "| batch: 227.000000 |\n",
            "| loss:   0.471709 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   0.410116 |\n",
            "| batch: 229.000000 |\n",
            "| loss:   0.513729 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   0.502809 |\n",
            "| batch: 231.000000 |\n",
            "| loss:   0.633921 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   0.605815 |\n",
            "| batch: 233.000000 |\n",
            "| loss:   0.604861 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   0.476802 |\n",
            "| batch: 235.000000 |\n",
            "| loss:   0.542239 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   0.470985 |\n",
            "| batch: 237.000000 |\n",
            "| loss:   0.570194 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   0.658357 |\n",
            "| batch: 239.000000 |\n",
            "| loss:   0.585802 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   0.626984 |\n",
            "| batch: 241.000000 |\n",
            "| loss:   0.785470 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   0.489299 |\n",
            "| batch: 243.000000 |\n",
            "| loss:   0.475208 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   0.420931 |\n",
            "| batch: 245.000000 |\n",
            "| loss:   0.468000 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   0.492650 |\n",
            "| batch: 247.000000 |\n",
            "| loss:   0.651491 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   0.572427 |\n",
            "| batch: 249.000000 |\n",
            "| loss:   0.580871 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   0.565490 |\n",
            "| batch: 251.000000 |\n",
            "| loss:   0.400231 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   0.483181 |\n",
            "| batch: 253.000000 |\n",
            "| loss:   0.482403 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   0.512552 |\n",
            "| batch: 255.000000 |\n",
            "| loss:   0.563655 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   0.414891 |\n",
            "| batch: 257.000000 |\n",
            "| loss:   0.633261 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   0.507429 |\n",
            "| batch: 259.000000 |\n",
            "| loss:   0.384202 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   0.539706 |\n",
            "| batch: 261.000000 |\n",
            "| loss:   0.467828 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   0.441907 |\n",
            "| batch: 263.000000 |\n",
            "| loss:   0.403765 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   0.485760 |\n",
            "| batch: 265.000000 |\n",
            "| loss:   0.585838 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   0.630010 |\n",
            "| batch: 267.000000 |\n",
            "| loss:   0.437856 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   0.497438 |\n",
            "| batch: 269.000000 |\n",
            "| loss:   0.676751 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   0.458142 |\n",
            "| batch: 271.000000 |\n",
            "| loss:   0.432150 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   0.565477 |\n",
            "| batch: 273.000000 |\n",
            "| loss:   0.571501 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   0.567328 |\n",
            "| batch: 275.000000 |\n",
            "| loss:   0.492602 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   0.538931 |\n",
            "| batch: 277.000000 |\n",
            "| loss:   0.422072 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   0.591228 |\n",
            "| batch: 279.000000 |\n",
            "| loss:   0.673717 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   0.559077 |\n",
            "| batch: 281.000000 |\n",
            "| loss:   0.430178 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   0.779073 |\n",
            "| batch: 283.000000 |\n",
            "| loss:   0.636014 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   0.445088 |\n",
            "| batch: 285.000000 |\n",
            "| loss:   0.777240 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   0.594124 |\n",
            "| batch: 287.000000 |\n",
            "| loss:   0.537799 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   0.462982 |\n",
            "| batch: 289.000000 |\n",
            "| loss:   0.421796 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   0.515910 |\n",
            "| batch: 291.000000 |\n",
            "| loss:   0.529186 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   0.420349 |\n",
            "| batch: 293.000000 |\n",
            "| loss:   0.591403 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   0.472659 |\n",
            "| batch: 295.000000 |\n",
            "| loss:   0.526433 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   0.564636 |\n",
            "| batch: 297.000000 |\n",
            "| loss:   0.658202 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   0.446766 |\n",
            "| batch: 299.000000 |\n",
            "| loss:   0.513412 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   0.494592 |\n",
            "| batch: 301.000000 |\n",
            "| loss:   0.551574 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   0.557318 |\n",
            "| batch: 303.000000 |\n",
            "| loss:   0.626518 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   0.419487 |\n",
            "| batch: 305.000000 |\n",
            "| loss:   0.619476 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   0.520778 |\n",
            "| batch: 307.000000 |\n",
            "| loss:   0.448335 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   0.553674 |\n",
            "| batch: 309.000000 |\n",
            "| loss:   0.540720 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   0.469878 |\n",
            "| batch: 311.000000 |\n",
            "| loss:   0.722426 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   0.541621 |\n",
            "| batch: 313.000000 |\n",
            "| loss:   0.489272 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   0.647583 |\n",
            "| batch: 315.000000 |\n",
            "| loss:   0.459632 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   0.441942 |\n",
            "| batch: 317.000000 |\n",
            "| loss:   0.572901 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   0.536346 |\n",
            "| batch: 319.000000 |\n",
            "| loss:   0.598767 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   0.529000 |\n",
            "| batch: 321.000000 |\n",
            "| loss:   0.472283 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   0.508738 |\n",
            "| batch: 323.000000 |\n",
            "| loss:   0.384796 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   0.482303 |\n",
            "| batch: 325.000000 |\n",
            "| loss:   0.487437 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   0.581665 |\n",
            "| batch: 327.000000 |\n",
            "| loss:   0.480202 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   0.450023 |\n",
            "| batch: 329.000000 |\n",
            "| loss:   1.003685 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   0.614255 |\n",
            "| batch: 331.000000 |\n",
            "| loss:   0.423157 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   0.652121 |\n",
            "| batch: 333.000000 |\n",
            "| loss:   0.450534 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   0.659682 |\n",
            "| batch: 335.000000 |\n",
            "| loss:   0.739926 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   0.414428 |\n",
            "| batch: 337.000000 |\n",
            "| loss:   0.466857 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   0.405868 |\n",
            "| batch: 339.000000 |\n",
            "| loss:   0.463242 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   0.534465 |\n",
            "| batch: 341.000000 |\n",
            "| loss:   0.518501 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   0.547638 |\n",
            "| batch: 343.000000 |\n",
            "| loss:   0.530986 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   0.465248 |\n",
            "| batch: 345.000000 |\n",
            "| loss:   0.489993 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   0.442990 |\n",
            "| batch: 347.000000 |\n",
            "| loss:   0.599521 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   0.468765 |\n",
            "| batch: 349.000000 |\n",
            "| loss:   0.487304 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   0.596549 |\n",
            "| batch: 351.000000 |\n",
            "| loss:   0.595884 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   0.588178 |\n",
            "| batch: 353.000000 |\n",
            "| loss:   0.583814 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   0.564908 |\n",
            "| batch: 355.000000 |\n",
            "| loss:   0.549335 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   0.559360 |\n",
            "| batch: 357.000000 |\n",
            "| loss:   0.669360 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   0.536869 |\n",
            "| batch: 359.000000 |\n",
            "| loss:   0.593753 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   0.482355 |\n",
            "| batch: 361.000000 |\n",
            "| loss:   0.686628 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   0.715297 |\n",
            "| batch: 363.000000 |\n",
            "| loss:   0.496327 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   0.781269 |\n",
            "| batch: 365.000000 |\n",
            "| loss:   0.568488 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   0.561794 |\n",
            "| batch: 367.000000 |\n",
            "| loss:   0.473390 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   0.499886 |\n",
            "| batch: 369.000000 |\n",
            "| loss:   0.425141 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   0.480173 |\n",
            "| batch: 371.000000 |\n",
            "| loss:   0.502923 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   0.616079 |\n",
            "| batch: 373.000000 |\n",
            "| loss:   0.442155 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   0.449533 |\n",
            "| batch: 375.000000 |\n",
            "| loss:   0.671273 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   0.444519 |\n",
            "| batch: 377.000000 |\n",
            "| loss:   0.689223 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   0.585812 |\n",
            "| batch: 379.000000 |\n",
            "| loss:   0.851673 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   0.448462 |\n",
            "| batch: 381.000000 |\n",
            "| loss:   0.544606 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   0.445438 |\n",
            "| batch: 383.000000 |\n",
            "| loss:   0.511880 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   0.593805 |\n",
            "| batch: 385.000000 |\n",
            "| loss:   0.388761 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   0.766763 |\n",
            "| batch: 387.000000 |\n",
            "| loss:   0.578068 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   0.511698 |\n",
            "| batch: 389.000000 |\n",
            "| loss:   0.577016 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   0.627110 |\n",
            "| batch: 391.000000 |\n",
            "| loss:   0.572206 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   0.485998 |\n",
            "| batch: 393.000000 |\n",
            "| loss:   0.539074 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   0.502533 |\n",
            "| batch: 395.000000 |\n",
            "| loss:   0.590805 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   0.535684 |\n",
            "| batch: 397.000000 |\n",
            "| loss:   0.461474 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   0.623985 |\n",
            "| batch: 399.000000 |\n",
            "| loss:   0.513758 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   0.536001 |\n",
            "| batch: 401.000000 |\n",
            "| loss:   0.473834 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   0.805840 |\n",
            "| batch: 403.000000 |\n",
            "| loss:   0.604293 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   0.616300 |\n",
            "| batch: 405.000000 |\n",
            "| loss:   0.574078 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   0.586124 |\n",
            "| batch: 407.000000 |\n",
            "| loss:   0.600916 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   0.586919 |\n",
            "| batch: 409.000000 |\n",
            "| loss:   0.576231 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   0.630519 |\n",
            "| batch: 411.000000 |\n",
            "| loss:   0.410731 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   0.595865 |\n",
            "| batch: 413.000000 |\n",
            "| loss:   0.747039 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   0.718864 |\n",
            "| batch: 415.000000 |\n",
            "| loss:   0.661707 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   0.629921 |\n",
            "| batch: 417.000000 |\n",
            "| loss:   0.753078 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   0.557187 |\n",
            "| batch: 419.000000 |\n",
            "| loss:   0.671536 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   0.685955 |\n",
            "| batch: 421.000000 |\n",
            "| loss:   0.737975 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   0.609019 |\n",
            "| batch: 423.000000 |\n",
            "| loss:   0.505788 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   0.526040 |\n",
            "| batch: 425.000000 |\n",
            "| loss:   0.479948 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   0.437543 |\n",
            "| batch: 427.000000 |\n",
            "| loss:   0.472032 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   0.576364 |\n",
            "| batch: 429.000000 |\n",
            "| loss:   0.497050 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   0.459114 |\n",
            "| batch: 431.000000 |\n",
            "| loss:   0.453813 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   0.546757 |\n",
            "| batch: 433.000000 |\n",
            "| loss:   0.446664 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   0.530779 |\n",
            "| batch: 435.000000 |\n",
            "| loss:   0.546261 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   0.945490 |\n",
            "| batch: 437.000000 |\n",
            "| loss:   0.684789 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   0.546561 |\n",
            "| batch: 439.000000 |\n",
            "| loss:   0.540659 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   0.921948 |\n",
            "| batch: 441.000000 |\n",
            "| loss:   0.580018 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   0.651445 |\n",
            "| batch: 443.000000 |\n",
            "| loss:   0.449071 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   0.665292 |\n",
            "| batch: 445.000000 |\n",
            "| loss:   0.478702 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   0.533002 |\n",
            "| batch: 447.000000 |\n",
            "| loss:   0.619946 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   0.559653 |\n",
            "| batch: 449.000000 |\n",
            "| loss:   0.691447 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   0.507313 |\n",
            "| batch: 451.000000 |\n",
            "| loss:   0.531661 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   0.704192 |\n",
            "| batch: 453.000000 |\n",
            "| loss:   0.442951 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   0.543155 |\n",
            "| batch: 455.000000 |\n",
            "| loss:   0.534591 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   0.660534 |\n",
            "| batch: 457.000000 |\n",
            "| loss:   0.582969 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   0.758908 |\n",
            "| batch: 459.000000 |\n",
            "| loss:   0.597864 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   0.591831 |\n",
            "| batch: 461.000000 |\n",
            "| loss:   0.583557 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   0.664714 |\n",
            "| batch: 463.000000 |\n",
            "| loss:   0.567886 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   0.607245 |\n",
            "| batch: 465.000000 |\n",
            "| loss:   0.500433 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   0.529645 |\n",
            "| batch: 467.000000 |\n",
            "| loss:   0.633689 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   0.647980 |\n",
            "| batch: 469.000000 |\n",
            "| loss:   0.539768 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   0.582474 |\n",
            "| batch: 471.000000 |\n",
            "| loss:   0.577619 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   0.547567 |\n",
            "| batch: 473.000000 |\n",
            "| loss:   0.686722 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   0.554869 |\n",
            "| batch: 475.000000 |\n",
            "| loss:   0.953388 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   0.770711 |\n",
            "| batch: 477.000000 |\n",
            "| loss:   0.440397 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   0.648585 |\n",
            "| batch: 479.000000 |\n",
            "| loss:   0.787539 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   0.586597 |\n",
            "| batch: 481.000000 |\n",
            "| loss:   0.651498 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   0.579186 |\n",
            "| batch: 483.000000 |\n",
            "| loss:   0.576419 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   0.517520 |\n",
            "| batch: 485.000000 |\n",
            "| loss:   0.554182 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   0.475433 |\n",
            "| batch: 487.000000 |\n",
            "| loss:   0.648628 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   0.651215 |\n",
            "| batch: 489.000000 |\n",
            "| loss:   0.492698 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   0.549027 |\n",
            "| batch: 491.000000 |\n",
            "| loss:   0.557242 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   0.426466 |\n",
            "| batch: 493.000000 |\n",
            "| loss:   0.458367 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   0.474641 |\n",
            "| batch: 495.000000 |\n",
            "| loss:   0.544378 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   0.634590 |\n",
            "| batch: 497.000000 |\n",
            "| loss:   0.408141 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   0.647653 |\n",
            "| batch: 499.000000 |\n",
            "| loss:   0.479766 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   0.430349 |\n",
            "| batch: 501.000000 |\n",
            "| loss:   0.471452 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   0.491299 |\n",
            "| batch: 503.000000 |\n",
            "| loss:   0.581471 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   0.472588 |\n",
            "| batch: 505.000000 |\n",
            "| loss:   0.616488 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   0.541116 |\n",
            "| batch: 507.000000 |\n",
            "| loss:   0.753885 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   0.731005 |\n",
            "| batch: 509.000000 |\n",
            "| loss:   0.572735 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   0.641438 |\n",
            "| batch: 511.000000 |\n",
            "| loss:   0.541395 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   0.711519 |\n",
            "| batch: 513.000000 |\n",
            "| loss:   0.642198 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   0.480643 |\n",
            "| batch: 515.000000 |\n",
            "| loss:   0.494627 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   0.509765 |\n",
            "| batch: 517.000000 |\n",
            "| loss:   0.688898 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   0.482617 |\n",
            "| batch: 519.000000 |\n",
            "| loss:   0.581332 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   0.545970 |\n",
            "| batch: 521.000000 |\n",
            "| loss:   0.532916 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   0.445834 |\n",
            "| batch: 523.000000 |\n",
            "| loss:   0.570310 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   0.448496 |\n",
            "| batch: 525.000000 |\n",
            "| loss:   0.517771 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   0.461000 |\n",
            "| batch: 527.000000 |\n",
            "| loss:   0.466159 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   0.535484 |\n",
            "| batch: 529.000000 |\n",
            "| loss:   0.401004 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   0.431185 |\n",
            "| batch: 531.000000 |\n",
            "| loss:   0.638497 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   0.655723 |\n",
            "| batch: 533.000000 |\n",
            "| loss:   0.631059 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   0.461505 |\n",
            "| batch: 535.000000 |\n",
            "| loss:   0.562454 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   0.749767 |\n",
            "| batch: 537.000000 |\n",
            "| loss:   0.535244 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   0.504866 |\n",
            "| batch: 539.000000 |\n",
            "| loss:   0.578537 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   0.497444 |\n",
            "| batch: 541.000000 |\n",
            "| loss:   0.412323 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   0.525832 |\n",
            "| batch: 543.000000 |\n",
            "| loss:   0.435672 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   0.660859 |\n",
            "| batch: 545.000000 |\n",
            "| loss:   0.446909 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   0.600195 |\n",
            "| batch: 547.000000 |\n",
            "| loss:   0.560365 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   0.595202 |\n",
            "| batch: 549.000000 |\n",
            "| loss:   0.729913 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   0.574649 |\n",
            "| batch: 551.000000 |\n",
            "| loss:   0.800975 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   0.568522 |\n",
            "| batch: 553.000000 |\n",
            "| loss:   0.527381 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   0.810092 |\n",
            "| batch: 555.000000 |\n",
            "| loss:   0.546528 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   0.576321 |\n",
            "| batch: 557.000000 |\n",
            "| loss:   0.592336 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   0.554457 |\n",
            "| batch: 559.000000 |\n",
            "| loss:   0.481421 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   0.488246 |\n",
            "| batch: 561.000000 |\n",
            "| loss:   0.538001 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   0.531406 |\n",
            "| batch: 563.000000 |\n",
            "| loss:   0.766399 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   0.562558 |\n",
            "| batch: 565.000000 |\n",
            "| loss:   0.583337 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   0.638716 |\n",
            "| batch: 567.000000 |\n",
            "| loss:   0.530619 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   0.656389 |\n",
            "| batch: 569.000000 |\n",
            "| loss:   0.685366 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   0.590407 |\n",
            "| batch: 571.000000 |\n",
            "| loss:   0.494594 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   0.694148 |\n",
            "| batch: 573.000000 |\n",
            "| loss:   0.635619 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   0.560964 |\n",
            "| batch: 575.000000 |\n",
            "| loss:   0.645313 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   0.836625 |\n",
            "| batch: 577.000000 |\n",
            "| loss:   0.574297 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   0.611020 |\n",
            "| batch: 579.000000 |\n",
            "| loss:   1.168203 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   0.559775 |\n",
            "| batch: 581.000000 |\n",
            "| loss:   0.493221 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   0.516446 |\n",
            "| batch: 583.000000 |\n",
            "| loss:   0.887263 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   0.403017 |\n",
            "| batch: 585.000000 |\n",
            "| loss:   0.535963 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   0.542384 |\n",
            "| batch: 587.000000 |\n",
            "| loss:   0.535798 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   0.591967 |\n",
            "| batch: 589.000000 |\n",
            "| loss:   0.532865 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   0.518718 |\n",
            "| batch: 591.000000 |\n",
            "| loss:   0.478672 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   0.657563 |\n",
            "| batch: 593.000000 |\n",
            "| loss:   0.675224 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   0.520337 |\n",
            "| batch: 595.000000 |\n",
            "| loss:   0.556964 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   0.644847 |\n",
            "| batch: 597.000000 |\n",
            "| loss:   0.521240 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   0.553182 |\n",
            "| batch: 599.000000 |\n",
            "| loss:   0.658884 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   0.541021 |\n",
            "| batch: 601.000000 |\n",
            "| loss:   0.491237 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   0.680847 |\n",
            "| batch: 603.000000 |\n",
            "| loss:   0.929090 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   0.530544 |\n",
            "| batch: 605.000000 |\n",
            "| loss:   0.528896 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   0.642437 |\n",
            "| batch: 607.000000 |\n",
            "| loss:   0.527699 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   0.788584 |\n",
            "| batch: 609.000000 |\n",
            "| loss:   0.702860 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   0.511023 |\n",
            "| batch: 611.000000 |\n",
            "| loss:   0.498740 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   0.553976 |\n",
            "| batch: 613.000000 |\n",
            "| loss:   0.559032 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   0.855612 |\n",
            "| batch: 615.000000 |\n",
            "| loss:   0.631344 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   0.649329 |\n",
            "| batch: 617.000000 |\n",
            "| loss:   0.704875 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   0.758378 |\n",
            "Training stopped at Epoch 7\n",
            "This run of Question Generation -seq2seq attention ran for 2:10:28 and logs are available locally at: /root/.hyperdash/logs/question-generation-seq2seq-attention/question-generation-seq2seq-attention_2020-04-22t01-17-23-305982.log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHn9MnA6aOVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "create_test_df(model,final_dataloader,inpLang,optLang,save_path=model_path+\"output.csv\")\n",
        "if HYPERDASH:\n",
        "    exp.end()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qdaCn7EfgGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(len(sentence)/1.5,len(translation)/1.5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    cax = ax.matshow(attention[:len(translation),:len(sentence)+1], cmap='bone')\n",
        "   \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45)\n",
        "    ax.set_yticklabels(['']+translation)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCMJ4GljnOsg",
        "colab_type": "code",
        "outputId": "9f7c1c2b-c4a7-4f9d-d693-0617d22b90e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        }
      },
      "source": [
        "sent=random.choice(input_test)\n",
        "pred,attn=model.decode(sent,inpLang,optLang)\n",
        "display_attention(inpLang.tokenize(sent),pred.split(\" \"),attn)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2112x384 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "what did the term for the ? <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLv-U95M3y3c",
        "colab_type": "text"
      },
      "source": [
        "# Convolution Seq2seq "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mz4PUbdcIvhC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = INPUT_VOCAB\n",
        "OUTPUT_DIM = OUTPUT_VOCAB\n",
        "EMB_DIM = 300\n",
        "HID_DIM = 600 # each conv. layer has 2 * hid_dim filters\n",
        "ENC_LAYERS = 10 # number of conv. blocks in encoder\n",
        "DEC_LAYERS = 10 # number of conv. blocks in decoder\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "TRG_PAD_IDX = optLang.word2idx[optLang.special[\"pad_token\"]]\n",
        "HYPERDASH=True\n",
        "EPOCHS=6\n",
        "CLIP=0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_toJbaFZhuNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,inp_dim,emb_dim,hid_dim,n_layers,kernel_size,dropout,max_len,device):\n",
        "        super(Encoder,self).__init__()\n",
        "        assert kernel_size % 2 == 1\n",
        "        self.device=device\n",
        "        self.hid_dim=hid_dim\n",
        "        self.kernel_size=kernel_size\n",
        "        self.scale=torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        self.tok_embedding=nn.Embedding(inp_dim,emb_dim)\n",
        "        self.pos_embedding=nn.Embedding(max_len,emb_dim)\n",
        "        \n",
        "        self.emb2hid=nn.Linear(emb_dim,hid_dim)\n",
        "        self.hid2emb=nn.Linear(hid_dim,emb_dim)\n",
        "\n",
        "        self.conv_blocks=nn.ModuleList([nn.Conv1d(in_channels=hid_dim,\n",
        "                                    out_channels=2*hid_dim,\n",
        "                                    kernel_size=kernel_size,\n",
        "                                    padding=(kernel_size - 1) // 2)\n",
        "                            for i in range(n_layers)])\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,src):\n",
        "        batch_size=src.shape[0]\n",
        "        src_len=src.shape[1]\n",
        "        pos=torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) # (batch , seq_len)\n",
        "        tok_embedded=self.tok_embedding(src) #(batch, seq_len, emb_dim)\n",
        "        pos_embedded=self.pos_embedding(pos) #(batch, seq_len, emb_dim)\n",
        "        embedded=self.dropout(tok_embedded+pos_embedded)\n",
        "        hidden=self.emb2hid(embedded) #(batch, seq_len, hid_dim)\n",
        "        conv_input = hidden.permute(0, 2, 1) #(batch, hid_dim, seq_len)\n",
        "        for i,conv in enumerate(self.conv_blocks):\n",
        "            conved=conv(self.dropout(conv_input)) #(batch, 2 * hid_dim, src_len)\n",
        "            conved = F.glu(conved,dim=1) #(batch, hid_dim, src_len)\n",
        "            conved = (conved + conv_input) * self.scale #(batch, hid_dim, src_len)\n",
        "            conv_input = conved\n",
        "        conved=conved.permute(0, 2, 1) #(batch_size, src_len, hid_dim) \n",
        "        conved = self.hid2emb(conved) #(batch_size, src_len, emb_dim) \n",
        "        combined = (conved + embedded) * self.scale\n",
        "        return conved, combined"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzc2oivIqWa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_dim,emb_dim,hid_dim,n_layers,kernel_size,dropout,max_len,trg_pad_idx):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.device=device\n",
        "        self.output_dim=output_dim\n",
        "        self.hid_dim=hid_dim\n",
        "        self.kernel_size=kernel_size\n",
        "        self.trg_pad_idx=trg_pad_idx\n",
        "        self.scale=torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        self.tok_embedding=nn.Embedding(output_dim,emb_dim)\n",
        "        self.pos_embedding=nn.Embedding(max_len,emb_dim)\n",
        "\n",
        "        self.hid2emb=nn.Linear(hid_dim,emb_dim)\n",
        "        self.emb2hid=nn.Linear(emb_dim,hid_dim)\n",
        "\n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "\n",
        "        self.fc = nn.Linear(emb_dim, output_dim)\n",
        "\n",
        "        self.conv_blocks=nn.ModuleList([nn.Conv1d(in_channels=hid_dim,\n",
        "                                    out_channels=2*hid_dim,\n",
        "                                    kernel_size=kernel_size)\n",
        "                            for i in range(n_layers)])\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    \n",
        "    def calc_attn(self,embedded, conved, encoder_conved, encoder_combined):\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
        "        combined = (conved_emb + embedded) * self.scale\n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
        "        return attention, attended_combined\n",
        "\n",
        "    def forward(self,trg,enc_conved,enc_combined):\n",
        "        batch_size=trg.shape[0]\n",
        "        trg_len=trg.shape[1]\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device) #(batch, trg_len)\n",
        "\n",
        "        tok_embedded = self.tok_embedding(trg) #(batch, trg_len, emb_dim)\n",
        "        pos_embedded = self.pos_embedding(pos) #(batch, trg_len, emb_dim)\n",
        "        embedded=self.dropout(tok_embedded+ pos_embedded) #(batch, trg_len, emb_dim)\n",
        "        conv_input = self.emb2hid(embedded) #(batch, trg_len, hid_dim)\n",
        "        conv_input = conv_input.permute(0, 2, 1) #(batch, hid_dim, trg_len)\n",
        "        for i,conv in enumerate(self.conv_blocks):\n",
        "            conv_input = self.dropout(conv_input)\n",
        "            padding = torch.zeros(batch_size, \n",
        "                                  self.hid_dim, \n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2) # (batch, hid_dim, trg_len + kernel_size -1)\n",
        "            conved = conv(padded_conv_input) #(batch, 2*hid_dim , trg_len)\n",
        "            conved = F.glu(conved, dim = 1) #(batch, hid_dim, trg_len)\n",
        "            attention, conved = self.calc_attn(embedded, \n",
        "                                                conved, \n",
        "                                                enc_conved, \n",
        "                                                enc_combined) #(batch, trg_len, src_len),  (batch, hid_dim, trg_len)\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "            conv_input = conved\n",
        "        conved=conved.permute(0, 2, 1) #(batch, trg_len, hid_dim)\n",
        "        conved = self.hid2emb(conved) #(batch, trg_len, emb_dim)\n",
        "        output = self.fc(self.dropout(conved)) #(batch, trg_len, ouput_dim)\n",
        "        return output, attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZrWrfRjty6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConvSeq2seq(nn.Module):\n",
        "    def __init__(self, inp_dim, opt_dim, emb_dim, hid_dim, enc_layers, \n",
        "                 dec_layers, enc_kernel_size, dec_kernel_size, enc_dropout, \n",
        "                 dec_dropout, max_len, device, trg_pad_idx):\n",
        "        super(ConvSeq2seq,self).__init__()\n",
        "        self.max_len=max_len\n",
        "        self.device=device\n",
        "        self.encoder=Encoder(inp_dim,emb_dim,hid_dim,enc_layers,enc_kernel_size,enc_dropout,max_len,device)\n",
        "        self.decoder=Decoder(opt_dim,emb_dim,hid_dim,dec_layers,dec_kernel_size,dec_dropout, max_len,trg_pad_idx)\n",
        "    def forward(self,src,trg):\n",
        "        #src = [ src len, batch size]\n",
        "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
        "        encoder_conved, encoder_combined = self.encoder(src)\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
        "        return output, attention\n",
        "    def decode(self,input,inpLang,optLang):   \n",
        "        stop_token=optLang.word2idx[optLang.special[\"eos_token\"]]\n",
        "        start_token=optLang.word2idx[optLang.special[\"init_token\"]]\n",
        "        src=torch.tensor(inpLang.encode(input),device=self.device).unsqueeze(1).transpose(0,1)\n",
        "        batch_size=src.shape[0]\n",
        "        trg_vocab_size=self.decoder.output_dim\n",
        "        enc_out,hidden = self.encoder(src)\n",
        "        outputs=[start_token]\n",
        "        enc_conved, enc_combined=self.encoder(src)\n",
        "        for i in range(self.max_len):\n",
        "            input=torch.tensor(outputs,device=self.device).unsqueeze(0)\n",
        "            output, attention = self.decoder(input, enc_conved, enc_combined)\n",
        "            pred_token = output.argmax(2)[:,-1].item()\n",
        "            outputs.append(pred_token)\n",
        "            if pred_token == stop_token:\n",
        "                break\n",
        "        outputs=\" \".join(optLang.decode(outputs))\n",
        "        return outputs, attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAa4usyNtzt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=ConvSeq2seq(INPUT_DIM, OUTPUT_DIM, EMB_DIM,HID_DIM,\n",
        "                  ENC_LAYERS, DEC_LAYERS, ENC_KERNEL_SIZE,\n",
        "                  DEC_KERNEL_SIZE, ENC_DROPOUT, DEC_DROPOUT,\n",
        "                  MAX_LEN, device, TRG_PAD_IDX).to(device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI5Cuh-rqWrN",
        "colab_type": "code",
        "outputId": "ce875470-c35e-4b47-e236-8a470299b4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "it=iter(test_dataloader)\n",
        "x,y,_=next(it)\n",
        "print(y[:,:-1])\n",
        "# model(x.to(device),y.to(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   1,  192,    7,  ...,    0,    0,    0],\n",
            "        [   1,    7,   26,  ...,    0,    0,    0],\n",
            "        [   1,   17,  172,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [   1,    8,    9,  ...,    0,    0,    0],\n",
            "        [   1,   12,   20,  ...,    0,    0,    0],\n",
            "        [   1,   25, 1280,  ...,    0,    0,    0]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY2ttSzJ0wt_",
        "colab_type": "code",
        "outputId": "e002c98c-526e-48e3-d8ad-28a7c0a409b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 53,376,700 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxnC4VJJ1v_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITqj1bz41xe8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,iterator,optimizer,criterion,clip,exp):\n",
        "    model.train()\n",
        "    epoch_loss=0\n",
        "    print(\"Total Batches - \",len(iterator))\n",
        "    for i,(x,y,x_l) in enumerate(iterator):\n",
        "        src=x.to(device)\n",
        "        trg=y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        trg=trg[:,1:]\n",
        "        trg=trg.transpose(0,1)\n",
        "        output_dim=output.shape[-1]\n",
        "        output=output.contiguous().view(-1,output_dim)\n",
        "        trg=trg.contiguous().view(-1)\n",
        "        loss=criterion(output,trg)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss+=loss.item()\n",
        "        if exp!=None:\n",
        "            exp.metric(\"batch\",i)\n",
        "            exp.metric(\"loss\",loss.item())\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YyZ7jTS3hsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model,iterator,criterion,exp):\n",
        "    model.eval()\n",
        "    epoch_loss=0\n",
        "    print(\"Total Batches - \",len(iterator))\n",
        "    with torch.no_grad():\n",
        "        for i,(x,y,x_l) in enumerate(iterator):\n",
        "            src=x.to(device)\n",
        "            trg=y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "            trg=trg[:,1:]\n",
        "            trg=trg.transpose(0,1)\n",
        "            output_dim=output.shape[-1]\n",
        "            output=output.contiguous().view(-1,output_dim)\n",
        "            trg=trg.contiguous().view(-1)\n",
        "            loss=criterion(output,trg)\n",
        "            epoch_loss+=loss.item()\n",
        "            if exp!=None:\n",
        "                exp.metric(\"val_batch\",i)\n",
        "                exp.metric(\"val_loss\",loss.item())\n",
        "    return epoch_loss/len(iterator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTZBIGWv3xJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfnM-IVH34xX",
        "colab_type": "code",
        "outputId": "2be5c88a-a07d-4c2a-cdba-5000e2a69ba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "try:\n",
        "    nmt_path=\"_translation\" if testNMT else \"\" \n",
        "    if not os.path.exists(f\"/content/drive/My Drive/ALDA Project/ConvSeq2seq{nmt_path}/\"):\n",
        "        os.mkdir(f\"/content/drive/My Drive/ALDA Project/ConvSeq2seq{nmt_path}/\")\n",
        "    current_run= os.walk(f\"/content/drive/My Drive/ALDA Project/ConvSeq2seq{nmt_path}/\")\n",
        "    folders=[int(i) for i in list(current_run)[0][1] if i.isdigit()]\n",
        "    cur_run=max(folders)+1 if folders!=[] else 0\n",
        "    model_path=f\"/content/drive/My Drive/ALDA Project/ConvSeq2seq{nmt_path}/{str(cur_run)}/\"\n",
        "    if not os.path.exists(model_path):\n",
        "        os.mkdir(model_path)\n",
        "    import math\n",
        "    import pandas\n",
        "    import pickle\n",
        "    if HYPERDASH==True:\n",
        "        from hyperdash import Experiment\n",
        "        exp = Experiment(\"Question Generation-ConvSeq2seq\")\n",
        "        log_hyperdash(exp,**get_params())\n",
        "    else:\n",
        "        exp=None\n",
        "    CLIP=0.1\n",
        "    best_valid_loss=float('inf')\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        if HYPERDASH:\n",
        "            exp.metric(\"epoch\",epoch)\n",
        "        st_time=time.time()\n",
        "        train_loss=train(model,train_dataloader,optimizer,criterion,CLIP,exp)\n",
        "        valid_loss=eval(model,test_dataloader,criterion,exp)\n",
        "        e_time=time.time()\n",
        "        epoch_mins, epoch_secs = epoch_time(st_time, e_time)\n",
        "        if valid_loss<best_valid_loss:\n",
        "            best_valid_loss=valid_loss\n",
        "            torch.save(model.state_dict(),model_path+\"model.pt\")\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "    with open(model_path+\"inpLang.p\",'wb') as f:\n",
        "        pickle.dump(inpLang,f)\n",
        "    with open(model_path+\"optLang.p\",'wb') as f:\n",
        "        pickle.dump(optLang,f)\n",
        "    log_param(model_path+\"param.txt\",**get_params())\n",
        "except Exception as e:\n",
        "    print(f\"Training stopped {e}\")\n",
        "finally:\n",
        "    if HYPERDASH:\n",
        "        exp.end()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{ BATCH_SIZE: 128 }\n",
            "{ DEVICE: GPU }\n",
            "{ testNMT: True }\n",
            "{ SAMPLE: False }\n",
            "{ MAX_LEN: 100 }\n",
            "{ INPUT_VOCAB: 10000 }\n",
            "{ OUTPUT_VOCAB: 10000 }\n",
            "{ USE_PRETRAINED: False }\n",
            "{ INPUT_DIM: 10000 }\n",
            "{ OUTPUT_DIM: 10000 }\n",
            "{ ENC_EMB_DIM: None }\n",
            "{ DEC_HID_DIM: None }\n",
            "{ HID_DIM: 600 }\n",
            "{ ENC_DROPOUT: 0.25 }\n",
            "{ DEC_DROPOUT: 0.25 }\n",
            "{ N_LAYERS: None }\n",
            "{ LR: None }\n",
            "{ EPOCHS: 6 }\n",
            "{ HYPERDASH: True }\n",
            "{ CLIP: 0.1 }\n",
            "{ TOKENIZER: spacy }\n",
            "| epoch:   0.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   9.321305 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   8.833768 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   9.156312 |\n",
            "| batch:   6.000000 |\n",
            "| loss:  14.502038 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   7.940638 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   7.074958 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   7.623182 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   8.556947 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   7.511410 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   6.225251 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   7.891673 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   6.302212 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   6.517922 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   6.257035 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   6.727574 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   6.578529 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   6.415184 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   6.231853 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   6.327156 |\n",
            "| batch:  38.000000 |\n",
            "| loss:  13.302014 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   7.362460 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   6.116476 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   6.006164 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   5.953212 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   6.080295 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   5.937277 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   6.425776 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   7.213143 |\n",
            "| batch:  56.000000 |\n",
            "| loss:  12.753413 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   7.297827 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   6.175437 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   5.952997 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   6.026712 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   6.157630 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   6.193657 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   5.908314 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   6.054881 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   5.920168 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   7.305035 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   6.580988 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   6.192148 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   6.074443 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   5.902069 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   6.033083 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   5.905130 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   5.895275 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   5.798356 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   5.921915 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   5.817155 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   5.938296 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   6.204387 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   5.996404 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   5.852951 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   5.923953 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   6.007813 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   5.774415 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   5.952758 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   6.000957 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   5.801611 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   5.861797 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   6.003390 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   6.066807 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   5.951921 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   5.993402 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   6.013716 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   5.889422 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   5.832195 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   5.822803 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   6.130623 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   6.018477 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   6.060536 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   5.870646 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   5.872856 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   5.831300 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   5.750821 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   5.910366 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   6.110700 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   5.827515 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   5.864192 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   5.991969 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   5.942979 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   5.726636 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   6.125378 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   6.205336 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   6.182148 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   6.035398 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   6.051649 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   5.851468 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   5.751247 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   5.771042 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   5.883617 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   5.887275 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   5.941522 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   5.743414 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   5.931467 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   5.826954 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   5.822522 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   5.870421 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   5.819495 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   5.756629 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   5.853703 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   5.732450 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   5.792670 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   5.862136 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   5.989367 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   5.963390 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   5.819042 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   5.827188 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   5.766156 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   5.801264 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   5.788154 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   5.811908 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   5.859844 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   5.823205 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   5.962749 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   6.007776 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   5.759227 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   6.076375 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   6.070292 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   5.930236 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   5.791510 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   5.991846 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   6.051796 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   5.726387 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   5.829216 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   5.805914 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   5.771347 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   5.979288 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   5.803195 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   7.085823 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   5.867900 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   5.771693 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   5.930324 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   5.960705 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   5.728840 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   5.834660 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   5.761955 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   5.999300 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   5.698893 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   5.947396 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   5.806475 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   5.886573 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   5.782155 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   5.844288 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   5.875790 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   5.819603 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   5.879263 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   5.776344 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   5.787331 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   5.758861 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   5.772957 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   5.829183 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   6.004821 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   5.849225 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   5.739291 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   5.790836 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   5.777821 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   5.824693 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   5.800867 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   5.836900 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   5.891154 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   5.838200 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   5.910495 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   5.870960 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   5.817443 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   5.947231 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   5.657428 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   5.774694 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   5.829571 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   5.921850 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   5.980157 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   5.925435 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   5.775311 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   5.808547 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   5.819137 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   5.771980 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   5.888212 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   5.986454 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   6.061930 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   5.758193 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   6.035122 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   5.848933 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   5.807401 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   5.942535 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   5.774060 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   5.689540 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   5.844935 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   5.769262 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   5.749813 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   5.856167 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   5.761402 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   5.846586 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   5.776748 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   5.774259 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   5.763211 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   5.872077 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   5.906604 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   5.779724 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   5.684050 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   5.757796 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   5.710239 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   5.745426 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   5.757279 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   5.821587 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   5.743468 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   5.859916 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   5.726189 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   5.749568 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   5.881882 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   5.760641 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   5.862161 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   5.796788 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   5.814550 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   5.859581 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   5.818407 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   5.714999 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   5.755902 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   5.691153 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   5.892056 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   5.864418 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   6.022737 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   5.767514 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   5.819289 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   5.845882 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   5.784602 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   5.790273 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   5.761730 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   5.755946 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   5.874859 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   5.656902 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   5.739201 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   5.792923 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   5.713698 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   5.719223 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   5.898906 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   5.782311 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   5.664863 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   5.887868 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   5.748511 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   5.696194 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   5.725043 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   5.778757 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   5.782049 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   5.701332 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   5.853692 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   5.829382 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   6.028142 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   5.883390 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   5.717417 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   5.811088 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   5.784014 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   5.891335 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   5.708076 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   5.815405 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   5.775113 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   5.812072 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   5.779026 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   5.703962 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   5.810513 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   5.771161 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   5.807820 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   5.740551 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   5.690418 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   5.771567 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   5.881291 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   5.795730 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   5.859232 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   5.831225 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   5.871041 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   5.775166 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   5.844391 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   5.820633 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   5.661201 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   5.822463 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   5.868715 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   5.830390 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   5.662222 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   5.779046 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   5.727564 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   5.742144 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   5.722316 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   5.816039 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   5.871329 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   5.846795 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   5.725198 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   5.666212 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   5.673158 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   5.722523 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   5.774632 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   5.653823 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   5.832522 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   5.787782 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   5.707909 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   5.799676 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   5.789923 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   5.896219 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   5.779696 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   5.778780 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   5.774435 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   5.701940 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   5.691475 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   5.637377 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   5.789347 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   5.851822 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   5.688281 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   5.848260 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   5.680591 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   5.883098 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   5.664508 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   5.817531 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   5.799003 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   5.769641 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   5.773147 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   5.806405 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   5.758890 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   5.744942 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   5.788463 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   5.762687 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   5.824929 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   5.723626 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   5.755991 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   5.867838 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   5.821775 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   5.814939 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   5.815446 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   5.691664 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   5.925525 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   5.732264 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   5.669407 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   5.797137 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   5.788444 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   5.702955 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   5.742321 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   5.681185 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   5.802486 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   5.762434 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   5.773542 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   5.725681 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   5.673692 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   5.703793 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   5.713138 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   5.702252 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   5.656925 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   5.834981 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   5.735333 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   5.754936 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   5.803675 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   5.879734 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   5.768450 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   5.763780 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   5.714258 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   5.717731 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   5.724284 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   5.726487 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   5.751805 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   5.784874 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   5.756889 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   5.787817 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   5.721310 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   5.803782 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   5.701820 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   5.656999 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   5.742534 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   5.871160 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   5.715883 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   5.909935 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   5.651167 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   5.827352 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   5.682611 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   5.798133 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   5.739825 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   5.764270 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   5.748972 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   5.856106 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   5.845239 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   5.809463 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   5.681170 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   5.818104 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   5.765799 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   5.804741 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   5.940471 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   5.793853 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   5.758309 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   5.798359 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   5.863830 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   5.779267 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   5.715075 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   5.772823 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   5.705856 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   5.674917 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   5.781898 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   5.761892 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   5.599541 |\n",
            "| val_batch:   5.000000 |\n",
            "| val_loss:   5.714520 |\n",
            "| val_batch:  10.000000 |\n",
            "| val_loss:   5.641617 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   5.580828 |\n",
            "| val_batch:  20.000000 |\n",
            "| val_loss:   5.560865 |\n",
            "| val_batch:  25.000000 |\n",
            "| val_loss:   5.711654 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   5.680884 |\n",
            "| val_batch:  35.000000 |\n",
            "| val_loss:   5.586897 |\n",
            "| val_batch:  40.000000 |\n",
            "| val_loss:   5.590502 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   5.611343 |\n",
            "| val_batch:  50.000000 |\n",
            "| val_loss:   5.684686 |\n",
            "| val_batch:  55.000000 |\n",
            "| val_loss:   5.700348 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   5.575136 |\n",
            "| val_batch:  65.000000 |\n",
            "| val_loss:   5.671761 |\n",
            "| val_batch:  70.000000 |\n",
            "| val_loss:   5.585826 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   5.672555 |\n",
            "| val_batch:  80.000000 |\n",
            "| val_loss:   5.620677 |\n",
            "| val_batch:  85.000000 |\n",
            "| val_loss:   5.669804 |\n",
            "Epoch: 01 | Time: 8m 58s\n",
            "\tTrain Loss: 5.957 | Train PPL: 386.401\n",
            "\t Val. Loss: 5.636 |  Val. PPL: 280.450\n",
            "| epoch:   1.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   5.658408 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   5.788382 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   5.755613 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   5.991663 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   5.884627 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   5.930387 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   5.913888 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   5.792146 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   5.827546 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   5.797274 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   5.725907 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   5.667922 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   5.719576 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   5.767755 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   5.762637 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   5.716535 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   5.686958 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   5.706387 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   5.712187 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   5.748406 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   5.734859 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   5.662992 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   5.632387 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   5.767877 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   5.737778 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   5.739573 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   5.728389 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   5.768026 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   5.705655 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   5.757043 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   5.763410 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   5.721107 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   5.697320 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   5.744323 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   5.734537 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   5.740533 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   5.724536 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   5.772019 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   5.682655 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   5.750746 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   5.718029 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   5.767395 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   5.751846 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   5.797149 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   5.645374 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   5.873013 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   5.901284 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   5.678998 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   5.740245 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   5.690116 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   5.742292 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   5.677217 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   5.680717 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   5.835850 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   5.753208 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   5.729131 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   5.668268 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   5.635839 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   5.673113 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   5.653581 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   5.674055 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   5.716244 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   5.740390 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   5.637017 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   5.743043 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   5.722207 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   5.660543 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   5.675599 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   5.699668 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   5.723372 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   5.772144 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   5.610836 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   5.833613 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   5.758826 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   5.690568 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   5.837626 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   5.638175 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   5.853487 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   5.856492 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   5.776558 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   5.778787 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   5.705275 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   5.791703 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   5.664142 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   5.727321 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   5.561272 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   5.694277 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   5.736841 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   5.697117 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   5.747228 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   5.651895 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   5.704526 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   5.727156 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   5.774595 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   5.724667 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   5.728767 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   5.645438 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   5.677327 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   5.727718 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   5.674685 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   5.629585 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   5.725626 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   5.815066 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   5.661858 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   5.651476 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   5.706788 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   5.687106 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   5.735189 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   5.730253 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   5.763879 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   5.770602 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   5.737491 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   5.721822 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   5.629753 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   5.801476 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   5.845973 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   5.771328 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   5.754381 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   5.650208 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   5.735127 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   5.792888 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   5.716887 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   5.668636 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   5.691318 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   5.820355 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   5.792337 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   5.823469 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   5.775361 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   5.692720 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   5.781377 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   5.759266 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   5.685685 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   5.696914 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   5.743232 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   5.765132 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   5.695333 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   5.728191 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   5.758403 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   5.704201 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   5.719799 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   5.726937 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   5.726161 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   5.736198 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   5.722303 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   5.751392 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   5.713490 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   5.752182 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   5.698466 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   5.694920 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   5.795660 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   5.780225 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   5.768620 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   5.742793 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   5.723456 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   5.782447 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   5.725607 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   5.648567 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   5.700823 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   5.578979 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   5.692592 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   5.731044 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   5.613792 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   5.622735 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   5.729764 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   5.655200 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   5.751567 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   5.721050 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   5.825272 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   5.628562 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   5.678425 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   5.781508 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   5.759622 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   5.840647 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   5.661910 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   5.656950 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   5.744250 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   5.753531 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   5.834328 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   5.662269 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   5.771884 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   5.685627 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   5.775484 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   5.904779 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   5.694814 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   5.609749 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   5.667345 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   5.747707 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   5.733143 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   5.813321 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   5.690117 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   5.777415 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   5.740530 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   5.802842 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   5.708481 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   5.639164 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   5.615232 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   5.743604 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   5.720253 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   5.700825 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   5.732112 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   5.669214 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   5.609289 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   5.739692 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   5.701030 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   5.677710 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   5.627894 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   5.745865 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   5.656913 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   5.621812 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   5.654383 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   5.733513 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   5.584232 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   5.723208 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   5.704767 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   5.719423 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   5.747041 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   5.779286 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   5.674499 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   5.805099 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   5.714456 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   5.762326 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   5.784740 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   5.729759 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   5.723352 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   5.766372 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   5.707140 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   5.708197 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   5.630521 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   5.756882 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   5.755145 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   5.837692 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   5.630915 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   5.757505 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   5.729711 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   5.620917 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   5.642363 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   5.702419 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   5.716403 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   5.729944 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   5.736322 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   5.753849 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   5.658390 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   5.750036 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   5.737649 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   5.735063 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   5.677567 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   5.698031 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   5.744029 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   5.772972 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   5.657220 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   5.765053 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   5.698154 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   5.643842 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   5.727606 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   5.794415 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   5.816841 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   5.734321 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   5.687880 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   5.687414 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   5.786312 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   5.748486 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   5.827721 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   5.690687 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   5.747972 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   5.702269 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   5.666381 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   5.737341 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   5.734984 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   5.717700 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   5.587501 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   5.783344 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   5.749332 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   5.762189 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   5.827216 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   5.733104 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   5.738211 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   5.796141 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   5.691443 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   5.833067 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   5.740218 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   5.736630 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   5.759846 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   5.791707 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   5.708240 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   5.625573 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   5.614029 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   5.737397 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   5.746506 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   5.722336 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   5.746415 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   5.707130 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   5.624109 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   5.735320 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   5.668761 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   5.787550 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   5.794309 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   5.653983 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   5.722584 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   5.744401 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   5.675203 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   5.770792 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   5.670524 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   5.780843 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   5.617768 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   5.748393 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   5.674370 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   5.783954 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   5.731028 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   5.697046 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   5.596614 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   5.651691 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   5.694803 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   5.804145 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   5.774204 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   5.727534 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   5.628365 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   5.679231 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   5.649920 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   5.628094 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   5.660930 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   5.728160 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   5.776611 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   5.819901 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   5.676863 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   5.611654 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   5.722951 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   5.757203 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   5.741752 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   5.762406 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   5.785363 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   5.737353 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   5.718602 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   5.681339 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   5.762180 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   5.675930 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   5.687752 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   5.628701 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   5.701343 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   5.695826 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   5.687163 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   5.685526 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   5.696718 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   5.819473 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   5.774293 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   5.790382 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   5.670702 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   5.660022 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   5.737889 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   5.852078 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   5.667851 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   5.717340 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   5.595497 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   5.774585 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   5.835389 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   5.697140 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   5.680846 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   5.651894 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   5.728263 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   5.831588 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   5.751970 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   5.727772 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   5.846552 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   5.567788 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   5.727016 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   5.642200 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   5.799508 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   5.831373 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   5.687862 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   5.745752 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   5.802546 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   5.664538 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   5.719490 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   5.764418 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   5.807781 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   5.778503 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   5.792270 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   5.637290 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   5.770448 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   5.765045 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   5.804374 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   5.714454 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   5.687527 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   5.770342 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   5.754684 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   5.723099 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   5.601316 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   5.848142 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   5.625480 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   5.806643 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   5.802756 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   5.661559 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   5.705722 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   5.612738 |\n",
            "| val_batch:   5.000000 |\n",
            "| val_loss:   5.600931 |\n",
            "| val_batch:  10.000000 |\n",
            "| val_loss:   5.659193 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   5.635858 |\n",
            "| val_batch:  20.000000 |\n",
            "| val_loss:   5.620811 |\n",
            "| val_batch:  25.000000 |\n",
            "| val_loss:   5.623729 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   5.679681 |\n",
            "| val_batch:  35.000000 |\n",
            "| val_loss:   5.609297 |\n",
            "| val_batch:  40.000000 |\n",
            "| val_loss:   5.634489 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   5.686133 |\n",
            "| val_batch:  50.000000 |\n",
            "| val_loss:   5.680734 |\n",
            "| val_batch:  55.000000 |\n",
            "| val_loss:   5.633599 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   5.693100 |\n",
            "| val_batch:  65.000000 |\n",
            "| val_loss:   5.648417 |\n",
            "| val_batch:  70.000000 |\n",
            "| val_loss:   5.639806 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   5.639643 |\n",
            "| val_batch:  80.000000 |\n",
            "| val_loss:   5.648230 |\n",
            "| val_batch:  85.000000 |\n",
            "| val_loss:   5.720939 |\n",
            "Epoch: 02 | Time: 8m 59s\n",
            "\tTrain Loss: 5.729 | Train PPL: 307.553\n",
            "\t Val. Loss: 5.634 |  Val. PPL: 279.804\n",
            "| epoch:   2.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   5.697575 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   5.679622 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   5.724595 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   5.774322 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   5.655846 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   5.683894 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   5.652030 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   5.645000 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   5.692999 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   5.762444 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   5.622841 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   5.834751 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   5.653895 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   5.788292 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   5.649340 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   5.594267 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   5.604955 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   5.823383 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   5.759822 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   5.828482 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   5.706906 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   5.776613 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   5.743850 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   5.735874 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   5.728551 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   5.733748 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   5.729098 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   5.656404 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   5.714072 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   5.777350 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   5.632070 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   5.601521 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   5.728658 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   5.720886 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   5.807921 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   5.690363 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   5.776076 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   5.642745 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   5.742263 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   5.749213 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   5.833790 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   5.762778 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   5.692646 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   5.750612 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   5.742348 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   5.759519 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   5.704095 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   5.692801 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   5.716592 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   5.783620 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   5.793559 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   5.764966 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   5.697659 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   5.734467 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   5.648950 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   5.754776 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   5.624418 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   5.750718 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   5.647837 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   5.787889 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   5.723442 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   5.771584 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   5.690375 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   5.709062 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   5.680424 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   5.776539 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   5.650898 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   5.859180 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   5.683832 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   5.657832 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   5.727951 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   5.804443 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   5.780666 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   5.806307 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   5.704660 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   5.818672 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   5.668250 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   5.674327 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   5.702995 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   5.721333 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   5.739782 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   5.638579 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   5.769155 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   5.683887 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   5.732382 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   5.696769 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   5.676463 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   5.729767 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   5.783082 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   5.736355 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   5.757517 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   5.651106 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   5.557040 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   5.646317 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   5.682584 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   5.722846 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   5.721630 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   5.848519 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   5.783134 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   5.744981 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   5.758842 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   5.667139 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   5.711583 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   5.693856 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   5.717343 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   5.656995 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   5.778976 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   5.735861 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   5.620315 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   5.727370 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   5.643457 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   5.842853 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   5.738535 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   5.711823 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   5.651661 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   5.613060 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   5.691026 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   5.735853 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   5.639074 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   5.734004 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   5.745953 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   5.704364 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   5.591432 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   5.668770 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   5.741011 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   5.696789 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   5.701794 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   5.736721 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   5.797191 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   5.754464 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   5.674853 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   5.806060 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   5.762619 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   5.648882 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   5.735220 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   5.663118 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   5.759496 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   5.782730 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   5.719858 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   5.813630 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   5.699627 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   5.702271 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   5.779630 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   5.640476 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   5.720075 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   5.680377 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   5.666001 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   5.801410 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   5.636778 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   5.651702 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   5.790611 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   5.670262 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   5.705976 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   5.721766 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   5.713579 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   5.755258 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   5.792716 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   5.721153 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   5.750331 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   5.641586 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   5.600801 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   5.630857 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   5.509650 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   5.747468 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   5.724715 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   5.709509 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   5.761009 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   5.752569 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   5.794933 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   5.779468 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   5.740233 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   5.775633 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   5.744008 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   5.685060 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   5.743087 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   5.688439 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   5.786707 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   5.681790 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   5.727035 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   5.786952 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   5.683295 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   5.793267 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   5.712446 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   5.675790 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   5.646845 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   5.720790 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   5.677853 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   5.695400 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   5.725368 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   5.819426 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   5.668128 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   5.795910 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   5.636869 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   5.673226 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   5.710521 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   5.687860 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   5.672795 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   5.796301 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   5.688385 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   5.715917 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   5.719579 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   5.789900 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   5.625036 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   5.679859 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   5.694017 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   5.755793 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   5.779850 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   5.742232 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   5.691662 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   5.693457 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   5.799628 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   5.571265 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   5.730452 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   5.731907 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   5.680238 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   5.689325 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   5.763434 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   5.684981 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   5.725157 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   5.758130 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   5.736481 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   5.702364 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   5.753655 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   5.735059 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   5.669018 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   5.627189 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   5.704954 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   5.690584 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   5.733874 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   5.760540 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   5.693533 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   5.677404 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   5.715289 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   5.648337 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   5.809782 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   5.766498 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   5.673357 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   5.754246 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   5.801606 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   5.746482 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   5.738297 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   5.610778 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   5.726938 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   5.692633 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   5.670653 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   5.753634 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   5.699031 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   5.751096 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   5.588540 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   5.710166 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   5.753125 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   5.713441 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   5.549230 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   5.691215 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   5.690656 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   5.734285 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   5.766845 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   5.684866 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   5.730189 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   5.706943 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   5.730697 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   5.703668 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   5.662845 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   5.690241 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   5.803882 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   5.759246 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   5.826868 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   5.729819 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   5.705994 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   5.856902 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   5.869969 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   5.684620 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   5.711912 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   5.701963 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   5.553635 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   5.714571 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   5.628865 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   5.700231 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   5.626896 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   5.700408 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   5.656945 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   5.756075 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   5.554824 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   5.688107 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   5.634543 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   5.699183 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   5.664680 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   5.756910 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   5.711586 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   5.637745 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   5.745660 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   5.717890 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   5.650177 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   5.788968 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   5.742996 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   5.782400 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   5.724863 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   5.645697 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   5.689509 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   5.831557 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   5.706254 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   5.753421 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   5.710500 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   5.741735 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   5.757429 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   5.656940 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   5.797149 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   5.630174 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   5.684968 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   5.724215 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   5.767799 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   5.658064 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   5.767138 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   5.749933 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   5.726140 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   5.610121 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   5.701918 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   5.597041 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   5.785199 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   5.659917 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   5.629481 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   5.732296 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   5.680834 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   5.645422 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   5.662632 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   5.807083 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   5.746196 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   5.786518 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   5.794618 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   5.719518 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   5.689013 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   5.728263 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   5.857468 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   5.726367 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   5.739656 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   5.736725 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   5.760580 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   5.560767 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   5.774659 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   5.698072 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   5.754629 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   5.746130 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   5.703041 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   5.734842 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   5.753368 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   5.663872 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   5.650814 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   5.715044 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   5.736103 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   5.771570 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   5.708926 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   5.635612 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   5.717470 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   5.666970 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   5.691832 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   5.732281 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   5.767181 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   5.796824 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   5.725783 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   5.785274 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   5.797432 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   5.681141 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   5.661413 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   5.665378 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   5.763152 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   5.797561 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   5.776052 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   5.627722 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   5.764535 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   5.747617 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   5.660905 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   5.659225 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   5.770580 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   5.706812 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   5.705726 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   5.782704 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   5.735194 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   5.774484 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   5.781841 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   5.685013 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   5.765181 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   5.751567 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   5.729112 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   5.634558 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   5.734093 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   5.779271 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   5.695690 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   5.682508 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   5.695765 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   5.601885 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   5.772949 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   5.637681 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   5.607114 |\n",
            "| val_batch:   5.000000 |\n",
            "| val_loss:   5.681342 |\n",
            "| val_batch:  10.000000 |\n",
            "| val_loss:   5.622447 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   5.625502 |\n",
            "| val_batch:  20.000000 |\n",
            "| val_loss:   5.608755 |\n",
            "| val_batch:  25.000000 |\n",
            "| val_loss:   5.690156 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   5.618557 |\n",
            "| val_batch:  35.000000 |\n",
            "| val_loss:   5.613921 |\n",
            "| val_batch:  40.000000 |\n",
            "| val_loss:   5.571018 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   5.569407 |\n",
            "| val_batch:  50.000000 |\n",
            "| val_loss:   5.622468 |\n",
            "| val_batch:  55.000000 |\n",
            "| val_loss:   5.575238 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   5.584809 |\n",
            "| val_batch:  65.000000 |\n",
            "| val_loss:   5.575762 |\n",
            "| val_batch:  70.000000 |\n",
            "| val_loss:   5.570566 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   5.661107 |\n",
            "| val_batch:  80.000000 |\n",
            "| val_loss:   5.684563 |\n",
            "| val_batch:  85.000000 |\n",
            "| val_loss:   5.569521 |\n",
            "Epoch: 03 | Time: 8m 59s\n",
            "\tTrain Loss: 5.712 | Train PPL: 302.510\n",
            "\t Val. Loss: 5.624 |  Val. PPL: 277.006\n",
            "| epoch:   3.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   5.645493 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   5.757121 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   5.605476 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   5.593180 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   5.632604 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   5.718681 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   5.695519 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   5.674979 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   5.698753 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   5.737185 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   5.610034 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   5.687464 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   5.755051 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   5.631243 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   5.608193 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   5.737107 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   5.725560 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   5.745292 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   5.708537 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   5.759087 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   5.734712 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   5.741902 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   5.747027 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   5.661007 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   5.629370 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   5.682959 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   5.671671 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   5.749303 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   5.641132 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   5.626894 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   5.716918 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   5.733364 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   5.697552 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   5.636615 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   5.861191 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   5.585731 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   5.666881 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   5.639564 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   5.631254 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   5.518067 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   5.645663 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   5.807605 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   5.802881 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   5.701250 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   5.753723 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   5.734739 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   5.623590 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   5.772069 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   5.678965 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   5.815145 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   5.748306 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   5.633775 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   5.679103 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   5.646675 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   5.740026 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   5.675733 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   5.698423 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   5.725608 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   5.628926 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   5.721462 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   5.723512 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   5.679934 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   5.778680 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   5.691524 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   5.708320 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   5.922025 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   5.761045 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   5.625190 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   5.622691 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   5.803102 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   5.711809 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   5.690396 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   5.680948 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   5.716389 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   5.737009 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   5.663971 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   5.779038 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   5.730495 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   5.746323 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   5.707512 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   5.726824 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   5.649189 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   5.697884 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   5.807030 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   5.702104 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   5.765162 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   5.827522 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   5.713940 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   5.607836 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   5.732989 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   5.734998 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   5.618198 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   5.797153 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   5.736501 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   5.749429 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   5.766268 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   5.593130 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   5.688343 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   5.694525 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   5.736845 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   5.720665 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   5.728446 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   5.643810 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   5.629888 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   5.703547 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   5.759923 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   5.706615 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   5.805040 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   5.702881 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   5.595710 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   5.712461 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   5.595348 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   5.699163 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   5.731203 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   5.584459 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   5.683979 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   5.724161 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   5.636142 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   5.670285 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   5.741230 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   5.759749 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   5.629662 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   5.537866 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   5.831754 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   5.727048 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   5.709726 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   5.717206 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   5.668819 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   5.795051 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   5.677556 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   5.726724 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   5.692341 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   5.809154 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   5.785854 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   5.682741 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   5.730908 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   5.680377 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   5.769732 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   5.760359 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   5.705060 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   5.705887 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   5.810028 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   5.649746 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   5.671838 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   5.698802 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   5.719454 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   5.722482 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   5.712102 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   5.674959 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   5.594360 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   5.681294 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   5.722407 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   5.816909 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   5.730223 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   5.691607 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   5.774506 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   5.708235 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   5.706481 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   5.743701 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   5.850351 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   5.719759 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   5.694593 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   5.689905 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   5.727960 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   5.729020 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   5.775431 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   5.738056 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   5.672695 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   5.780303 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   5.698893 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   5.668684 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   5.636545 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   5.598778 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   5.714502 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   5.705736 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   5.676392 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   5.740421 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   5.799052 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   5.735244 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   5.676908 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   5.788490 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   5.628976 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   5.755623 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   5.696356 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   5.719020 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   5.747815 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   5.724793 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   5.752596 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   5.775008 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   5.591331 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   5.646389 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   5.694003 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   5.750506 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   5.793022 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   5.763052 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   5.810633 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   5.693377 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   5.605463 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   5.707697 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   5.759997 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   5.680562 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   5.733744 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   5.650793 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   5.744700 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   5.633415 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   5.724506 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   5.720726 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   5.715970 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   5.747776 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   5.680799 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   5.775360 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   5.706872 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   5.673137 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   5.819096 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   5.778435 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   5.711201 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   5.723787 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   5.722647 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   5.628577 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   5.577684 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   5.624831 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   5.686293 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   5.674661 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   5.645845 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   5.712154 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   5.641747 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   5.621647 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   5.698351 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   5.613010 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   5.671952 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   5.777691 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   5.762547 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   5.683349 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   5.657861 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   5.597888 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   5.681138 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   5.650387 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   5.816244 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   5.670116 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   5.707508 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   5.775920 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   5.833476 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   5.838514 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   5.763089 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   5.664240 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   5.691061 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   5.642694 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   5.812754 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   5.701580 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   5.696179 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   5.631618 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   5.761097 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   5.695715 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   5.770572 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   5.742684 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   5.701385 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   5.762882 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   5.635183 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   5.802387 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   5.695900 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   5.786072 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   5.679654 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   5.742878 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   5.762248 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   5.781100 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   5.694143 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   5.649101 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   5.705952 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   5.614811 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   5.747760 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   5.698408 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   5.733796 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   5.700554 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   5.604715 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   5.705097 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   5.739432 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   5.793573 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   5.705539 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   5.669876 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   5.811495 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   5.719526 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   5.715337 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   5.792516 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   5.794484 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   5.749475 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   5.643003 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   5.671647 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   5.700513 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   5.746192 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   5.731277 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   5.749846 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   5.598228 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   5.788732 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   5.710936 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   5.684872 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   5.720799 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   5.639478 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   5.761469 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   5.743706 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   5.708800 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   5.695125 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   5.761906 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   5.798569 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   5.639696 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   5.692784 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   5.695391 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   5.731989 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   5.719376 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   5.668240 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   5.759366 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   5.649389 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   5.631785 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   5.600300 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   5.685108 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   5.772280 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   5.738639 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   5.664686 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   5.642121 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   5.699533 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   5.685508 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   5.709492 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   5.740199 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   5.749403 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   5.792692 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   5.643574 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   5.737951 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   5.778469 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   5.800860 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   5.616168 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   5.736759 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   5.756711 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   5.624963 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   5.786909 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   5.670992 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   5.711480 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   5.779577 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   5.719059 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   5.600681 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   5.621231 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   5.610260 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   5.709155 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   5.719849 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   5.595439 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   5.734329 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   5.650711 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   5.677357 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   5.614995 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   5.714682 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   5.797844 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   5.684325 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   5.780741 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   5.724542 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   5.584402 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   5.787219 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   5.724755 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   5.728809 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   5.748427 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   5.712545 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   5.816249 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   5.834683 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   5.671581 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   5.751262 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   5.637852 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   5.806036 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   5.699714 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   5.764262 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   5.671124 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   5.748859 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   5.599090 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   5.721955 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   5.832439 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   5.726436 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   5.637226 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   5.589481 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   5.659775 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   5.704967 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   5.634155 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   5.745891 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   5.732528 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   5.687830 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   5.726411 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   5.775650 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   5.702263 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   5.771420 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   5.641159 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   5.728556 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   5.748216 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   5.766631 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   5.716961 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   5.722339 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   5.592709 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   5.689598 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   5.601572 |\n",
            "| val_batch:   5.000000 |\n",
            "| val_loss:   5.568596 |\n",
            "| val_batch:  10.000000 |\n",
            "| val_loss:   5.588375 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   5.686486 |\n",
            "| val_batch:  20.000000 |\n",
            "| val_loss:   5.559841 |\n",
            "| val_batch:  25.000000 |\n",
            "| val_loss:   5.632776 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   5.684013 |\n",
            "| val_batch:  35.000000 |\n",
            "| val_loss:   5.669973 |\n",
            "| val_batch:  40.000000 |\n",
            "| val_loss:   5.637525 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   5.654432 |\n",
            "| val_batch:  50.000000 |\n",
            "| val_loss:   5.653825 |\n",
            "| val_batch:  55.000000 |\n",
            "| val_loss:   5.639712 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   5.759598 |\n",
            "| val_batch:  65.000000 |\n",
            "| val_loss:   5.753765 |\n",
            "| val_batch:  70.000000 |\n",
            "| val_loss:   5.583190 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   5.584458 |\n",
            "| val_batch:  80.000000 |\n",
            "| val_loss:   5.616534 |\n",
            "| val_batch:  85.000000 |\n",
            "| val_loss:   5.599543 |\n",
            "Epoch: 04 | Time: 8m 59s\n",
            "\tTrain Loss: 5.709 | Train PPL: 301.489\n",
            "\t Val. Loss: 5.627 |  Val. PPL: 277.891\n",
            "| epoch:   4.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   5.807359 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   5.680962 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   5.735465 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   5.735775 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   5.679404 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   5.735265 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   5.701844 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   5.653663 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   5.622218 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   5.675012 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   5.692220 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   5.747139 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   5.656055 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   5.623281 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   5.811700 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   5.733619 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   5.626960 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   5.752442 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   5.798823 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   5.713440 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   5.714990 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   5.810955 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   5.725652 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   5.644553 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   5.639507 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   5.697931 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   5.748589 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   5.725152 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   5.682357 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   5.663486 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   5.646431 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   5.709259 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   5.688042 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   5.756073 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   5.745062 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   5.699541 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   5.714361 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   5.688016 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   5.728144 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   5.698020 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   5.715428 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   5.611802 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   5.700648 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   5.664405 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   5.734694 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   5.748083 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   5.742314 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   5.671045 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   5.806717 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   5.711580 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   5.706063 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   5.651803 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   5.737978 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   5.618825 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   5.788474 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   5.739388 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   5.714977 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   5.737663 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   5.670087 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   5.675596 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   5.678206 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   5.793541 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   5.761040 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   5.739052 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   5.664183 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   5.734134 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   5.731309 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   5.816288 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   5.688414 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   5.718943 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   5.624452 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   5.723940 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   5.658979 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   5.659381 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   5.733752 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   5.648478 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   5.688820 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   5.651253 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   5.726390 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   5.739287 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   5.736698 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   5.687639 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   5.721036 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   5.808518 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   5.653229 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   5.654761 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   5.685887 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   5.767158 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   5.593616 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   5.576678 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   5.772262 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   5.777267 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   5.631911 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   5.731057 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   5.711717 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   5.662239 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   5.624628 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   5.642879 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   5.666169 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   5.757170 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   5.793612 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   5.692737 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   5.623520 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   5.683998 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   5.764071 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   5.747982 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   5.743556 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   5.777404 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   5.615477 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   5.742491 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   5.769726 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   5.714755 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   5.692820 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   5.702009 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   5.818426 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   5.666534 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   5.759428 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   5.712423 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   5.731276 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   5.777153 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   5.694025 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   5.635311 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   5.717918 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   5.658195 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   5.761945 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   5.762417 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   5.841602 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   5.686233 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   5.661793 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   5.578639 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   5.671544 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   5.692572 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   5.616514 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   5.677972 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   5.661582 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   5.733386 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   5.610726 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   5.737956 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   5.702792 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   5.579166 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   5.694597 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   5.757226 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   5.694222 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   5.678529 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   5.698540 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   5.708851 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   5.676761 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   5.668136 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   5.721339 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   5.689138 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   5.729635 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   5.645068 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   5.638779 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   5.712245 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   5.689271 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   5.683365 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   5.725605 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   5.696842 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   5.768169 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   5.591928 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   5.765522 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   5.801765 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   5.691175 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   5.754734 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   5.756364 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   5.646962 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   5.773836 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   5.674190 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   5.680704 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   5.693482 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   5.696898 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   5.716145 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   5.735235 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   5.714551 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   5.696028 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   5.781195 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   5.680096 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   5.646189 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   5.838773 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   5.733444 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   5.747946 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   5.759583 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   5.783074 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   5.673146 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   5.742131 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   5.663224 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   5.751711 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   5.704743 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   5.741795 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   5.751820 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   5.643662 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   5.715625 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   5.678847 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   5.587923 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   5.731068 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   5.669440 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   5.760722 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   5.787449 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   5.680185 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   5.808830 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   5.764607 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   5.702566 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   5.836216 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   5.714566 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   5.658123 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   5.627288 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   5.599800 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   5.642772 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   5.822442 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   5.776040 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   5.708403 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   5.707883 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   5.726480 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   5.720222 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   5.767764 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   5.739165 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   5.644824 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   5.641533 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   5.705092 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   5.655951 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   5.787130 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   5.710900 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   5.770439 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   5.681570 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   5.716104 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   5.742727 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   5.753045 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   5.721836 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   5.724597 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   5.722074 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   5.753389 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   5.731744 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   5.591436 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   5.640094 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   5.653119 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   5.683780 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   5.610635 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   5.638947 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   5.771138 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   5.719397 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   5.637700 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   5.710858 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   5.688830 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   5.778794 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   5.733412 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   5.590157 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   5.757192 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   5.632985 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   5.787723 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   5.686263 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   5.717214 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   5.709905 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   5.691178 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   5.720784 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   5.628454 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   5.707686 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   5.763906 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   5.661236 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   5.817286 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   5.652172 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   5.799858 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   5.700315 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   5.728081 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   5.719665 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   5.733743 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   5.660692 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   5.793640 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   5.851629 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   5.740516 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   5.769664 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   5.791156 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   5.727241 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   5.756124 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   5.693304 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   5.755918 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   5.740488 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   5.658354 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   5.699911 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   5.672641 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   5.784632 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   5.584271 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   5.817861 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   5.586516 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   5.718834 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   5.781036 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   5.747301 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   5.708541 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   5.632216 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   5.673307 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   5.689343 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   5.783081 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   5.708105 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   5.565106 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   5.742348 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   5.714181 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   5.768498 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   5.783947 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   5.618010 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   5.739001 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   5.675504 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   5.790898 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   5.645183 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   5.786788 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   5.752676 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   5.731377 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   5.741294 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   5.736592 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   5.644577 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   5.755841 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   5.738358 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   5.664208 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   5.685497 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   5.637039 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   5.721868 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   5.686100 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   5.600221 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   5.688947 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   5.700695 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   5.729429 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   5.686925 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   5.669312 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   5.705153 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   5.786040 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   5.725442 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   5.701177 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   5.625323 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   5.759696 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   5.722284 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   5.763160 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   5.634651 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   5.586893 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   5.767056 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   5.721928 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   5.741219 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   5.620068 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   5.682167 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   5.737765 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   5.646754 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   5.722983 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   5.754710 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   5.683350 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   5.698572 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   5.656858 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   5.736958 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   5.679796 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   5.772479 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   5.710761 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   5.713335 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   5.763165 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   5.680886 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   5.800598 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   5.698649 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   5.830098 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   5.724114 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   5.646429 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   5.717454 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   5.769917 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   5.694995 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   5.665528 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   5.798620 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   5.819703 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   5.674326 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   5.692837 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   5.653160 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   5.718270 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   5.748737 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   5.708562 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   5.741337 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   5.785467 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   5.695020 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   5.777929 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   5.732729 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   5.787920 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   5.839922 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   5.806780 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   5.798328 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   5.626266 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   5.676068 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   5.669994 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   5.555285 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   5.722963 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   5.719091 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   5.802067 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   5.680784 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   5.658582 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   5.629765 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   5.688686 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   5.738195 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   5.644615 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   5.784770 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   5.625569 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   5.773759 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   5.676120 |\n",
            "| val_batch:   5.000000 |\n",
            "| val_loss:   5.581091 |\n",
            "| val_batch:  10.000000 |\n",
            "| val_loss:   5.638088 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   5.610476 |\n",
            "| val_batch:  20.000000 |\n",
            "| val_loss:   5.627530 |\n",
            "| val_batch:  25.000000 |\n",
            "| val_loss:   5.568529 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   5.575339 |\n",
            "| val_batch:  35.000000 |\n",
            "| val_loss:   5.718987 |\n",
            "| val_batch:  40.000000 |\n",
            "| val_loss:   5.627662 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   5.627329 |\n",
            "| val_batch:  50.000000 |\n",
            "| val_loss:   5.535779 |\n",
            "| val_batch:  55.000000 |\n",
            "| val_loss:   5.595096 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   5.692594 |\n",
            "| val_batch:  65.000000 |\n",
            "| val_loss:   5.660980 |\n",
            "| val_batch:  70.000000 |\n",
            "| val_loss:   5.579331 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   5.528059 |\n",
            "| val_batch:  80.000000 |\n",
            "| val_loss:   5.670823 |\n",
            "| val_batch:  85.000000 |\n",
            "| val_loss:   5.450863 |\n",
            "Epoch: 05 | Time: 8m 59s\n",
            "\tTrain Loss: 5.707 | Train PPL: 301.108\n",
            "\t Val. Loss: 5.623 |  Val. PPL: 276.642\n",
            "| epoch:   5.000000 |\n",
            "Total Batches -  783\n",
            "| batch:   0.000000 |\n",
            "| loss:   5.667582 |\n",
            "| batch:   2.000000 |\n",
            "| loss:   5.648247 |\n",
            "| batch:   4.000000 |\n",
            "| loss:   5.743462 |\n",
            "| batch:   6.000000 |\n",
            "| loss:   5.692322 |\n",
            "| batch:   8.000000 |\n",
            "| loss:   5.618281 |\n",
            "| batch:  10.000000 |\n",
            "| loss:   5.686652 |\n",
            "| batch:  12.000000 |\n",
            "| loss:   5.711118 |\n",
            "| batch:  14.000000 |\n",
            "| loss:   5.624662 |\n",
            "| batch:  16.000000 |\n",
            "| loss:   5.689831 |\n",
            "| batch:  18.000000 |\n",
            "| loss:   5.698855 |\n",
            "| batch:  20.000000 |\n",
            "| loss:   5.540161 |\n",
            "| batch:  22.000000 |\n",
            "| loss:   5.549154 |\n",
            "| batch:  24.000000 |\n",
            "| loss:   5.792439 |\n",
            "| batch:  26.000000 |\n",
            "| loss:   5.571295 |\n",
            "| batch:  28.000000 |\n",
            "| loss:   5.702034 |\n",
            "| batch:  30.000000 |\n",
            "| loss:   5.690733 |\n",
            "| batch:  32.000000 |\n",
            "| loss:   5.720173 |\n",
            "| batch:  34.000000 |\n",
            "| loss:   5.721071 |\n",
            "| batch:  36.000000 |\n",
            "| loss:   5.638579 |\n",
            "| batch:  38.000000 |\n",
            "| loss:   5.697585 |\n",
            "| batch:  40.000000 |\n",
            "| loss:   5.644522 |\n",
            "| batch:  42.000000 |\n",
            "| loss:   5.849556 |\n",
            "| batch:  44.000000 |\n",
            "| loss:   5.663601 |\n",
            "| batch:  46.000000 |\n",
            "| loss:   5.665915 |\n",
            "| batch:  48.000000 |\n",
            "| loss:   5.594171 |\n",
            "| batch:  50.000000 |\n",
            "| loss:   5.802279 |\n",
            "| batch:  52.000000 |\n",
            "| loss:   5.637410 |\n",
            "| batch:  54.000000 |\n",
            "| loss:   5.738673 |\n",
            "| batch:  56.000000 |\n",
            "| loss:   5.807767 |\n",
            "| batch:  58.000000 |\n",
            "| loss:   5.686956 |\n",
            "| batch:  60.000000 |\n",
            "| loss:   5.622929 |\n",
            "| batch:  62.000000 |\n",
            "| loss:   5.708193 |\n",
            "| batch:  64.000000 |\n",
            "| loss:   5.691917 |\n",
            "| batch:  66.000000 |\n",
            "| loss:   5.766180 |\n",
            "| batch:  68.000000 |\n",
            "| loss:   5.679742 |\n",
            "| batch:  70.000000 |\n",
            "| loss:   5.817025 |\n",
            "| batch:  72.000000 |\n",
            "| loss:   5.711470 |\n",
            "| batch:  74.000000 |\n",
            "| loss:   5.592335 |\n",
            "| batch:  76.000000 |\n",
            "| loss:   5.628354 |\n",
            "| batch:  78.000000 |\n",
            "| loss:   5.618124 |\n",
            "| batch:  80.000000 |\n",
            "| loss:   5.789630 |\n",
            "| batch:  82.000000 |\n",
            "| loss:   5.644951 |\n",
            "| batch:  84.000000 |\n",
            "| loss:   5.713184 |\n",
            "| batch:  86.000000 |\n",
            "| loss:   5.702551 |\n",
            "| batch:  88.000000 |\n",
            "| loss:   5.661451 |\n",
            "| batch:  90.000000 |\n",
            "| loss:   5.793264 |\n",
            "| batch:  92.000000 |\n",
            "| loss:   5.814282 |\n",
            "| batch:  94.000000 |\n",
            "| loss:   5.762391 |\n",
            "| batch:  96.000000 |\n",
            "| loss:   5.707211 |\n",
            "| batch:  98.000000 |\n",
            "| loss:   5.663021 |\n",
            "| batch: 100.000000 |\n",
            "| loss:   5.733643 |\n",
            "| batch: 102.000000 |\n",
            "| loss:   5.689954 |\n",
            "| batch: 104.000000 |\n",
            "| loss:   5.659670 |\n",
            "| batch: 106.000000 |\n",
            "| loss:   5.746597 |\n",
            "| batch: 108.000000 |\n",
            "| loss:   5.625047 |\n",
            "| batch: 110.000000 |\n",
            "| loss:   5.603240 |\n",
            "| batch: 112.000000 |\n",
            "| loss:   5.700243 |\n",
            "| batch: 114.000000 |\n",
            "| loss:   5.654166 |\n",
            "| batch: 116.000000 |\n",
            "| loss:   5.767020 |\n",
            "| batch: 118.000000 |\n",
            "| loss:   5.650284 |\n",
            "| batch: 120.000000 |\n",
            "| loss:   5.756500 |\n",
            "| batch: 122.000000 |\n",
            "| loss:   5.634144 |\n",
            "| batch: 124.000000 |\n",
            "| loss:   5.695945 |\n",
            "| batch: 126.000000 |\n",
            "| loss:   5.655732 |\n",
            "| batch: 128.000000 |\n",
            "| loss:   5.647169 |\n",
            "| batch: 130.000000 |\n",
            "| loss:   5.714536 |\n",
            "| batch: 132.000000 |\n",
            "| loss:   5.765784 |\n",
            "| batch: 134.000000 |\n",
            "| loss:   5.617044 |\n",
            "| batch: 136.000000 |\n",
            "| loss:   5.613208 |\n",
            "| batch: 138.000000 |\n",
            "| loss:   5.791802 |\n",
            "| batch: 140.000000 |\n",
            "| loss:   5.768951 |\n",
            "| batch: 142.000000 |\n",
            "| loss:   5.719052 |\n",
            "| batch: 144.000000 |\n",
            "| loss:   5.709709 |\n",
            "| batch: 146.000000 |\n",
            "| loss:   5.671079 |\n",
            "| batch: 148.000000 |\n",
            "| loss:   5.691010 |\n",
            "| batch: 150.000000 |\n",
            "| loss:   5.758157 |\n",
            "| batch: 152.000000 |\n",
            "| loss:   5.550975 |\n",
            "| batch: 154.000000 |\n",
            "| loss:   5.662760 |\n",
            "| batch: 156.000000 |\n",
            "| loss:   5.856037 |\n",
            "| batch: 158.000000 |\n",
            "| loss:   5.707410 |\n",
            "| batch: 160.000000 |\n",
            "| loss:   5.682099 |\n",
            "| batch: 162.000000 |\n",
            "| loss:   5.611322 |\n",
            "| batch: 164.000000 |\n",
            "| loss:   5.698998 |\n",
            "| batch: 166.000000 |\n",
            "| loss:   5.618070 |\n",
            "| batch: 168.000000 |\n",
            "| loss:   5.859137 |\n",
            "| batch: 170.000000 |\n",
            "| loss:   5.758991 |\n",
            "| batch: 172.000000 |\n",
            "| loss:   5.784593 |\n",
            "| batch: 174.000000 |\n",
            "| loss:   5.663435 |\n",
            "| batch: 176.000000 |\n",
            "| loss:   5.613940 |\n",
            "| batch: 178.000000 |\n",
            "| loss:   5.739725 |\n",
            "| batch: 180.000000 |\n",
            "| loss:   5.748941 |\n",
            "| batch: 182.000000 |\n",
            "| loss:   5.697735 |\n",
            "| batch: 184.000000 |\n",
            "| loss:   5.679906 |\n",
            "| batch: 186.000000 |\n",
            "| loss:   5.710120 |\n",
            "| batch: 188.000000 |\n",
            "| loss:   5.762179 |\n",
            "| batch: 190.000000 |\n",
            "| loss:   5.685689 |\n",
            "| batch: 192.000000 |\n",
            "| loss:   5.671332 |\n",
            "| batch: 194.000000 |\n",
            "| loss:   5.723959 |\n",
            "| batch: 196.000000 |\n",
            "| loss:   5.545518 |\n",
            "| batch: 198.000000 |\n",
            "| loss:   5.724441 |\n",
            "| batch: 200.000000 |\n",
            "| loss:   5.654722 |\n",
            "| batch: 202.000000 |\n",
            "| loss:   5.661948 |\n",
            "| batch: 204.000000 |\n",
            "| loss:   5.638768 |\n",
            "| batch: 206.000000 |\n",
            "| loss:   5.632142 |\n",
            "| batch: 208.000000 |\n",
            "| loss:   5.689116 |\n",
            "| batch: 210.000000 |\n",
            "| loss:   5.667794 |\n",
            "| batch: 212.000000 |\n",
            "| loss:   5.834950 |\n",
            "| batch: 214.000000 |\n",
            "| loss:   5.743100 |\n",
            "| batch: 216.000000 |\n",
            "| loss:   5.749458 |\n",
            "| batch: 218.000000 |\n",
            "| loss:   5.585772 |\n",
            "| batch: 220.000000 |\n",
            "| loss:   5.715462 |\n",
            "| batch: 222.000000 |\n",
            "| loss:   5.796510 |\n",
            "| batch: 224.000000 |\n",
            "| loss:   5.796500 |\n",
            "| batch: 226.000000 |\n",
            "| loss:   5.598905 |\n",
            "| batch: 228.000000 |\n",
            "| loss:   5.833711 |\n",
            "| batch: 230.000000 |\n",
            "| loss:   5.601559 |\n",
            "| batch: 232.000000 |\n",
            "| loss:   5.766496 |\n",
            "| batch: 234.000000 |\n",
            "| loss:   5.702941 |\n",
            "| batch: 236.000000 |\n",
            "| loss:   5.763667 |\n",
            "| batch: 238.000000 |\n",
            "| loss:   5.720950 |\n",
            "| batch: 240.000000 |\n",
            "| loss:   5.815277 |\n",
            "| batch: 242.000000 |\n",
            "| loss:   5.724886 |\n",
            "| batch: 244.000000 |\n",
            "| loss:   5.658473 |\n",
            "| batch: 246.000000 |\n",
            "| loss:   5.671096 |\n",
            "| batch: 248.000000 |\n",
            "| loss:   5.675913 |\n",
            "| batch: 250.000000 |\n",
            "| loss:   5.724156 |\n",
            "| batch: 252.000000 |\n",
            "| loss:   5.656196 |\n",
            "| batch: 254.000000 |\n",
            "| loss:   5.646010 |\n",
            "| batch: 256.000000 |\n",
            "| loss:   5.762313 |\n",
            "| batch: 258.000000 |\n",
            "| loss:   5.736423 |\n",
            "| batch: 260.000000 |\n",
            "| loss:   5.646635 |\n",
            "| batch: 262.000000 |\n",
            "| loss:   5.719352 |\n",
            "| batch: 264.000000 |\n",
            "| loss:   5.617198 |\n",
            "| batch: 266.000000 |\n",
            "| loss:   5.822911 |\n",
            "| batch: 268.000000 |\n",
            "| loss:   5.668917 |\n",
            "| batch: 270.000000 |\n",
            "| loss:   5.737894 |\n",
            "| batch: 272.000000 |\n",
            "| loss:   5.731742 |\n",
            "| batch: 274.000000 |\n",
            "| loss:   5.713710 |\n",
            "| batch: 276.000000 |\n",
            "| loss:   5.763665 |\n",
            "| batch: 278.000000 |\n",
            "| loss:   5.683166 |\n",
            "| batch: 280.000000 |\n",
            "| loss:   5.682159 |\n",
            "| batch: 282.000000 |\n",
            "| loss:   5.675693 |\n",
            "| batch: 284.000000 |\n",
            "| loss:   5.769903 |\n",
            "| batch: 286.000000 |\n",
            "| loss:   5.682584 |\n",
            "| batch: 288.000000 |\n",
            "| loss:   5.734526 |\n",
            "| batch: 290.000000 |\n",
            "| loss:   5.715013 |\n",
            "| batch: 292.000000 |\n",
            "| loss:   5.781519 |\n",
            "| batch: 294.000000 |\n",
            "| loss:   5.679394 |\n",
            "| batch: 296.000000 |\n",
            "| loss:   5.668206 |\n",
            "| batch: 298.000000 |\n",
            "| loss:   5.739726 |\n",
            "| batch: 300.000000 |\n",
            "| loss:   5.785971 |\n",
            "| batch: 302.000000 |\n",
            "| loss:   5.750833 |\n",
            "| batch: 304.000000 |\n",
            "| loss:   5.800136 |\n",
            "| batch: 306.000000 |\n",
            "| loss:   5.623332 |\n",
            "| batch: 308.000000 |\n",
            "| loss:   5.717695 |\n",
            "| batch: 310.000000 |\n",
            "| loss:   5.629852 |\n",
            "| batch: 312.000000 |\n",
            "| loss:   5.636949 |\n",
            "| batch: 314.000000 |\n",
            "| loss:   5.668684 |\n",
            "| batch: 316.000000 |\n",
            "| loss:   5.611304 |\n",
            "| batch: 318.000000 |\n",
            "| loss:   5.755435 |\n",
            "| batch: 320.000000 |\n",
            "| loss:   5.740435 |\n",
            "| batch: 322.000000 |\n",
            "| loss:   5.758149 |\n",
            "| batch: 324.000000 |\n",
            "| loss:   5.744200 |\n",
            "| batch: 326.000000 |\n",
            "| loss:   5.690626 |\n",
            "| batch: 328.000000 |\n",
            "| loss:   5.729604 |\n",
            "| batch: 330.000000 |\n",
            "| loss:   5.723950 |\n",
            "| batch: 332.000000 |\n",
            "| loss:   5.714353 |\n",
            "| batch: 334.000000 |\n",
            "| loss:   5.725259 |\n",
            "| batch: 336.000000 |\n",
            "| loss:   5.781127 |\n",
            "| batch: 338.000000 |\n",
            "| loss:   5.699281 |\n",
            "| batch: 340.000000 |\n",
            "| loss:   5.697299 |\n",
            "| batch: 342.000000 |\n",
            "| loss:   5.655828 |\n",
            "| batch: 344.000000 |\n",
            "| loss:   5.713370 |\n",
            "| batch: 346.000000 |\n",
            "| loss:   5.675503 |\n",
            "| batch: 348.000000 |\n",
            "| loss:   5.769488 |\n",
            "| batch: 350.000000 |\n",
            "| loss:   5.624644 |\n",
            "| batch: 352.000000 |\n",
            "| loss:   5.638533 |\n",
            "| batch: 354.000000 |\n",
            "| loss:   5.786303 |\n",
            "| batch: 356.000000 |\n",
            "| loss:   5.695043 |\n",
            "| batch: 358.000000 |\n",
            "| loss:   5.686474 |\n",
            "| batch: 360.000000 |\n",
            "| loss:   5.780255 |\n",
            "| batch: 362.000000 |\n",
            "| loss:   5.665841 |\n",
            "| batch: 364.000000 |\n",
            "| loss:   5.724397 |\n",
            "| batch: 366.000000 |\n",
            "| loss:   5.640125 |\n",
            "| batch: 368.000000 |\n",
            "| loss:   5.777256 |\n",
            "| batch: 370.000000 |\n",
            "| loss:   5.639462 |\n",
            "| batch: 372.000000 |\n",
            "| loss:   5.754659 |\n",
            "| batch: 374.000000 |\n",
            "| loss:   5.644741 |\n",
            "| batch: 376.000000 |\n",
            "| loss:   5.800364 |\n",
            "| batch: 378.000000 |\n",
            "| loss:   5.690261 |\n",
            "| batch: 380.000000 |\n",
            "| loss:   5.786300 |\n",
            "| batch: 382.000000 |\n",
            "| loss:   5.653154 |\n",
            "| batch: 384.000000 |\n",
            "| loss:   5.748506 |\n",
            "| batch: 386.000000 |\n",
            "| loss:   5.709845 |\n",
            "| batch: 388.000000 |\n",
            "| loss:   5.594295 |\n",
            "| batch: 390.000000 |\n",
            "| loss:   5.729974 |\n",
            "| batch: 392.000000 |\n",
            "| loss:   5.689091 |\n",
            "| batch: 394.000000 |\n",
            "| loss:   5.805434 |\n",
            "| batch: 396.000000 |\n",
            "| loss:   5.637497 |\n",
            "| batch: 398.000000 |\n",
            "| loss:   5.731547 |\n",
            "| batch: 400.000000 |\n",
            "| loss:   5.590235 |\n",
            "| batch: 402.000000 |\n",
            "| loss:   5.684525 |\n",
            "| batch: 404.000000 |\n",
            "| loss:   5.643516 |\n",
            "| batch: 406.000000 |\n",
            "| loss:   5.712907 |\n",
            "| batch: 408.000000 |\n",
            "| loss:   5.754455 |\n",
            "| batch: 410.000000 |\n",
            "| loss:   5.721120 |\n",
            "| batch: 412.000000 |\n",
            "| loss:   5.695110 |\n",
            "| batch: 414.000000 |\n",
            "| loss:   5.624801 |\n",
            "| batch: 416.000000 |\n",
            "| loss:   5.714654 |\n",
            "| batch: 418.000000 |\n",
            "| loss:   5.785615 |\n",
            "| batch: 420.000000 |\n",
            "| loss:   5.598886 |\n",
            "| batch: 422.000000 |\n",
            "| loss:   5.595267 |\n",
            "| batch: 424.000000 |\n",
            "| loss:   5.726578 |\n",
            "| batch: 426.000000 |\n",
            "| loss:   5.672776 |\n",
            "| batch: 428.000000 |\n",
            "| loss:   5.643095 |\n",
            "| batch: 430.000000 |\n",
            "| loss:   5.775186 |\n",
            "| batch: 432.000000 |\n",
            "| loss:   5.647961 |\n",
            "| batch: 434.000000 |\n",
            "| loss:   5.724834 |\n",
            "| batch: 436.000000 |\n",
            "| loss:   5.800786 |\n",
            "| batch: 438.000000 |\n",
            "| loss:   5.715930 |\n",
            "| batch: 440.000000 |\n",
            "| loss:   5.728625 |\n",
            "| batch: 442.000000 |\n",
            "| loss:   5.743277 |\n",
            "| batch: 444.000000 |\n",
            "| loss:   5.696329 |\n",
            "| batch: 446.000000 |\n",
            "| loss:   5.741280 |\n",
            "| batch: 448.000000 |\n",
            "| loss:   5.818482 |\n",
            "| batch: 450.000000 |\n",
            "| loss:   5.655516 |\n",
            "| batch: 452.000000 |\n",
            "| loss:   5.781500 |\n",
            "| batch: 454.000000 |\n",
            "| loss:   5.748113 |\n",
            "| batch: 456.000000 |\n",
            "| loss:   5.662438 |\n",
            "| batch: 458.000000 |\n",
            "| loss:   5.643672 |\n",
            "| batch: 460.000000 |\n",
            "| loss:   5.649267 |\n",
            "| batch: 462.000000 |\n",
            "| loss:   5.738368 |\n",
            "| batch: 464.000000 |\n",
            "| loss:   5.662013 |\n",
            "| batch: 466.000000 |\n",
            "| loss:   5.700292 |\n",
            "| batch: 468.000000 |\n",
            "| loss:   5.679734 |\n",
            "| batch: 470.000000 |\n",
            "| loss:   5.651541 |\n",
            "| batch: 472.000000 |\n",
            "| loss:   5.646913 |\n",
            "| batch: 474.000000 |\n",
            "| loss:   5.715169 |\n",
            "| batch: 476.000000 |\n",
            "| loss:   5.671159 |\n",
            "| batch: 478.000000 |\n",
            "| loss:   5.695395 |\n",
            "| batch: 480.000000 |\n",
            "| loss:   5.637673 |\n",
            "| batch: 482.000000 |\n",
            "| loss:   5.696552 |\n",
            "| batch: 484.000000 |\n",
            "| loss:   5.695168 |\n",
            "| batch: 486.000000 |\n",
            "| loss:   5.725201 |\n",
            "| batch: 488.000000 |\n",
            "| loss:   5.771994 |\n",
            "| batch: 490.000000 |\n",
            "| loss:   5.683330 |\n",
            "| batch: 492.000000 |\n",
            "| loss:   5.735773 |\n",
            "| batch: 494.000000 |\n",
            "| loss:   5.646560 |\n",
            "| batch: 496.000000 |\n",
            "| loss:   5.690721 |\n",
            "| batch: 498.000000 |\n",
            "| loss:   5.799345 |\n",
            "| batch: 500.000000 |\n",
            "| loss:   5.670804 |\n",
            "| batch: 502.000000 |\n",
            "| loss:   5.563524 |\n",
            "| batch: 504.000000 |\n",
            "| loss:   5.604422 |\n",
            "| batch: 506.000000 |\n",
            "| loss:   5.748996 |\n",
            "| batch: 508.000000 |\n",
            "| loss:   5.662284 |\n",
            "| batch: 510.000000 |\n",
            "| loss:   5.678110 |\n",
            "| batch: 512.000000 |\n",
            "| loss:   5.783669 |\n",
            "| batch: 514.000000 |\n",
            "| loss:   5.819732 |\n",
            "| batch: 516.000000 |\n",
            "| loss:   5.682385 |\n",
            "| batch: 518.000000 |\n",
            "| loss:   5.698215 |\n",
            "| batch: 520.000000 |\n",
            "| loss:   5.713952 |\n",
            "| batch: 522.000000 |\n",
            "| loss:   5.642363 |\n",
            "| batch: 524.000000 |\n",
            "| loss:   5.655123 |\n",
            "| batch: 526.000000 |\n",
            "| loss:   5.612864 |\n",
            "| batch: 528.000000 |\n",
            "| loss:   5.665408 |\n",
            "| batch: 530.000000 |\n",
            "| loss:   5.810832 |\n",
            "| batch: 532.000000 |\n",
            "| loss:   5.779718 |\n",
            "| batch: 534.000000 |\n",
            "| loss:   5.711176 |\n",
            "| batch: 536.000000 |\n",
            "| loss:   5.639143 |\n",
            "| batch: 538.000000 |\n",
            "| loss:   5.685913 |\n",
            "| batch: 540.000000 |\n",
            "| loss:   5.740880 |\n",
            "| batch: 542.000000 |\n",
            "| loss:   5.661249 |\n",
            "| batch: 544.000000 |\n",
            "| loss:   5.747193 |\n",
            "| batch: 546.000000 |\n",
            "| loss:   5.740572 |\n",
            "| batch: 548.000000 |\n",
            "| loss:   5.723024 |\n",
            "| batch: 550.000000 |\n",
            "| loss:   5.791772 |\n",
            "| batch: 552.000000 |\n",
            "| loss:   5.773244 |\n",
            "| batch: 554.000000 |\n",
            "| loss:   5.746439 |\n",
            "| batch: 556.000000 |\n",
            "| loss:   5.622389 |\n",
            "| batch: 558.000000 |\n",
            "| loss:   5.730453 |\n",
            "| batch: 560.000000 |\n",
            "| loss:   5.616403 |\n",
            "| batch: 562.000000 |\n",
            "| loss:   5.666820 |\n",
            "| batch: 564.000000 |\n",
            "| loss:   5.758934 |\n",
            "| batch: 566.000000 |\n",
            "| loss:   5.708869 |\n",
            "| batch: 568.000000 |\n",
            "| loss:   5.714576 |\n",
            "| batch: 570.000000 |\n",
            "| loss:   5.676291 |\n",
            "| batch: 572.000000 |\n",
            "| loss:   5.800718 |\n",
            "| batch: 574.000000 |\n",
            "| loss:   5.603433 |\n",
            "| batch: 576.000000 |\n",
            "| loss:   5.703969 |\n",
            "| batch: 578.000000 |\n",
            "| loss:   5.761146 |\n",
            "| batch: 580.000000 |\n",
            "| loss:   5.723878 |\n",
            "| batch: 582.000000 |\n",
            "| loss:   5.677853 |\n",
            "| batch: 584.000000 |\n",
            "| loss:   5.662667 |\n",
            "| batch: 586.000000 |\n",
            "| loss:   5.773773 |\n",
            "| batch: 588.000000 |\n",
            "| loss:   5.779829 |\n",
            "| batch: 590.000000 |\n",
            "| loss:   5.836771 |\n",
            "| batch: 592.000000 |\n",
            "| loss:   5.720297 |\n",
            "| batch: 594.000000 |\n",
            "| loss:   5.729251 |\n",
            "| batch: 596.000000 |\n",
            "| loss:   5.626362 |\n",
            "| batch: 598.000000 |\n",
            "| loss:   5.698863 |\n",
            "| batch: 600.000000 |\n",
            "| loss:   5.661419 |\n",
            "| batch: 602.000000 |\n",
            "| loss:   5.757712 |\n",
            "| batch: 604.000000 |\n",
            "| loss:   5.698205 |\n",
            "| batch: 606.000000 |\n",
            "| loss:   5.682545 |\n",
            "| batch: 608.000000 |\n",
            "| loss:   5.745944 |\n",
            "| batch: 610.000000 |\n",
            "| loss:   5.767560 |\n",
            "| batch: 612.000000 |\n",
            "| loss:   5.644779 |\n",
            "| batch: 614.000000 |\n",
            "| loss:   5.739784 |\n",
            "| batch: 616.000000 |\n",
            "| loss:   5.828448 |\n",
            "| batch: 618.000000 |\n",
            "| loss:   5.723170 |\n",
            "| batch: 620.000000 |\n",
            "| loss:   5.692955 |\n",
            "| batch: 622.000000 |\n",
            "| loss:   5.667472 |\n",
            "| batch: 624.000000 |\n",
            "| loss:   5.779748 |\n",
            "| batch: 626.000000 |\n",
            "| loss:   5.783281 |\n",
            "| batch: 628.000000 |\n",
            "| loss:   5.603606 |\n",
            "| batch: 630.000000 |\n",
            "| loss:   5.652443 |\n",
            "| batch: 632.000000 |\n",
            "| loss:   5.700125 |\n",
            "| batch: 634.000000 |\n",
            "| loss:   5.759459 |\n",
            "| batch: 636.000000 |\n",
            "| loss:   5.777976 |\n",
            "| batch: 638.000000 |\n",
            "| loss:   5.677731 |\n",
            "| batch: 640.000000 |\n",
            "| loss:   5.670565 |\n",
            "| batch: 642.000000 |\n",
            "| loss:   5.644856 |\n",
            "| batch: 644.000000 |\n",
            "| loss:   5.688981 |\n",
            "| batch: 646.000000 |\n",
            "| loss:   5.761822 |\n",
            "| batch: 648.000000 |\n",
            "| loss:   5.791437 |\n",
            "| batch: 650.000000 |\n",
            "| loss:   5.620008 |\n",
            "| batch: 652.000000 |\n",
            "| loss:   5.610426 |\n",
            "| batch: 654.000000 |\n",
            "| loss:   5.694695 |\n",
            "| batch: 656.000000 |\n",
            "| loss:   5.708746 |\n",
            "| batch: 658.000000 |\n",
            "| loss:   5.677609 |\n",
            "| batch: 660.000000 |\n",
            "| loss:   5.687150 |\n",
            "| batch: 662.000000 |\n",
            "| loss:   5.763006 |\n",
            "| batch: 664.000000 |\n",
            "| loss:   5.735489 |\n",
            "| batch: 666.000000 |\n",
            "| loss:   5.715061 |\n",
            "| batch: 668.000000 |\n",
            "| loss:   5.774557 |\n",
            "| batch: 670.000000 |\n",
            "| loss:   5.645854 |\n",
            "| batch: 672.000000 |\n",
            "| loss:   5.692641 |\n",
            "| batch: 674.000000 |\n",
            "| loss:   5.765306 |\n",
            "| batch: 676.000000 |\n",
            "| loss:   5.736177 |\n",
            "| batch: 678.000000 |\n",
            "| loss:   5.711516 |\n",
            "| batch: 680.000000 |\n",
            "| loss:   5.829175 |\n",
            "| batch: 682.000000 |\n",
            "| loss:   5.743356 |\n",
            "| batch: 684.000000 |\n",
            "| loss:   5.610192 |\n",
            "| batch: 686.000000 |\n",
            "| loss:   5.695099 |\n",
            "| batch: 688.000000 |\n",
            "| loss:   5.691004 |\n",
            "| batch: 690.000000 |\n",
            "| loss:   5.801718 |\n",
            "| batch: 692.000000 |\n",
            "| loss:   5.616231 |\n",
            "| batch: 694.000000 |\n",
            "| loss:   5.685544 |\n",
            "| batch: 696.000000 |\n",
            "| loss:   5.729452 |\n",
            "| batch: 698.000000 |\n",
            "| loss:   5.677748 |\n",
            "| batch: 700.000000 |\n",
            "| loss:   5.815055 |\n",
            "| batch: 702.000000 |\n",
            "| loss:   5.774103 |\n",
            "| batch: 704.000000 |\n",
            "| loss:   5.748333 |\n",
            "| batch: 706.000000 |\n",
            "| loss:   5.795395 |\n",
            "| batch: 708.000000 |\n",
            "| loss:   5.707425 |\n",
            "| batch: 710.000000 |\n",
            "| loss:   5.699366 |\n",
            "| batch: 712.000000 |\n",
            "| loss:   5.635588 |\n",
            "| batch: 714.000000 |\n",
            "| loss:   5.830424 |\n",
            "| batch: 716.000000 |\n",
            "| loss:   5.665092 |\n",
            "| batch: 718.000000 |\n",
            "| loss:   5.763077 |\n",
            "| batch: 720.000000 |\n",
            "| loss:   5.712405 |\n",
            "| batch: 722.000000 |\n",
            "| loss:   5.750157 |\n",
            "| batch: 724.000000 |\n",
            "| loss:   5.681701 |\n",
            "| batch: 726.000000 |\n",
            "| loss:   5.559276 |\n",
            "| batch: 728.000000 |\n",
            "| loss:   5.712912 |\n",
            "| batch: 730.000000 |\n",
            "| loss:   5.695651 |\n",
            "| batch: 732.000000 |\n",
            "| loss:   5.779403 |\n",
            "| batch: 734.000000 |\n",
            "| loss:   5.747378 |\n",
            "| batch: 736.000000 |\n",
            "| loss:   5.736737 |\n",
            "| batch: 738.000000 |\n",
            "| loss:   5.735701 |\n",
            "| batch: 740.000000 |\n",
            "| loss:   5.643704 |\n",
            "| batch: 742.000000 |\n",
            "| loss:   5.656758 |\n",
            "| batch: 744.000000 |\n",
            "| loss:   5.741230 |\n",
            "| batch: 746.000000 |\n",
            "| loss:   5.830055 |\n",
            "| batch: 748.000000 |\n",
            "| loss:   5.735395 |\n",
            "| batch: 750.000000 |\n",
            "| loss:   5.734758 |\n",
            "| batch: 752.000000 |\n",
            "| loss:   5.601075 |\n",
            "| batch: 754.000000 |\n",
            "| loss:   5.822720 |\n",
            "| batch: 756.000000 |\n",
            "| loss:   5.682706 |\n",
            "| batch: 758.000000 |\n",
            "| loss:   5.779654 |\n",
            "| batch: 760.000000 |\n",
            "| loss:   5.731943 |\n",
            "| batch: 762.000000 |\n",
            "| loss:   5.776437 |\n",
            "| batch: 764.000000 |\n",
            "| loss:   5.821845 |\n",
            "| batch: 766.000000 |\n",
            "| loss:   5.691924 |\n",
            "| batch: 768.000000 |\n",
            "| loss:   5.631874 |\n",
            "| batch: 770.000000 |\n",
            "| loss:   5.842930 |\n",
            "| batch: 772.000000 |\n",
            "| loss:   5.620048 |\n",
            "| batch: 774.000000 |\n",
            "| loss:   5.637345 |\n",
            "| batch: 776.000000 |\n",
            "| loss:   5.771146 |\n",
            "| batch: 778.000000 |\n",
            "| loss:   5.818068 |\n",
            "| batch: 780.000000 |\n",
            "| loss:   5.755413 |\n",
            "| batch: 782.000000 |\n",
            "| loss:   5.657232 |\n",
            "Total Batches -  87\n",
            "| val_batch:   0.000000 |\n",
            "| val_loss:   5.672854 |\n",
            "| val_batch:   5.000000 |\n",
            "| val_loss:   5.615663 |\n",
            "| val_batch:  10.000000 |\n",
            "| val_loss:   5.660030 |\n",
            "| val_batch:  15.000000 |\n",
            "| val_loss:   5.607353 |\n",
            "| val_batch:  20.000000 |\n",
            "| val_loss:   5.532246 |\n",
            "| val_batch:  25.000000 |\n",
            "| val_loss:   5.670418 |\n",
            "| val_batch:  30.000000 |\n",
            "| val_loss:   5.633608 |\n",
            "| val_batch:  35.000000 |\n",
            "| val_loss:   5.603541 |\n",
            "| val_batch:  40.000000 |\n",
            "| val_loss:   5.631882 |\n",
            "| val_batch:  45.000000 |\n",
            "| val_loss:   5.622099 |\n",
            "| val_batch:  50.000000 |\n",
            "| val_loss:   5.612194 |\n",
            "| val_batch:  55.000000 |\n",
            "| val_loss:   5.661093 |\n",
            "| val_batch:  60.000000 |\n",
            "| val_loss:   5.635187 |\n",
            "| val_batch:  65.000000 |\n",
            "| val_loss:   5.613966 |\n",
            "| val_batch:  70.000000 |\n",
            "| val_loss:   5.568614 |\n",
            "| val_batch:  75.000000 |\n",
            "| val_loss:   5.633792 |\n",
            "| val_batch:  80.000000 |\n",
            "| val_loss:   5.547260 |\n",
            "| val_batch:  85.000000 |\n",
            "| val_loss:   5.572479 |\n",
            "Epoch: 06 | Time: 8m 59s\n",
            "\tTrain Loss: 5.706 | Train PPL: 300.719\n",
            "\t Val. Loss: 5.620 |  Val. PPL: 275.951\n",
            "Saved Hyperparameters at /content/drive/My Drive/ALDA Project/ConvSeq2seq_translation/6/param.txt\n",
            "This run of Question Generation-ConvSeq2seq ran for 0:54:03 and logs are available locally at: /root/.hyperdash/logs/question-generation-convseq2seq/question-generation-convseq2seq_2020-04-19t19-56-30-239716.log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar5FpWsL4_yi",
        "colab_type": "code",
        "outputId": "2f28308e-c8b4-425a-89ab-dcf827bf2d0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "x=random.choice(input_)\n",
        "model.decode(x,inpLang,optLang)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('el amor de mary . mary . mary . el <UNK> de mary . mary . el <UNK> de mary . el <UNK> de mary . mary . mary . mary . el <UNK> . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary . mary .',\n",
              " tensor([[[0.0065, 0.3775, 0.0538,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.0043, 0.1225, 0.0818,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.0072, 0.0348, 0.0343,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[0.0228, 0.1370, 0.0208,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.0560, 0.1364, 0.0297,  ..., 0.0000, 0.0000, 0.0000]],\n",
              " \n",
              "         [[0.0227, 0.1355, 0.0205,  ..., 0.0000, 0.0000, 0.0000]]],\n",
              "        device='cuda:0', grad_fn=<CopySlices>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwBXyuoUdNkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "def display_attention(sentence, translation, attention):\n",
        "    \n",
        "    fig = plt.figure(figsize=(len(sentence)/1.5,len(translation)/1.5))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    cax = ax.matshow(attention[:len(translation),:len(sentence)+1], cmap='bone')\n",
        "   \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels(['']+['<sos>']+[t.lower() for t in sentence]+['<eos>'], \n",
        "                       rotation=45)\n",
        "    ax.set_yticklabels(['']+translation)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOElg6r_ukcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent=random.choice(input_test)\n",
        "pred,attn=model.decode(sent,inpLang,optLang)\n",
        "display_attention(inpLang.tokenize(sent),pred.split(\" \"),attn)\n",
        "print(pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xV9wwguoKpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}